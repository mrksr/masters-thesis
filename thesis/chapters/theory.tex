\chapter{Theoretical Background}
\label{cha:theory}
The bicycle benchmark is a problem in a branch of machine learning called reinforcement learning.
This chapter gives a brief introduction into reinforcement learning and defines the notation necessary to reason about the bicycle benchmark and similar problems in a mathematical way.
The approach used in this thesis to solve it is called model-based reinforcement learning and is discussed next.

The models in model-based reinforcement learning are used to learn the dynamics of the system to be controlled in order to be able to make predictions about its future development.
Gaussian processes provide a framework to learn such dynamics in a way which adds information about the uncertainty of a prediction.
This is achieved by learning a distribution over possible models rather than deciding on one single model.
The uncertainty about the development in a single time step can be used to reason about the uncertainty of predictions multiple steps into the future.

Being able to make judgments about the future dependent on the next action to take allows the definition of a controller.
Particle swarm optimization is introduced as a way to choose the best action to take based on a performance-measure of the predicted future states.

\section{Reinforcement Learning}
Consider an agent who wants to learn how to learn how to drive a bicycle to a certain position.
In the case of a supervised learning environment, an expert who already knows how to solve the problem could tell this agent which actions lead to success.
In the absence of such a teacher however, the agent must learn from interaction with the bicycle.

An agent might start by applying random actions to the system and quickly fall down, making it impossible to ever reach the goal.
This gives the agent an opportunity to learn:
It has to avoid falling down in order to have the chance of achieving its objective.
A human agent who falls down from a bicycle feels pain when hitting the asphalt which they will try to avoid.
Such feedback is called a \emph{reward} (or in this case, a punishment) and is the basis on which the agent can learn to judge the viability of actions in a certain state.

After multiple trials, the agent might be able to avoid falling and be able to stabilize the bicycle.
To achieve this, it may have built a basic understanding of how the bicycle system behaves.
It might have recognized that a bicycle which is already leaning on one side if left alone will fall down because of the gravitational pull or that driving in a curve means that the centrifugal force pushes the bicycle to the outside.
Note that to gain these insights, it is not necessary for the agent to have an understanding of the underlying physics.
It is enough to observe situations in which the effects play out and generalize from there.

When the agent has learnt how to stabilize the bicycle by driving small corrective curves, it has not yet solved the original task of navigating the bicycle to a specific position.
It will have to shift its focus from the short-term goal of avoiding falling over to the more high-level and long-term task of following a targeted trajectory.
Following this trajectory might require some compromises concerning the previous step of avoiding to fall down, since it is easier to drive straight than it is to drive along a specific curve.
In the case that reaching the goal is time-critical, the optimal trajectory would be constrained by the minimum radius of a curve the agent can handle.
In the case of a perfect controller, it is constrained by the maximum lean angle $\omega$.

The requirements an agent faces when solving the bicycle benchmark can be understood in a more general sense, which is formulated in the problem statement of reinforcement learning.
It formalizes the ideas of and agent, it's interaction with a system and the positive or negative feedback it receives.

\subsection{Problem Statement}
Reinforcement learning is meant to describe the general problem of learning to control a system by interaction in order to achieve a predefined goal.
In order to achieve this goal, the learning entity or \emph{agent} has to decide on specific actions to influence its \emph{environment} which is everything outside of the agent.
The boundary between agent and environment is a well-defined and narrow interface illustrated in \cref{fig:agent_environment_interaction}.
They interact via this interface at specific discrete \emph{time steps}, usually indexed with the natural numbers $\N$.
\begin{figure}[t]
    \centering
    \includestandalone{figures/theory_agent_environment}
    \caption{
        The interaction between an agent and its environment in reinforcement learning happens at discrete time steps.
        At every time step $t$, the agent observes the current state of the environment $s_t$ and decides on an action $a_t$ which should be performed.
        Based on this action, the environment evolves and is observed to be at the state $s_{t+1}$ at the next time step.
        Additionally, the agent receives immediate feedback about this new state in form of the numeric reward $r_{t+1}$.
    }
    \label{fig:agent_environment_interaction}
\end{figure}

At every such time step $t$, the agent can observe the environment and receives some information in form of its \emph{state} $s_t \in \Es$, where $\Es$ is the continuous space of all possible states.
Based on this information, the agent has to decide on which action $a_t \in \Ah$ to perform.
The space of all possible actions $\Ah$ is assumed to be continuous and remains constant for all time steps and states.

The decision-process an agent employs in order to choose an action is called the agent's \emph{policy}.
A policy is a function which maps states to actions.
Policies are often encoded in closed forms such as linear functions \cite{deisenroth_efficient_2010}.
\begin{definition}[Policy]
    A \emph{policy $\pi$} an agent follows encodes the choice it makes when faced with a decision.
    It is a function
    \begin{align}
        \pi \colon \Es \to \Ah
    \end{align}
    which maps the current state of the system to the action the agent will perform.
\end{definition}

Once the agent has chosen an action for time step $t$, the \emph{transition dynamics} of the system determine the state $s_{t+1}$.
These dynamics are unknown to the agent and can be subject to probabilistic factors such as noise.
While the agent will always observe one single element $s_{t+1} \in \Es$ when prompted for the next decision, the transition dynamics return a probability distribution over the possible next states.

Many problems like the bicycle benchmark have a natural notion of states which are \emph{terminal}.
If the cyclist falls down or reaches the goal, the interaction between the agent and the environment comes to an end and there are no more decisions to make.
Such tasks are called \emph{episodic} and one sequence from a start state to a terminal state is called an \emph{episode}.
Notationally, $\Tee$ is defined to be the set of all terminal states with $\Tee \cap \Es = \emptyset$.
An episode ends when the transition dynamics return a state which is in $\Tee$.
The set $\Es \cup \Tee$ is called the \emph{extended state space $\Es^+$} and combines both non-terminal and terminal states.
A task which does not have terminal states is called \emph{continuous} and has episodes of infinite length.

\begin{definition}[Transition Dynamics]
    \label{def:transition_dynamics}
    The \emph{transition dynamics $\Dyn$} of a system encode its physical behaviour.
    These dynamics
    \begin{align}
        \Dyn \colon \Es \times \Ah \to \Dists(\Es^+)
    \end{align}
    mapping a state and an action to a distribution over possibly terminal states stay constant over time but can be probabilistic in nature.
\end{definition}
An important observation following from this definition is that the transition dynamics fulfill the Markov property \cite{sutton_reinforcement_1998}.
This means that the distribution of the state $s_{t+1}$ is independent of all states before $s_t$ given $s_t$.
In other words, the transition dynamics only depend on the present state of the system and are conditionally independent of the past.

At time step $t+1$, the agent observes the environment again in the form of the state $s_{t+1}$.
Additionally, it also receives immediate feedback about this new state called the \emph{reward $r_{t+1}$}.
The reward is a quality assigned to each state and serves as a measure of how good a state to be in it is.
This measure is independent of the future or past development of the system.
It is obtained from a real-valued \emph{reward function $\Rwd$} such that $r_{t+1} = \Rwd(s_{t+1})$.
\begin{definition}[Reward Function]
    \label{def:reward_function}
    The \emph{reward function $\Rwd$} assigns a quality to each state in the extended state space and has the signature
    \begin{align}
        \Rwd \colon \Es^+ \to \R.
    \end{align}
    This reward is the immediate feedback an agent receives when interacting with the system.
\end{definition}

The goal of the agent is to maximize the sum of all rewards earned throughout an episode.
A greedy agent which is only concerned with the next immediate reward might not be the most successful, since it may be necessary to make a decision which is bad in the short term to gain an advantage in the long run, such as sacrificing a piece in chess to end up in a better position overall.

The \emph{value function} is a measure for how good a policy behaves in the long run.
Given a policy and a start state, it is defined as the expected sum of rewards earned in a time horizon $T$.
Since the transition dynamics are assumed to be probabilistic, the states at all time steps greater than zero are random variables.
Given a distribution of the state $\rv{s_t}$, the distribution for the next state $\rv{s_{t+1}}$ for all $t \in \N_{>0}$ can be calculated as
\begin{align}
    \Prob{\rv{s_{t+1}}} &= \int \Prob{\rv{s_{t+1}} \given \rv{s_t}, \pi}\Prob{\rv{s_t}} \diff \rv{s_t} \\
    \intertext{where}
    \label{eq:rl_successive_state}
    \Prob{\rv{s_{t+1}} \given \rv{s_t}, \pi} &= \begin{cases}
        \delta_{\rv{s_t}, \rv{s_{t+1}}} & \text{ if $\rv{s_t} \in \Tee$} \\
        \Dyn\Cond{\rv{s_{t+1}} \given \rv{s_t}, \pi(\rv{s_t})} & \text{ otherwise.}
    \end{cases}
\end{align}
Here, $\delta$ denotes the Dirac-delta-distribution and the notation $\Dyn\Cond{\rv{s_{t+1}} \given \rv{s_t}, \pi(\rv{s_t})}$ means the probability of $\rv{s_{t+1}}$ under the distribution $\Dyn(\rv{s_t}, \pi(\rv{s_t}))$.
Once the dynamics have reached a terminal state, all future states will be this same state.
This extends all episodes to potentially infinite length.

The time horizon $T$ can be chosen freely but does not depend on either the current policy or state.
The larger the time horizon, the more far-sighted a agent has to be to be successful.
For large values of $T$, it can be helpful to focus on rewards earned in the near future and to weigh potential rewards further along with a smaller factor.
This can be achieved using the discount factor $\gamma$ which is a constant real number between zero and one.
In the case of an infinite time horizon, $\gamma$ must be chosen smaller than one to ensure the well-definedness of the function.

\begin{definition}[Value Function]
    \label{def:value_function}
    Given transition dynamics $\Dyn$, a policy $\pi$, a time horizon $T \in \N \cup \left\{ \infty \right\}$ and a discount factor $0 \leq \gamma \leq 1$, the \emph{value function $\Vlu^\pi$} denotes the expected long-term reward of a state and is given by
    \begin{align}
        \Vlu^\pi \colon \left\{
            \begin{aligned}
                \Es^+ &\to \R \\
                s &\mapsto \Moment*{\E}{\sum_{t=0}^T \gamma^t \Rwd(\rv{s_t}) \given \Dyn, \pi, \rv{s_0} = s }.
            \end{aligned}
        \right.
    \end{align}
    If the time horizon is infinite, $\gamma$ must be smaller than 1.
\end{definition}

Assume now a known distribution of possible start states $\Prob{\rv{s_0}}$.
The sets $\Es^+$ and $\Ah$ of states and actions together with the transition dynamics $\Dyn$, the reward function $\Rwd$ and this distribution $\Prob{\rv{s_0}}$ defines a fully observable Markov decision process (MDP) \cite{sutton_reinforcement_1998,murphy_machine_2012}.

The objective of the reinforcement learning problem stated here is to find the most successful policy to control this decision process under the assumption that no expert knowledge about the transition dynamics is available.
The optimal policy $\pi^*$ maximizes the expected value for all start states, that is it solves the optimization problem
\begin{align}
    \label{eq:optimal_policy}
    \pi^* &\in \argmax_{\pi} \Moment*{\E_{\rv{s_0}}}{V^\pi(\rv{s_0})} \\
    &= \argmax_{\pi} \int V^\pi(\rv{s_0}) \Prob{\rv{s_0}} \diff \rv{s_0}.
\end{align}

There is a multitude of different approaches of arriving at good policies.
The following will introduce the important distinction of model-based and model-free methods and give a high level view of how the former will be used within this thesis.

\subsection{Model-Based Reinforcement Learning}
To find a good policy, an agent has to gain experience about its environment via interaction.
In \emph{direct reinforcement learning} \cite{sutton_reinforcement_1998}, or model-free methods, this experience is used to update the current policy or an estimation of its value function directly.
After enough time spent with the system, iteratively improving the current policy can converge to the optimal policy.

In \emph{model-based} reinforcement learning, the experience is used to learn an internal representation of the transition dynamics $\Dyn$.
This allows the agent to make predictions about the future behaviour of the system and therefore approximatively evaluate the value function without actually interacting with the system.
A closed form policy can then be found by solving the non-linear optimization problem proposed in \cref{eq:optimal_policy}.

\citeauthor{deisenroth_efficient_2010} \cite{deisenroth_efficient_2010} describes an algorithmic scheme called PILCO in which phases of exploration on the real system to gather more data to improve the internal model alternate with phases of improvement of the current policy based on the internal simulation.
This iteration allows the agent to explore promising directions in the state space using intermediate policies.

In this thesis it is assumed that all interaction with the system has to happen before any learning can take place.
This assumption is sensible in the context of industrial applications where interactions with a system can be very expensive and dangerous and where a potentially bad agent cannot be allowed to choose actions to perform.
Instead of allowing interaction, the agent is presented with a data set of observations of the transition dynamics in the form of tuples $(\mat{s_t}, \mat{a_t}, \mat{s_{t+1}})$.
These observations can be obtained via a mix of random exploration and actions chosen by a sub-optimal controller.

Using an internal representation instead of the real transition dynamics to choose actions leads to one of the major drawbacks of model-based reinforcement learning.
If this representation does not capture the important characteristics of the original system well, a policy found to be optimal on the simulation might not lead to good results on the correct dynamics.
This effect is called the \emph{model bias} of a policy.

Reducing this model bias is the main goal of explicitly representing uncertainties within this thesis.
Instead of focusing on one single dynamics model, a probabilistic representation of all plausible models to explain the observed data will be considered.
This yields a measure of uncertainty of predictions one step into the future and can be extended to long-term predictions as required in the value function.
Assuming deterministic transition dynamics such as the bicycle benchmark, this measure does not describe a property of the system but rather the uncertainty about the model itself.

Gaussian Processes provide a framework to represent such a distribution over possible models for the transition dynamics.
The following section will introduce their definition and derive how they can be used to solve regression problems.
The last part of this chapter will be concerned with introducing a policy-representation based on these results which does not depend on a closed form solution.

\section{Gaussian Process Regression}
\label{sec:gp_regression}
The transition dynamics $\Dyn \colon \Es \times \Ah \to \Dists(\Es^+)$ are a function mapping states and actions to a probability distribution of following states.
In order to estimate them using Gaussian processes, some assumptions about the structure of this function are needed.
First, it will be assumed that both the set of states $\Es$ and the set of actions $\Ah$ are euclidean real valued vector spaces and that the set of terminal states $\Tee$ is empty, that is, $\Es^+$ and $\Es$ are assumed equal, requiring that episode endings have to be modelled separately.
And secondly, the probability distribution of the following state is assumed to be unimodal.
This unimodality can result from deterministic transition functions, such as the one of the bicycle benchmark defined in \cref{cha:the_bicycle_benchmark}, being disturbed slightly by Gaussian noise.

Estimating a function $f$ on the basis of observations $\mat{y_i} = f(\mat{x_i}) + \epsilon_i \in \R$ with input vectors $\mat{x_i} \in \R^d$ and a noise term $\epsilon_i$ is a \emph{regression problem}.
Since the number of observations is finite and the function $f$ is an infinite object, the estimation of $f$ is uncertain and based on prior assumptions about its structure.

In classic control scenarios, these prior assumptions often follow from physical descriptions of the system to be modelled.
While the physics of driving a bicycle is understood quite well and can be described using differential equations, a controller for a specific bicycle depends on some parameters $\gamma$ such as the masses and measures detailed in \cref{tab:bicycle_constants}.
In this setting, solving the regression problem corresponds to finding a choice of parameters $\gamma^\ast$ which explain the observations of the true system best.

In a Bayesian context, instead of deciding on one specific vector of parameters, it might be more interesting to derive a distribution $\Prob{\gamma^*}$ of probable parameter values which then represents the uncertainty about their true value.
When making a prediction for a new input point $\mat{x_\ast}$, this uncertainty can be used to derive a predictive distribution $\Prob{y_\ast \given \mat{x_\ast}, \mat{\gamma^\ast}}$, which propagates this uncertainty through the model to the prediction.

This approach represents uncertainty about the correct choice of parameters but assumes that the predefined structure of the function is correct, making it a \emph{parametric model}.
Such structure has the advantage of making it easier to find the best set of parameters, since the search space is relatively limited.
It does however limit the expressiveness of the model, which can lead to bad performance.
A physical description of the system might be too idealized and not take account of all factors in reality, such as the assumptions of frictionless mechanics or limited turbulences in fluid mechanics.
Accounting for all possible effects can make the model very complicated.
This means that both the number of parameters becomes large and it may be hard to interpret the model in a physical sense.

\emph{Non-parametric models} are not based on insights about the concrete structure of the function to be modelled but rather only make assumptions about properties of the function itself, such as smoothness or differentiability.
Instead of modeling a distribution of parameter values, a Bayesian non-parametric model is concerned with finding a distribution $\Prob{f^\ast}$ of probable functions which represents the belief of the model about the function $f$ to be estimated.

\emph{Gaussian processes (GPs)} are a state-of-the-art framework for non-parametric regression.
They are a way of representing a probability distribution over functions in a way which is both computationally feasible and allows for Bayesian inference.
This section introduces Gaussian processes and describes how to encode a prior distribution over functions to represent preference in the space of all possible functions $f$.
Based on observed data, GPs can be used make predictions about the predictive distribution $\Prob{y_\ast \given \mat{x_\ast}, f^\ast}$ taking all functions in the distribution $\Prob{f^\ast}$ into account.
Since these predictions are not computationally cheap, an extension of Gaussian processes for large data sets, sparse Gaussian processes using pseudo-inputs \cite{snelson_sparse_2005}, is introduced last.

\subsection{Definition}
Gaussian processes are a generalization of the Gaussian distribution to function spaces.
A multivariate Gaussian $\mat{x} \sim \Gaussian{\mat{\mu}, \mat{\Sigma}}$ describes a distribution over the finitely many elements in the vector $\mat{x}$.
Every such element $\rv{x_i}$ is normally distributed according to $\rv{x_i} \sim \Gaussian{\mu_i, \Sigma_{ii}}$ with a particular dependency structure between them.
For every pair $(\rv{x_i}, \rv{x_j})$, their covariance is given by $\Moment{\cov}{\rv{x_i}, \rv{x_j}} = \mat{\Sigma}_{ij}$.

Modeling functions in general requires an infinite number of random variables, one for every function value.
An infinite number of possibly dependent random variables mapping from the same probability space to the same value space is called a \emph{stochastic process} and is represented via a function.

\begin{definition}[Stochastic Process]
    \label{def:stochastic_process}
    Given a probability space $(\Omega, \mathcal{F}, P)$, an index set $T$ and a measurable space $Y$, a \emph{stochastic process $\rv{X}$} is a function
    \begin{align}
        \rv{X} \colon \left\{\begin{aligned}
            T \times \Omega &\to Y \\
            (t, \omega) &\mapsto \rv{X_t}(\omega)
        \end{aligned}\right.
    \end{align}
    mapping indices $t$ to $Y$-valued random-variables.
    For a fixed $\omega \in \Omega$, $\rv{X}(\cdot, \omega)$ is called a \emph{trajectory} of the process \cite{astrom_introduction_1971}.
\end{definition}

The index set of a stochastic process can be an arbitrary set.
It is often interpreted as a time index which can be both discrete and continuous.
A Gaussian process is a particular stochastic process.
\begin{definition}[Gaussian Process]
    \label{def:gaussian_process}
    A stochastic process $\rv{X}$ is called a \emph{Gaussian process} if for any finite subset $\tau \subseteq T$ of its index set, the random variables $\rv{X}_\tau$ have a joint Gaussian distribution \cite{astrom_introduction_1971}.
\end{definition}
When using a Gaussian process $\rv{X}$ to model a function $f \colon A \to B$, the index set $T$ is assumed to be $A$ and all random variables are $B$-valued.
The random variable $\rv{X_a}$ then models the function value $f(a)$ for all $a \in A$.
Sampling a trajectory from $\rv{X}$ corresponds to sampling one possible function $f^\ast$.

Similar to the finite case, the random variables have a dependency structure.
Instead of a mean vector $\mat{\mu}$ and a covariance matrix $\mat{\Sigma}$, a Gaussian process is completely determined by a \emph{mean function} $\mu_f(a) = \Moment{\E}{f(a)}$ and a \emph{covariance function}
\begin{align}
    \K(a, a^\prime) &\coloneqq \Moment{\E}{(f(a) - \mu_f(a))(f(a^\prime) - \mu_f(a^\prime))} \\
    &= \Moment{\cov}{f(a), f(a^\prime)} \\
    &= \Moment{\cov}{\rv{X_a}, \rv{X_{a^\prime}}}
\end{align}
with $a, a^\prime \in A$.
The mean function encodes the point wise mean over all trajectories which could be sampled from $\rv{X}$.
The covariance function is also called a \emph{kernel} and describes the interaction between different parts of the function.
A function which is distributed according to a Gaussian process is denoted as $f \sim \GP\Cond{\mu_f, \K}$.

For convenience it is often assumed that the prior mean function $\mu_f$ is constant zero.
This assumption is without loss of generality \cite{rasmussen_gaussian_2006} since in the case the mean function is known to be different to zero, the observations $\left( \mat{X}, \mat{y} \right)$ can be transformed to $\mat{y^\prime} = \mat{y} - \mu(\mat{X})$.
The Gaussian process based on the observations $\left( \mat{X}, \mat{y^\prime} \right)$ then only models the differences to the mean function.
It is the covariance functions which encode the assumptions about the underlying function.

\subsection{Kernels}
Gaussian processes are collections of random variables, any finite subset of which have a joint multivariate Gaussian distribution.
For any pair $(\rv{X_i}, \mat{X_j})$ of these random variables, their covariance is given by the covariance function $\Moment{\cov}{\rv{X_i}, \rv{X_j}} = \K(i, j)$.
The pairwise covariances in a multivariate Gaussian $\Gaussian{\mat{\mu}, \mat{\Sigma}}$ are given by its \emph{covariance matrix} $\mat{\Sigma}$.
For any finite set of random variables, the matrix obtained by pairwise application of the covariance function is called the \emph{Gram matrix}.
\begin{definition}[Gram Matrix]
    Given a non-empty set $A$, a function $\K \colon A^2 \to \R$ and two sets $X = \Set*{x_i \in A \with i \in [n]}$ and $Y = \Set*{y_j \in A \with j \in [m]}$.
    The $n \times m$ matrix
    \begin{align}
        \K(X, Y) = \mat{K_{XY}} \coloneqq \bigg( \K(x_i, y_j) \bigg)_{\substack{i \in [n], \\ j \in [m]}}
    \end{align}
    is called the \emph{Gram matrix} of $\K$ with respect to $X$ and $Y$ \cite{scholkopf_learning_2002}.
\end{definition}
In order for the Gram matrix to be a valid covariance matrix $\mat{\Sigma}$ of a Gaussian distribution, it must be positive definite.
\emph{Kernels} are functions which fulfill the property that for every possible subset of random variables, or more generally every set of elements in their domain, their induced Gram matrix is positive definite.
\begin{definition}[Kernel]
    Given a non-empty set $A$, a function
    \begin{align}
        \K \colon A^2 \to \R
    \end{align}
    is called a \emph{(positive definite) kernel} or \emph{covariance function}, if for any finite subset $X \subseteq A$, the Gram matrix $\K(X, X)$ is positive definite \cite{scholkopf_learning_2002}.
\end{definition}
The kernel is crucial in encoding the assumptions about the function a Gaussian process should estimate.
It is a measure of \emph{similarity} of different points in the observed data and of new points to be predicted.
A natural assumption to make is to assume that the closer together in the domain two points lie, the more similar their function values will be.
Similarly, to predict a test point, training points close to it are probably more informative than those further away.

But closeness is not the only possible reason two points could be similar.
Assume a function to be modeled which is a possibly noisy sinusoidal wave with a known frequency.
Then, two points which are a multiple of wavelengths apart should also have similar function values.
A kernel which is not only dependent on the distance between two points but also their position in the input space is called \emph{non-stationary}.
A simple example of such a non-stationary kernel is the linear kernel.
\begin{definition}[Linear Kernel]
    For a finite dimensional euclidean vector space $\R^d$, the \emph{linear kernel} is defined as
    \begin{align}
        \K_{\text{linear}}(\mat{x}, \mat{y}) \coloneqq \mat{x}\tran \mat{y} = \left\langle \mat{x}, \mat{y}\right\rangle.
    \end{align}
\end{definition}
Consider a function $f \colon \R \to \R$ which is distributed according to a Gaussian process with the linear kernel $f \sim \GP\Cond{\mat{0}, \K_{\text{linear}}}$.
According to the definition of Gaussian processes, for any two input numbers $x$, $y \in \R$ their corresponding random variables $\rv{f_x}$ and $\rv{f_y}$ have a joint Gaussian distribution
\begin{align}
    \begin{pmatrix}
        \rv{f_x} \\ \rv{f_y}
    \end{pmatrix} \sim \Gaussian*{\mat{0}, \begin{bmatrix}
        \K(x, x) & \K(x, y) \\
        \K(y, x) & \K(y, y)
    \end{bmatrix}}
\end{align}
where $\K = \K_{\text{linear}}$.
Assuming that both $x$ and $y$ are not equal to zero, the correlation coefficient $\corr$ of these two variables is given by
\begin{align}
    \Moment{\corr}{\rv{f_x}, \rv{f_y}} &= \frac{\Moment{\cov}{\rv{f_x}, \rv{f_y}}}{\sqrt{\Moment{\var}{\rv{f_x}\vphantom{\rv{f_y}}}}\sqrt{\Moment{\var}{\rv{f_y}}}} \\
    &= \frac{\K(x, y)}{\sqrt{\K(x, x)} \sqrt{\K(y, y)}} = \frac{xy}{\sqrt{\vphantom{y^2}x^2}\sqrt{\vphantom{y^2}y^2}} \in \left\{ -1, 1 \right\}.
\end{align}
A correlation coefficient of plus or minus one implies that the value of one of the random variables is a linear function of the other.
Any function drawn from this Gaussian process, such as the ones shown in \cref{fig:gp_samples:linear}, is therefore a linear function.
This observation generalizes to higher dimensions \cite{rasmussen_gaussian_2006}.
Gaussian process regression with a linear kernel is equivalent to Bayesian linear regression.
\begin{figure}[tb]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.2\textheight]{Samples of Linear GP}
        \caption{Samples of Linear GP}
        \label{fig:gp_samples:linear}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.2\textheight]{Samples of RBF GP, good hyperparameters}
        \caption{Samples of RBF GP, good hyperparameters}
        \label{fig:gp_samples:rbf_normal}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.2\textheight]{Samples of RBF GP, noisy hyperparameters}
        \caption{Samples of RBF GP, noisy hyperparameters}
        \label{fig:gp_samples:rbf_noisy}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.2\textheight]{Samples of RBF GP, short lengthscale hyperparameters}
        \caption{Samples of RBF GP, short lengthscale hyperparameters}
        \label{fig:gp_samples:rbf_lengthscale}
    \end{subfigure}
    \caption{GP samples}
    \label{fig:gp_samples}
\end{figure}

Because of its restrictiveness, the linear kernel is not very relevant for real-world applications of Gaussian processes.
As described above, the similarity of two data points $\mat{x}$ and $\mat{y}$ is often dependent on their relative position.
A kernel which is a function of $\mat{x} - \mat{y}$ is called \emph{stationary} and is invariant to translations in the input space.
The most important stationary kernel is the squared exponential kernel.
\begin{definition}[Squared Exponential Kernel]
    \label{def:rbf_kernel}
    For a finite dimensional euclidean vector space $\R^d$, the \emph{squared exponential kernel} (or \emph{RBF kernel}) is defined as
    \begin{align}
        \K_{\text{SE}}(\mat{x}, \mat{y}) \coloneqq \sigma_f^2 \cdot \exp\!\left( -\frac{1}{2} (\mat{x} - \mat{y})\tran \mat{\Lambda}^{-1} (\mat{x} - \mat{y}) \right).
    \end{align}
    The parameter $\sigma_f^2 \in \R_{>0}$ is called the \emph{signal variance} and $\mat{\Lambda} = \diag(l_1^2, \dots, l_d^2)$ is a diagonal matrix of the squared \emph{length scales} $l_i \in \R_{>0}$.
\end{definition}
The similarity of two data points approaches one when they are close together and for larger distances approaches zero with exponential drop off.
It can be shown that this kernel represents all infinitely differentiable functions \cite{rasmussen_gaussian_2006}.
Gaussian processes with this covariance function are universal function approximators.

The squared exponential kernel is dependent on multiple parameters which influence its behaviour.
In contrast to weight parameters in linear regression or constants in physical models, these parameters do not specify the estimated function but rather the prior belief about this function.
In order to separate the two, they are called \emph{hyperparameters}.
The vector of all hyperparameters in a model is called $\mat{\theta}$.

The hyperparameters of the RBF kernel describe the expected dynamic range of the function.
The signal variance $\sigma_f^2$ specifies the average distance of function values from the mean function.
The different length scale parameters $l_i$ roughly specify the distance of data points along their respective axis required for the function values to change considerably.
\Cref{fig:gp_samples} compares sample functions drawn from Gaussian processes with squared exponential kernels with different hyperparameters.

These plots show continuous functions being drawn from their respective processes.
It is however only possible to evaluate the Gaussian process at finitely many points and then connect the resulting samples.
Drawing the function values of a finite amount of sample input points $\mat{X_\ast}$ from a Gaussian process prior is equivalent to drawing a sample from the Gaussian $\Gaussian{\mat{0}, \mat{K_\ast}}$ where $\mat{K_\ast}$ is a short hand notation for $\K(\mat{X_\ast}, \mat{X_\ast})$.

\subsection{Predictions and Posterior}
In order to use Gaussian processes for regression, it is necessary to combine observations with a Gaussian process prior $f \sim \GP\Cond{\mat{0}, \K}$ in order to obtain a predictive posterior.
The $N$ data points observed are denoted as $\D = \left( \mat{X}, \mat{y} \right)$ with $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma_n^2 \Eye}$ and $\abs{\mat{y}} = N$.
The observed function values $\mat{y}$ are assumed to not be the true latent function values $\mat{f} = f(\mat{X})$ but rather have some additive Gaussian noise which is independent and identically distributed for all observations.
The variance of this noise $\sigma_n^2$ is a hyperparameter of the Gaussian process model.

Assuming further that given the latent function and the input points, the observations are conditionally independent, their likelihood is given by
\begin{align}
    \Prob{\mat{y} \given f, \mat{X}} = \Prob{\mat{y} \given \mat{f}} &= \prod_{i = 1}^N \Prob{y_i \given f_i} \\
    &= \prod_{i = 1}^N \Gaussian{y_i \given f_i, \sigma_n^2} = \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2 \Eye}
\end{align}
because of the assumed noise model.
Given some vector of hyperparameters $\mat{\theta}$, the definition of Gaussian processes yields a joint Gaussian distribution for the latent function values $\mat{f}$ given by
\begin{align}
    \Prob{\mat{f} \given \mat{X}, \mat{\theta}} = \Gaussian*{\mat{f} \given \mat{0}, \mat{K_N}}
\end{align}
where $\mat{K_N} = \K(\mat{X}, \mat{X})$ denotes the Gram matrix of the observed data.
Combining the two distributions according to the law of total probability yields the probability distribution of the outputs conditioned on the inputs and is given by
\begin{align}
    \begin{split}
        \label{eq:gp_marginal_likelihood}
        \Prob{\mat{y} \given \mat{X}, \mat{\theta}} &= \int \Prob{\mat{y} \given \mat{f}} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f} \\
        &= \int \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2 \Eye} \Gaussian*{\mat{f} \given \mat{0}, \mat{K_N}} \diff \mat{f} \\
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{K_N} + \sigma_n^2 \Eye}.
    \end{split}
\end{align}
Note that this distribution is obtained by integrating over all possible latent function values $\mat{f}$ and thereby taking all possible function realizations into account.
This integration is called the \emph{marginalization} of $\mat{f}$.
The closed form solution of the integral is obtained using well-known results about Gaussian distributions which are for example detailed in \cite{petersen_matrix_2008}.

Now consider a set of test points $\mat{X_\ast}$ for which the predictive posterior should be obtained.
By definition the latent function values $\mat{f}$ of the training set and the latent function values of the test set $\mat{f_\ast} = f(\mat{X_\ast})$ have the joint Gaussian distribution
\begin{align}
    \Prob*{\begin{pmatrix}
        \mat{f} \\
        \mat{f_\ast}
    \end{pmatrix} \given \mat{X}, \mat{X_\ast}, \mat{\theta}} &= \Gaussian*{\begin{pmatrix}
        \mat{f} \\
        \mat{f_\ast}
    \end{pmatrix} \given \mat{0}, \begin{bmatrix}
        \mat{K_N} & \mat{K_{N\ast}} \\
        \mat{K_{\ast N}} & \mat{K_{\ast}}
    \end{bmatrix}}.
\end{align}
Adding the noise model to this distribution leads to the joint Gaussian of training outputs $\mat{y}$ and test outputs $\mat{f_\ast}$ which is given by
\begin{align}
    \Prob*{\begin{pmatrix}
        \mat{y} \\
        \mat{f_\ast}
    \end{pmatrix} \given \mat{X}, \mat{X_\ast}, \mat{\theta}} &= \Gaussian*{\begin{pmatrix}
        \mat{y} \\
        \mat{f_\ast}
    \end{pmatrix} \given \mat{0}, \begin{bmatrix}
        \mat{K_N} + \sigma_n^2 \Eye & \mat{K_{N\ast}} \\
        \mat{K_{\ast N}} & \mat{K_{\ast}}
    \end{bmatrix}}.
\end{align}

In this distribution, the training outputs $\mat{y}$ are known.
The predictive posterior for the test outputs $\mat{f_\ast}$ can be obtained by applying the rules for marginalization of multivariate Gaussians, yielding another Gaussian distribution.
\begin{lemma}[GP predictive posterior]
    \label{lem:gp_posterior}
    Given a latent function with a Gaussian process distribution $f \sim \GP(\mat{0}, \K)$ and $N$ training points $\mat{X}$ with noisy observations of the form $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma_n^2 \Eye}$.
    The predictive posterior $\mat{f_\ast}$ of the test points $\mat{X_\ast}$ is then given by
    \begin{align}
        \Prob{\mat{f_\ast} \given \mat{X}, \mat{y}, \mat{X_\ast}} &= \Gaussian*{\mat{f_\ast} \given \mat{\mu_\ast}, \mat{\Sigma_\ast}} \text{, where} \\
        \mat{\mu_\ast} &= \mat{K_{\ast N}} \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{y} \\
        \mat{\Sigma_\ast} &= \mat{K_\ast} - \mat{K_{\ast N}} \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{K_{N\ast}}.
    \end{align}
\end{lemma}
\begin{figure}[tb]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.25\textheight]{GP Prior}
        \caption{GP Prior}
        \label{fig:gp_posterior:prior}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.25\textheight]{GP Posterior}
        \caption{GP Posterior}
        \label{fig:gp_posterior:posterior}
    \end{subfigure}
    \caption{GP posterior}
    \label{fig:gp_posterior}
\end{figure}

This predictive posterior makes it possible to evaluate the function approximation based on the input at arbitrary points in the input space.
Since any set of these points always has a joint Gaussian distribution, the predictive posterior defines a new Gaussian process, which is the posterior Gaussian process given the observations.
This posterior process $\GP(\mu_\text{post}, \K_\text{post})$ has new mean and covariance functions given by
\begin{align}
    \mu_\text{post}(\mat{a}) &= \K(\mat{a}, \mat{X}) \left(\mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{y} \\
    \K_\text{post}(\mat{a}, \mat{b}) &= \K(\mat{a}, \mat{b}) - \K(\mat{a}, \mat{X}) \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \K(\mat{X}, \mat{b}).
\end{align}
Note that the posterior mean function is not necessarily the constant zero function.
\Cref{fig:gp_posterior} shows samples from a pair of prior and posterior Gaussian processes.

Computing the inverse $\left(\mat{K_N} + \sigma_n^2 \Eye \right)^{-1}$ costs $\Oh(N^3)$ but can be done as a preprocessing step since it is independent of the test points.
Predicting the mean of a single test point is a weighted sum of $N$ basis functions $\mu_\ast = \mat{K_{\ast N}} \mat{\beta}$ where $\mat{\beta} = \left(\mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{y}$ which can be precomputed.
After this precomputation, predicting the mean of a single test point costs $\Oh(N)$.
To predict its variance, it is still necessary to perform a matrix multiplication which costs $\Oh(N^2)$.
Since all of these operations are dependent on the number of training points, evaluating Gaussian processes on large data sets can be computationally expensive.
Before introducing sparse approximations with better asymptotic complexity, the next section deals with choosing good values for the vector of hyperparameters $\mat{\theta}$.

\subsection{Choosing Hyperparameters}
\label{sub:gp_hyperparameters}
In the previous section, the hyperparameters $\mat{\theta}$ were assumed to be known and constant, that is, the prior assumptions about the function to be estimated were fixed.
In this case, Gaussian processes do not have a training stage, since any test point can be predicted according to the predictive posterior.
Usually however, the correct choice of hyperparameters is not clear a priori.
A major advantage of Gaussian processes is the ability to select hyperparameters from training data directly instead of requiring a scheme such as cross validation.

In a fully Bayesian setup, the correct way to model uncertainty about hyperparameters is to assign them a prior $\Prob{\mat{\theta}}$ and marginalize it to derive the dependent distributions
\begin{align}
    \Prob{f} &= \int \Prob{f \given \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{\theta} \\
    \Prob{\mat{y} \given \mat{X}} &= \int \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{\theta}. \label{eq:theta_posterior_integration}
\end{align}
Updating the belief about the distribution of the hyperparameters then becomes part of the process of obtaining a posterior model.
A new distribution is obtained by combining the prior with the likelihood of the training data observed using Bayes' theorem:
\begin{align}
    \Prob{\mat{\theta} \given \mat{X}, \mat{y}} &= \frac{\Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}}}{\Prob{\mat{y} \given \mat{X}}} \\ &= \frac{\Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}}}{\int \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}} \diff \theta}
\end{align}
The integration required in \cref{eq:theta_posterior_integration} is very hard in practice \todo{Is there an easy argument for this which does not rely on the likelihood?}\cite[109]{rasmussen_gaussian_2006}, since $\mat{y}$ is a complicated function of $\mat{\theta}$.
Instead, a common approximation is to use a \emph{maximum-a-postiori (MAP)} estimate of the correct hyperparameters.
This estimate is obtained by maximizing $\Prob{\mat{\theta} \given \mat{X}, \mat{y}}$ and does not require evaluation of the denominator since it is constant.

For many choices of priors $\Prob{\mat{\theta}}$ this is still a hard problem.
But assuming a flat prior which assigns almost equal probability to all choices of hyperparameters, it holds that
\begin{align}
    \Prob{\mat{\theta} \mid \mat{X}, \mat{y}} &\propto \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \\
    &= \int \Prob{\mat{y} \given \mat{f}, \mat{\theta}} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f},
\end{align}
that is, the posterior distribution is proportional to the likelihood term and can be obtained using a maximum likelihood estimate on the \emph{marginal likelihood} after integrating out the function values $\mat{f}$.
Optimizing this term is called a \emph{type II maximum likelihood estimate (ML-II)}.

The marginal likelihood is an integral over a product of Gaussians obtained from the noise model and the distribution of function values according to the Gaussian process definition.
It is given by
\begin{align}
    \begin{split}
        \label{eq:gp_f_marginalization}
        \Prob{\mat{y} \mid \mat{X}, \mat{\theta}} &= \int \Prob{\mat{y} \given \mat{f}, \mat{\theta}} \Prob{\mat{f} \given \mat{\theta}} \diff \mat{f} \\
        &= \int \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2 \Eye} \cdot \Gaussian{\mat{f} \given \mat{0}, \mat{K_N}} \diff \mat{f} \\
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{K_N} + \sigma_n^2 \Eye}
    \end{split}
\end{align}
The solution of this integral is a Gaussian density function \cite{petersen_matrix_2008}.
For practical reasons, it is convenient to minimize the negative logarithm of the likelihood which is given by
\begin{align}
    \Ell(\mat{\theta}) = -\log\Prob{\mat{y} \given \mat{X}, \mat{\theta}} =
    \frac{1}{2} \mat{y\tran} \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{y} +
    \frac{1}{2} \log \abs{\mat{K_N} + \sigma_n^2 \Eye} +
    \frac{N}{2} \log(2\pi).
\end{align}
The estimation of hyperparameters is the solution of the optimization problem
\begin{align}
    \mat{\theta}^\ast &\in \argmin_{\mat{\theta}} \Ell(\mat{\theta})
\end{align}
and is calculated using standard approaches to non-convex optimization such as scaled conjugate gradient (SCD) techniques, since finding the derivatives of $\Ell$ is comparatively easy \cite{rasmussen_gaussian_2006}.
The computational complexity of evaluating the likelihood term and its derivatives is dominated by the inversion of $\mat{K_N} + \sigma_n^2 \Eye$.

Since this optimization scheme does not choose parameters of the function approximation directly but rather changes a small number of broad and high-level assumptions about it, overfitting does not tend to be a problem for Gaussian processes in general \cite{snelson_flexible_2007}.
The sparse approximation of Gaussian processes presented in the next section chooses a small number of points in the input space to represent a large training set.
The positions of these input points can be interpreted as hyperparameters to the original Gaussian process and induce a kernel function with many hyperparameters, where overfitting can become relevant.

\subsection{Sparse Approximations using Inducing Inputs}
A major drawback of Gaussian processes in real-world applications is their high computational cost for large data sets.
Assume a data set $(\mat{X}, \mat{y})$ with $\abs{\mat{y}} = N$, then the operations on a posterior Gaussian process are usually dominated by the inversion of the kernel matrix $\mat{K_N}$ which takes $\Oh(N^3)$ time.
While this is only a preprocessing step, the cost of predicting the mean and variance of one test point remains $\Oh(N)$ and $\Oh(N^2)$ respectively.
Additionally, these operations have a space requirement of $\Oh(N^2)$.
The goal of sparse approximations of Gaussian processes is to find model representations which avoid these quadratic complexities or at least restrict them to the training phase of finding hyperparameters.
This section introduces one type of approximation based on representing the complete data set through a smaller set of points.

The most simple approach to achieve this is to only use a small subset of $M \ll N$ \emph{inducing} points of the original training set and learn a normal Gaussian process.
This approach can work for data sets with a very high level of redundancy but does impose the problem of choosing an appropriate subset.
While choosing a random subset can be effective \cite{snelson_flexible_2007}, the optimal choice is dependent on the hyperparameters and both should therefore be chosen in a joint optimization scheme.
This is a combinatorical optimization problem which can be very hard to solve in practice since the function to be optimized is very non-smooth.

To overcome this problem, \emph{sparse pseudo input Gaussian processes (SPGP)} \cite{snelson_flexible_2007} lift the restriction of choosing inducing points from the training set and instead allow arbitrary positions in the input space.
The original data set is replaced by a \emph{pseudo data set} $(\ps{\mat{X}}, \ps{\mat{f}})$ of \emph{pseudo inputs} $\ps{\mat{X}}$ and \emph{pseudo targets} $\ps{\mat{f}} = f(\ps{\mat{X}})$ which are equal true latent function $f \sim \GP(\mat{0}, \K)$.
Since they are not true observations, they are assumed to be noise-free.

With known positions of the pseudo inputs and fixed hyperparameters $\mat{\theta}$, the predictive posterior of a Gaussian process based on this pseudo data set for test points $(\mat{X_\ast}, \mat{f_\ast})$ is given by
\begin{align}
    \Prob{\mat{f_\ast} \given \mat{X_\ast}, \ps{\mat{X}}, \ps{\mat{f}}, \mat{\theta}} &= \Gaussian{\mat{K_{\ast M}}\mat{K_M}^{-1} \ps{\mat{f}}, \mat{K_\ast} - \mat{K_{\ast M}} \mat{K_M}^{-1} \mat{K_{M \ast}}}
\end{align}
according to \cref{lem:gp_posterior} with the notation $\mat{K_M} = \K(\ps{\mat{X}}, \ps{\mat{X}})$ meaning the Gram matrix of the pseudo inputs compared to $\mat{K_N} = \K(\mat{X}, \mat{X})$, the Gram matrix of the original training data.

The true data set is independent given the latent function and can therefore be assumed independent given the pseudo data set which should be a good representation of it.
The likelihood of the original data under the Gaussian process trained on the pseudo data set is given by
\begin{align}
    \Prob{\mat{y} \given \mat{X}, \ps{\mat{X}}, \ps{\mat{f}}, \mat{\theta}} &= \prod_{i=1}^N \Prob{y_n \given \mat{x_n}, \ps{\mat{X}}, \ps{\mat{f}}, \mat{\theta}} \\
    &= \prod_{i=1}^N \Gaussian*{y_n \given \mat{K_{n M}}\mat{K_M}^{-1} \ps{\mat{f}}, \mat{K_n} - \mat{K_{n M}} \mat{K_M}^{-1} \mat{K_{M n}} + \sigma_n^2} \\
    &= \Gaussian*{\mat{y} \given \mat{K_{N M}}\mat{K_M}^{-1} \ps{\mat{f}}, \diag\left( \mat{K_N} - \mat{K_{N M}} \mat{K_M}^{-1} \mat{K_{M N}} \right) + \sigma_n^2 \Eye} \\
    &= \Gaussian*{\mat{y} \given \mat{K_{N M}}\mat{K_M}^{-1} \ps{\mat{f}}, \diag\left( \mat{K_N} - \mat{Q_N} \right) + \sigma_n^2 \Eye}
\end{align}
with $\mat{Q_N} \coloneqq \mat{K_{N M}} \mat{K_M}^{-1} \mat{K_{M N}}$.
The additive term $\sigma_n^2$ comes from the noise model assumed about the observations $\mat{y}$ in the original data set.
Rather than using maximum likelihood on this term to learn the complete pseudo data set $(\ps{\mat{X}}, \ps{\mat{f}})$, the pseudo targets $\ps{\mat{f}}$ can be marginalized.
This can be combared to the marginalization of the latent function values $\mat{f}$ in the derivation of Gaussian processes in \cref{eq:gp_f_marginalization}.
Assuming the pseudo targets to be distributed very similarly to the real data, a reasonable prior for them is given by
\begin{align}
    \Prob{\ps{\mat{f}} \given \ps{\mat{X}}} = \Gaussian{\ps{\mat{f}} \given \mat{0}, \mat{K_M}}.
\end{align}

The marginalization is stated as the integral of a product of two Gaussian distributions which has a closed form solution and is given by
\begin{align}
    \Prob{\mat{y} \given \mat{X}, \ps{\mat{X}}, \mat{\theta}} &= \int \Prob{\mat{y} \given \mat{X}, \ps{\mat{X}}, \ps{\mat{f}}, \mat{\theta}} \Prob{\ps{\mat{f}} \given \ps{\mat{X}}} \diff \ps{\mat{f}} \\
    &= \int \Prob{\mat{y} \given \mat{X}, \ps{\mat{X}}, \ps{\mat{f}}, \mat{\theta}} \Gaussian{\ps{\mat{f}} \given \mat{0}, \mat{K_M}} \diff \ps{\mat{f}} \\
    &= \Gaussian*{\mat{y} \given \mat{0}, \mat{K_{NM}} \mat{K_M}^{-1} \mat{K_M} \left( \mat{K_{NM}} \mat{K_M}^{-1} \right)\tran + \diag\left( \mat{K_N} - \mat{Q_N} \right) + \sigma_n^2 \Eye} \\
    &= \Gaussian*{\vphantom{\left( \mat{K_M}^{-1} \right)\tran} \mat{y} \given \mat{0}, \mat{Q_N} + \diag\left( \mat{K_N} - \mat{Q_N} \right) + \sigma_n^2 \Eye}.
\end{align}
This \emph{SPGP marginal likelihood} can be interpreted as the marginal likelihood of a Gaussian process given the original data set $(\mat{X}, \mat{y})$ in \cref{eq:gp_marginal_likelihood}.
In this Gaussian process, the original kernel $\K$ is replaced by the kernel $\K_{\text{SPGP}}$.
With $\Ind$ denoting the indicator function it is defined as
\begin{align}
    \Q(\mat{a}, \mat{b}) &\coloneqq \mat{K_{aM}} \mat{K_M}^{-1} \mat{K_{Mb}} \\
    \K_{\text{SPGP}}(\mat{a}, \mat{b}) &\coloneqq \Q(\mat{a}, \mat{b}) + \Indicator{\mat{a} = \mat{b}} \left( \K(\mat{a}, \mat{b}) - \Q(\mat{a}, \mat{b}) \right).
\end{align}
This kernel is equal to $\K$ when both arguments are identical and equal to $\Q$ everywhere else.
For well-chosen pseudo inputs, $\mat{Q_N}$ is a low-rank approximation of $\mat{K_N}$ \cite{snelson_flexible_2007}.
Because of this identity, an SPGP is a normal Gaussian process with an altered kernel function.
The pseudo inputs $\ps{\mat{X}}$ are hidden in the kernel matrix $\mat{K_M}$ and are additional hyperparameters to this kernel.
This observation directly yields the SPGP predictive posterior using \cref{lem:gp_posterior}.
\begin{lemma}[SPGP predictive posterior]
    \label{lem:spgp_posterior}
    Given a latent function with a sparse pseudo-input Gaussian process distribution $f \sim \GP(\mat{0}, \K_{\text{SPGP}})$, $N$ training points $\mat{X}$ with noisy observations of the form $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma_n^2 \Eye}$ and $M$ positions of pseudo-inputs $\ps{\mat{X}}$.
    The predictive posterior $\mat{f_\ast}$ of the test points $\mat{X_\ast}$ is then given by
    \begin{align}
        \Prob{\mat{f_\ast} \given \mat{X_\ast}, \mat{X}, \mat{y}, \ps{\mat{X}}} &= \Gaussian*{\mat{f_\ast} \given \mat{\mu_\ast}, \mat{\Sigma_\ast}} \text{, where} \\
        \mat{\mu_\ast} &= \mat{Q_{\ast N}} \left( \mat{Q_N} + \diag(\mat{K_N} - \mat{Q_N}) + \sigma_n^2 \Eye \right)^{-1} \mat{y} \\
        \mat{\Sigma_\ast} &= \mat{K_\ast} - \mat{Q_{\ast N}} \left( \mat{Q_N} + \diag(\mat{K_N} - \mat{Q_N}) + \sigma_n^2 \Eye \right)^{-1} \mat{Q_{N \ast}}.
    \end{align}
    and $\mat{Q_N} \coloneqq \mat{K_{N M}} \mat{K_M}^{-1} \mat{K_{M N}}$.
\end{lemma}
\begin{figure}[t]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.3\textheight]{GP on some Data}
        \caption{GP on some Data}
        \label{fig:spgp_example:gp}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.3\textheight]{SPGP on some Data}
        \caption{SPGP on some Data}
        \label{fig:spgp_example:spgp}
    \end{subfigure}
    \caption{GP vs. SPGP}
    \label{fig:spgp_example}
\end{figure}

The predictive distribution as written in the previous equations can easily be compared to the predictive posterior of Gaussian processes in \cref{lem:gp_posterior}.
They do however still involve the inversion of matrices of size $N \times N$ and therefore do not offer computational improvements.
Using the matrix inversion lemma \cite{petersen_matrix_2008}, they can be rewritten to the form\todo{fix line spacing in equation}
\begin{align}
    \mat{\mu_\ast} &= \mat{K_{\ast M}} \mat{B}^{-1} \mat{K_{MN}} \left( \diag(\mat{K_N} - \mat{Q_N}) + \sigma_n^2 \Eye \right)^{-1} \mat{y} \\
    \mat{\Sigma_\ast} &= \mat{K_\ast} - \mat{K_{\ast M}} \left( \mat{K_M}^{-1} - \mat{B}^{-1} \right) \mat{K_{M \ast}} \\
    \mat{B} &= \mat{K_M} + \mat{K_{MN}} \left( \diag(\mat{K_N} - \mat{Q_N}) + \sigma_n^2 \Eye \right)^{-1} \mat{K_{NM}},
\end{align}
which only involves the inversion of $M \times M$ matrices and one diagonal $N \times N$ matrix.
Implemented this way, the calculation of all terms independent of the test points has a complexity of $\Oh(NM^2)$ and predicting means and variances takes $\Oh(M)$ and $\Oh(M^2)$ time respectively.
The space requirement also drops to $\Oh(M^2)$.

Since the positions of the pseudo inputs $\ps{\mat{X}}$ are additional hyperparameters in $\K_{\text{SPGP}}$, they can be chosen together with the hyperparameters of the original kernel $\mat{\theta}$ using maximum likelihood as explained in \cref{sub:gp_hyperparameters}.
Because they can be placed anywhere in the input space, the derivatives of the marginal likelihood by their positions are smooth functions \cite{snelson_sparse_2005}.
This optimization chooses the positions in such a way that together with appropriate other hyperparameters, the original data is represented as good as possible.
The curse of dimensionality of requiring exponentially many points in a grid given the number of input dimensions does therefore not necessarily apply to the number of pseudo inputs needed in an SPGP approximation.
\Cref{fig:spgp_example} shows that a surprisingly small number of pseudo inputs can be enough to represent the dynamics of a function.

With a large number of pseudo inputs, the number of hyperparameters can grow large.
This implies the danger of overfitting since the altered Gaussian process has no direct connection to the original Gaussian process over the complete training set.
As an alternative to the optimization of the SPGP marginal likelihood, \citeauthor{titsias_variational_2009} proposed a variational approach \cite{titsias_variational_2009} which optimizes a lower bound of the marginal likelihood of the original Gaussian process.
Instead of choosing a sparse model which explains the data well, this optimization chooses a sparse model which is as close as possible to the original full GP.
Since this strategy leads to better convergence and more robust results in practice, these variational sparse approximation of full Gaussian processes is used to model transition dynamics within this thesis.

In order to solve the control problem of the bicycle benchmark, the next step after modeling the transition dynamics using Gaussian processes is to find a policy representation.
Instead of a closed form representation of the policy, the choice of which action to take is made by directly optimizing over the value function using particle swarm optimization.
This technique is presented in the next section.

\section{Particle Swarm Optimization Policy}
Given a starting state, the way to evaluate a policy in the general reinforcement learning setup is to evaluate its value function as defined in \cref{def:value_function}.
Since the value is defined as the sum of expected rewards of all future states in the time horizon, calculating it correctly requires knowledge of the true transition function or, in the case of deterministic dynamics, interaction with the system.
Model-based reinforcement learning is centered around learning a model of the transition function and use this representation to approximate the value function.

This allows simulated interaction with the system and can be used to choose optimal parameters for a parametric closed-form policy formulation.
Similar to the comparison of parametric and non-parametric models in \cref{sec:gp_regression}, the performance of such policies heavily depends on the choice of function representation.
In a Bayesian setting, the uncertainties about the transition model should be propagated through to an uncertainty about the choice of policy parameters and finally marginalized to obtain a distribution over good actions to take.
In order to keep this process computationally feasible, trade-offs must be made in the flexibility of policy representations.
\citeauthor{deisenroth_pilco:_2011} recommend the use of rather limited linear policies or non-linear representations via RBF networks \cite{deisenroth_pilco:_2011}.

This thesis chooses the non-parametric approach of the \emph{particle swarm optimization policy} described by \citeauthor{hein_reinforcement_2016} in \cite{hein_reinforcement_2016}.
For a finite time horizon $T$, finding an optimal parametric policy using the approximated value function means finding policy parameters which result in good choices for actions at every time step.
In contrast to this, PSO-P directly examines the \emph{action-value-function $\AVlu$} given by
\begin{align}
    \label{eq:action_value_function}
    \AVlu_{\mat{s_0}} \colon \left\{
        \begin{aligned}
            \Ah^T &\to \R \\
            \left( \mat{a_0}, \dots, \mat{a_{T-1}} \right) &\mapsto \Moment*{\E}{\sum_{t=0}^T \gamma^t \Rwd(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}}
        \end{aligned}
    \right.
\end{align}
where $f$ denotes a model of the transition function, $\mat{s_0}$ denotes a state and $\gamma$ is the discount factor.
PSO-P optimizes over the vector of all possible actions in the time horizon and then applies the best action found for time step zero to the system.
\begin{definition}[PSO-P]
    \label{def:psop}
    The \emph{particle swarm optimization policy (PSO-P)} \cite{hein_reinforcement_2016} chooses actions via direct optimization of the action value function $\AVlu$ and is defined as
    \begin{align}
        \pi_{\text{PSO-P}} &: \left\{
            \begin{aligned}
                \Es &\to \Ah\\
                \mat{s} &\mapsto \mat{a^\ast}_0 \text{, where } \mat{a^\ast} \in \argmax_{\mat{a} \in \Ah^T} \AVlu_{\mat{s}}(\mat{a})
            \end{aligned}
        \right..
    \end{align}
\end{definition}
The optimization over $\AVlu$ at time step $t$ yields a vector of optimal actions for the complete time horizon.
The PSO-Policy still only applies the first of these actions and repeats the optimization at time step $t+1$.
This is done to both reduce model bias and profit from the more accurate information about $\mat{s_{t+1}}$ from the system.

The action value function is itself not probabilistic but is defined as an expected value dependent on all future states.
Since the transition model is Bayesian, evaluating this expected values implies iterated marginalizations of beliefs about both model hyperparameters and intermediate states.
The dependency of the action value on the different actions is therefore very complex and finding their gradients for the optimization is analytically intractable.

\emph{Particle swarm optimization} is the heuristic technique which is used in {PSO-P} to solve this non-convex optimization problem and is not dependent on gradients.
It works by creating a set of candidate solutions in the search space.
These particles move through the search space with the direction being dependent on their own function value and the function values of the other particles in the swarm.
This section introduces a basic version of the algorithm and discusses the different choices of parameters.

\subsection{Basic Particle Swarm Optimization}
To choose an appropriate action for the current state, PSO-P needs to solve a non-convex optimization problem.
Classical non-linear optimization schemes such as the scaled conjugated gradient technique or Newton methods \cite{press_numerical_2007} rely on the evaluation of the first or even second derivatives of the objective function.
In the setting of optimizing the action value function with respect to all actions at the different time steps, finding these gradients is a hard problem.

Particle swarm optimization is a heuristic technique which does not assume knowledge about the derivatives of the objective function and is therefore called \emph{gradient-free}.
It is a population-based optimization approach, where a number of solution candidates move through the domain of the target function in search of a good solution.
The most well-known class of population-based approaches, which are often inspired by nature, are evolutionary or genetic algorithms.
Genetic algorithms mimic natural selection by evaluating a generation of individuals and allowing the most successful to survive and recombine, giving rise to a new generation of candidates.

In contrast, the set of individuals, or \emph{swarm}, remains constant in PSO.
Instead of implementing survival of the fittest, PSO is based on social interaction.
Every individual, or \emph{particle}, flies in the search space with a velocity which is dynamically adjusted according to its own past experience and the experience of the other particles in the swarm.
While they are initialized randomly throughout the space, they are expected to collapse on a single point which, ideally, should be the optimum.

More formally, PSO is mostly used in a non-convex optimization setting.
Given some function $f \colon \X \to \R$, the problem is to find an $\mat{x^\ast} \in \X$ such that
\begin{align}
    \mat{x^\ast} \in \argmax_{\mat{x} \in \X} f(x).
\end{align}
While variantes of PSO exist which can handle arbitrary constraints \cite{engelbrecht_fundamentals_2006}, the basic variant presented in this thesis assumes that the domain $\X$ of $f$ is a set of the form
\begin{align}
    \X \coloneqq \Set*{\mat{x} \in \R^d \with x^{\text{min}}_i \leq x_i \leq x^{\text{max}}_i}.
\end{align}
This set of feasible points $\X$ is an axis-parallel cuboid in the finite dimensional real vector space $\R^d$.
The boundaries of this cuboid are defined by the vectors $\mat{x^{\text{min}}}$ and $\mat{x^{\text{max}}}$ which are both in $\R^d$ and specify the minimal and maximal value for every dimension.

The swarm of a PSO run consists of a finite set $\Pe$ of particles.
These particles each have a position and velocity which are both updated for all particles simultaneously and at discrete time steps.
The set of all time steps $\Tee$ is usually considered to be the natural numbers.
At every such time step, the objective function value is evaluated for every particle.
Since PSO models social interaction and communication, every particle has a neighbourhood of particles it communicates with.
This communication influences the particle's velocity as it is attracted by particles with good performance.
\begin{definition}[PSO Instance]
    An instance of the \emph{particle swarm optimization (PSO)} scheme on the objective function $f \colon \R^d \to \R$ is defined by the set of particles $\Pe$.
    For every time step in $\Tee$, the particles' positions and velocities are given by the functions
    \begin{align}
        \Pos &\colon \Pe \times \Tee \to \R^d \text{\quad and} \\
        \Vel &\colon \Pe \times \Tee \to \R^d
    \end{align}
    respectively. The position of particle $i$ at time step $t$ is denoted as $\Pos_i(t)$ and its velocity as $\Vel_i(t)$. A particle can be influenced by the set of its neighbours given by the neighbourhood function
    \begin{align}
        \Neigh \colon \Pe \to \Powerset{\Pe}.
    \end{align}
\end{definition}

The positions and velocities of the particles are defined using dynamic programming.
The bounded search space allows for a uniform initial distribution of positions and velocities which are, for every particle $i \in \Pe$, defined as
\begin{align}
    \Pos_i(0), \Vel_i(0) \sim \mat{x^{\text{min}}} + \Uniform{0, 1} \cdot \left( \mat{x^{\text{max}}} - \mat{x^{\text{min}}} \right),
\end{align}
where $\Uniform{0, 1}$ denotes the standard uniform distribution.
The update of the positions only depends on the particle's current state, such that for every $t \in \Tee$ it is defined as
\begin{align}
    \Pos_i(t+1) \coloneqq \Pos_i(t) + \Vel_i(t).
\end{align}
This operation can move particles out of the set of feasible points.
\citeauthor{engelbrecht_fundamentals_2006} discusses multiple possible boundary conditions such as circular algebra or reflection \cite{engelbrecht_fundamentals_2006}.
The most simple solution is to stop the particle at the boundary.

Velocities can be thought of as the result of forces pulling on the particles.
Ideally, these forces would originate from the optimum of the function, but this position is unknown.
In classical optimization schemes, gradients are used as the forces as they point to local extrema.
Since gradients are not available, the particles must rely on other sources of information.
At the first time step, no information about the function or the search space is available, so particle velocities are initialized randomly.
At later steps, the particles have already visited parts of the search space and have gained some knowledge.

This knowledge is represented by the personal best position $\Cog$ of every particle, which is the best position in the search space the particle has visited since the first time step.
A particle is pulled towards its own personal best position since for the single individual, it is the best guess for the position of the maximum.
It is defined as
\begin{align}
    \Cog_i(t) &\coloneqq \Pos_i \left(\min\argmax_{t^\prime \in [t]} f(\Pos_i(t^\prime)) \right).
\end{align}
Since the particles are initialized randomly in the search space, most of these personal best positions are expected to not be very good.
Particles at bad parts of the search space should communicate with their neighbours and be pulled towards the most successful.
For every particle, the best position seen by any of its neighbours $\Soc$ is defined as
\begin{align}
    \Soc_i(t) &\coloneqq \Cog_{n^\ast}(t) \text{, where} \\
    n^\ast &\in \argmin_{n \in \Neigh(i)} f(\Cog_n(t)).
\end{align}

The update of the velocity of a particle is a linear combination of these two components and the previous velocity in order to simulate inertia.
It is defined as
\begin{align}
    \label{eq:pso_velocity}
    \Vel_i(t+1) \coloneqq
        \underbracket[1pt]{\vphantom{\Cog_i}\omega \cdot \Vel_i(t)}_{\text{inertia}}
        + \underbracket[1pt]{\gamma_c \cdot r_c \cdot \left( \Cog_i(t) - \Pos_i(t) \right)}_{\text{cognitive component}}
        + \underbracket[1pt]{\gamma_s \cdot r_s \cdot \left( \Soc_i(t) - \Pos_i(t) \right)}_{\text{social component}}.
\end{align}
The real constants $\omega$, $\gamma_c$ and $\gamma_s$ weigh the relative influences of the different components and $r_c, r_s \sim \Uniform{0, 1}$ introduce a stochastic element to the algorithm which enables some random exploration after the first time step.
Since both the cognitive and social components are proportional to the distance of the particle to either the personal or the social best position, velocities can become very large.
A common solution to avoid this is \emph{velocity clamping} which bounds the maximum velocity of a particle to a fraction $\zeta$ of the size of the input space.

Using the update steps for the positions and velocities, the behaviour of the swarm $\Pe$ can be calculated for an arbitrary number of time steps.
The result of the optimization using PSO is the best position visited by any particle at any time step and is given by
\begin{align}
    \mat{x^\ast} \in \lim_{t \to \infty} \argmax_{p \in \Pe} f(\Cog_p(t)).
\end{align}
If $f$ is bounded, this limit is well-defined since $f(\Cog_p(\cdot))$ is monotonically increasing for all $p \in \Pe$.
Assuming a neighbourhood which transitively connects all particles, an arbitrary amount of time and a unique global optimum which gets visited by a particle at some time, the swarm should intuitively collapse to this global optimum, since the best particle attracts all other particles.
While the heuristic is surprisingly successful in practice, the basic PSO algorithm can be shown to not converge to a single point or to the global optimum in scenarios which are not so ideal \cite{engelbrecht_fundamentals_2006}.

Since PSO cannot judge whether it has converged to the limit, standard termination criteria for optimization have to be employed.
Common problem-dependent choices are a minimum step size between successive best solutions, minimum improvement of the objective function between successive best solutions or a maximum number of iterations or iterations without improvement.
Besides these criteria, there are many other variable parameters in PSO such as the number of particles or the structure of the neighbourhood function.

\subsection{Choosing Parameters}
The performance of basic PSO depends on choices for multiple parameters, namely the number of particles in the swarm, the structure of the neighbourhood, the number of iterations and the constants in the velocity update step.
Since it is a heuristic algorithm, the correct settings of these parameters are problem-dependent and it is hard to give mathematical proofs.
This subsection describes the empirical findings of this thesis and summarizes the recommendations given by \citeauthor{engelbrecht_fundamentals_2006} in \cite{engelbrecht_fundamentals_2006}.

As with many other optimization algorithms, there are two important trade-offs when deciding on parameter values.
PSO evaluates the objective function $\Oh(\abs{\Pe} \cdot \abs{\Tee})$ times, about once per particle and time step.
This introduces the first compromise between computational time and the quality of the result.
Both a higher number of particles and time steps can be expected to improve the overall best position, up to a point of saturation where the algorithm converges.
Therefore, this product should be as large as the available computational resources allow.
The minimum number of function evaluations required is highly dependent on the structure of the search space.
Too few iterations can terminate the search prematurely, not giving the swarm time to collapse to a good solution.
On the other hand, too few particles can mean that relevant parts of the search space are never visited.

Dividing computational power between the number of particles and the number of time steps introduces the second trade-off of exploration and exploitation.
The more particles there are in the swarm, the more diversity is there in the initial positions of the swarm.
During the first few iterations of PSO where particles are just starting to influence each other, more unknown parts of the search space will be visited and therefore more of the search space will be explored.
If $\abs{\Pe}$ is increased in expense of the number of iterations, more particles mean a higher chance that some of them find good parts of the search space.
However, fewer iterations can mean that there is less time for the swarm to collapse towards these good parts and search them more thoroughly to exploit the gained knowledge.
\citeauthor{engelbrecht_fundamentals_2006} recommends a number of about 10 to 30 particles as a good starting point for an unknown problem but suggests cross-validation to improve the results.
In this thesis, PSO is used to optimize in the space of all future actions in the time horizon.
To ensure good coverage of this space, which for the bicycle benchmark is about 50-dimensional, 250 particles were used during the experiments.

\begin{figure}[t]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \includestandalone{figures/theory_pso_ring_topology}
        \caption{Ring topology}
        \label{fig:pso_topology:ring}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \includestandalone{figures/theory_pso_star_topology}
        \caption{Star topology}
        \label{fig:pso_topology:star}
    \end{subfigure}
    \caption{
        Neighbourhood topologies define the social interaction between particles in a swarm.
        \Cref{fig:pso_topology:ring} shows a ring topology with six particles, where every particle is connected to its neighbours and itself.
        Since every particle is connected to exactly three other particles, the average degree of connectivity $\lambda_{\Neigh}$ is one half.
        The special case of the ring topology where every particle is connected to every other particle is called the start topology and shown in \cref{fig:pso_topology:star}.
    }
    \label{fig:pso_topology}
\end{figure}
The feature separating PSO from other population-based optimization schemes is social interaction.
Particles in the swarm communicate with their neighbours to obtain additional knowledge and move towards better parts of the search space.
The structure of this neighbourhood is determined by the constant neighbourhood function $\Neigh$ and the performance of PSO depends strongly on its structure.
The information flow through this network is described by the average degree of connectivity and the average distance between two arbitrary particles.

The average degree of connectivity $\lambda_{\Neigh}$ is the average fraction of all available particles a particle is connected to.
It the degree of connectivity is high and the average distance between two arbitrary particles low, information about a good position in the search space propagates quickly to all particles and the swarm will tend to collapse sooner.
For simple or unimodal objective functions, this behaviour can be very beneficial, since it lowers the number of iterations required for the swarm to collapse towards the global optimum.
For more complex problems however, the swarm is in danger of quickly collapsing in a local optimum without good exploration of the search space.

\citeauthor{engelbrecht_fundamentals_2006} describes a number of different social network structures.
This thesis uses the \emph{ring topology} where every particle communicates with itself and a number of immediate neighbours as shown in \cref{fig:pso_topology:ring}.
A higher degree of connectivity can be achieved by increasing the number of immediate neighbours a particle is connected to.
This topology guarantees transitive connections between every particle while still allowing for enough exploration since knowledge must potentially pass through several particles to reach a particle on the other side of the ring.
Neighbourhoods move smoothly around the ring and overlap, which means that this topology does not favour the formation of clusters of particles in the search space.
The special case of the ring topology where $\lambda_{\Neigh}$ is one is the \emph{star topology} as shown in \cref{fig:pso_topology:star}.
In this topology, every particle is connected with every other particle, which implies that the PSO will quickly converge towards the globally best solution.

Having established a swarm of particles with a neighbourhood $\Neigh$, the constants $\omega$, $\gamma_c$ and $\gamma_s$ in the velocity update in \cref{eq:pso_velocity} define the relative influences of the different kinds of information a particle has gathered to its trajectory.
The cognitive weight $\gamma_c$ and the social weight $\gamma_s$ describe the trust of a particle towards its own experiences and the experiences of its neighbours.
If $\gamma_s$ is equal to zero, the communication of the particles is ignored and every particle performs a local optimization independent of the rest of the swarm.
On the other hand, if $\gamma_c$ is zero in a star topology, every particle would be attracted towards the single best known position and PSO turns into a stochastic hill-climber \cite{engelbrecht_fundamentals_2006}.
The strength of PSO comes from weighing the two aspects, so a common choice is to set both constants to similar values.
The inertia weight $\omega$ is meant to ensure smooth trajectories and is set to be between zero and one to allow the swarm to converge.
Based on empirical studies\todo{Alex suggested these are actually based on a proof?}, \citeauthor{eberhart_comparing_2000} suggest setting $\omega$ to 0.7291 and both $\gamma_c$ and $\gamma_s$ to 1.49618 \cite{eberhart_comparing_2000}.

The velocity clamping factor $\zeta$ defines the maximum absolute value of the velocity per dimension to be given by $\mat{v^\text{max}} = \zeta \cdot ( \mat{x^\text{max}} - \mat{x^\text{min}} )$.
A small value avoids erratic movement of the particles by jumping over huge parts of the search space in one time step.
Like the weight factors, the velocity clamping factor is defined to be constant throughout a PSO run within this thesis.
It is chosen as 0.1 which allows a particle to cross the complete space in comparatively few iterations but still limits its dynamic range.

In the experiments of this thesis, the convergence of PSO did not present a problem.
For other problem instances, time dependent values of the constants can be used to influence the behaviour of the algorithm.
It may be interesting to switch focus from exploration in the early time steps to exploitation in the later time steps and finally force convergence.
This can be achieved by adaptively changing the different weights or the velocity clamping factor \cite{engelbrecht_fundamentals_2006}.

The choices for the parameters of the PSO used in this thesis are presented in \cref{tab:pso_parameters}.
The problem of choosing a vector of actions in PSO-P is very high dimensional.
Because of this, the ratio of number of particles to number of time steps leans towards a higher than usual number of particles.
This is compensated for with a high average connectivity in the ring neighbourhood.
The different weights and the velocity clamping factor are standard choices since they proved to be successful.

\begin{table}[t]
    \centering
    \caption{The PSO parameters used in this thesis.}
    \label{tab:pso_parameters}
    \begin{tabularx}{\tablewidth}{cXc}
        \toprule
        Parameter & Description & Value \\
        \midrule
        $\abs{\Pe}$ & Number of particles & 250 \\
        $\abs{\Tee}$ & Maximum number of time steps & 80 \\
        $\Neigh$ & Neighbourhood topology & Ring \\
        $\lambda_{\Neigh}$ & Average connectivity in the neighbourhood & 0.1 \\
        $\omega$ & Inertia weight & 0.72981 \\
        $\gamma_c$ & Cognitive weight & 1.49618 \\
        $\gamma_s$ & Social weight & 1.49618 \\
        $\zeta$ & Velocity clamping factor & 0.1 \\
        \bottomrule
    \end{tabularx}
\end{table}

\section{Summary}
This chapter presented the mathematical framework of reinforcement learning to describe solutions to the bicycle benchmark presented in \cref{cha:the_bicycle_benchmark} with mathematical rigor.
With Gaussian processes and the PSO-Policy it introduced the two main tools used in this thesis to solve it in a Bayesian way.
The next chapter introduces a standard approach to controlling the bicycle with PSO-P and compares it to two strategies which incorporate uncertainty into their decision making process.
