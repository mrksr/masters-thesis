\chapter{State of the Art}
\label{cha:state_of_the_art}
The bicycle benchmark is an instance of a problem in a branch of machine learning called reinforcement learning.
This chapter gives a brief introduction into reinforcement learning necessary and defines the notation necessary to reason about the bicycle benchmark and similar problems in a mathematical way.
The approach used in this thesis to solve this problem is called model-based reinforcement learning and is discussed next.
These models are used to learn the dynamics of the system to be controlled in order to be able to make predictions about its future development.
The remainder of the chapter will be concerned with introducing two tools used in this approach.

Gaussian Processes provide a framework to learn dynamics in a way which adds information about the uncertainty of a prediction.
This is achieved by learning a distribution over possible models rather than deciding on one single model.
The choice of which action to take will then be made via optimization over the space of all possible actions for a certain amount of decisions into the future.
Since this problem is highly non-linear and hard to describe analytically, the gradient-free heuristic method of particle swarm optimization is used to solve it.

\section{Reinforcement Learning}
\label{sec:reinforcement_learning}
Consider an agent who wants to learn how to learn how to drive a bicycle to a certain position.
In the case of a supervised learning environment, an expert who already knows how to solve the problem could tell this agent which actions lead to success.
In the absence of such a teacher however, the agent must learn from interaction with the bicycle.

An agent might start by applying random actions to the system and quickly fall down, making it impossible to ever reach the goal.
This gives the agent an opportunity to learn:
It has to avoid falling down in order to have the chance of achieving its objective.
Such a feedback is called a reward (or in this case, a punishment) and is the basis on which the agent can learn to judge the viability of actions in a certain state.

After multiple trials, the agent might be able to avoid falling an be able to stabilize the bicycle.
To achieve this, the agent may have built a basic understanding of how the bicycle system behaves.
It might have recognized that a bicycle which is already leaning on one side if left alone will fall down because of the gravitational pull or driving in a curve means that the centrifugal force pushes the bicycle to the outside.
Note that to gain these insights, it is not necessary for the agent to have an understanding of the underlying physics.
It is enough to observe situations in which the effects play out and generalize from there.

When the agent has learnt how to stabilize the bicycle by driving small corrective curves, it has not yet solved the original task of navigating the bicycle to a specific position.
It will have to shift its focus from the short-term goal of avoiding falling over to the more high-level and long-term task of following a targeted trajectory.
Following this trajectory might require some compromises, since driving along a very sharp curve requires the cyclist to lean very precisely.
In the case that reaching the goal is time-critical, the optimal trajectory would be constrained by the minimum radius of a curve and therefore by the amount the bicycle is allowed to be leaning before falling over.

The requirements an agent faces when solving the bicycle benchmark can however be understood in a more general sense, leading to the problem statement of reinforcement learning.
In the following, it will be formulated in a mathematical way to allow a principled approach to a solution.

\subsection{Problem Statement}
\label{sub:problem_statement}
Reinforcement learning is meant to catch the general problem of learning by interaction to control a system in order to achieve a predefined goal.
In order to achieve this goal, the learning entity or \emph{agent} has to decide on specific actions to influence its \emph{environment}, which is everything outside of the agent.
The boundary between agent and environment is a well-defined and narrow interface illustrated in \cref{fig:agent_environment_interaction}.
They interact via this interface at specific discrete \emph{time steps}, usually indexed with the natural numbers.
\begin{figure}[htb]
    \centering
    \missingfigure[figheight=0.25\textheight]{Agent-Environment-Interaction}
    \caption{Agent-Environment-Interaction}
    \label{fig:agent_environment_interaction}
\end{figure}

At every such time step $t$, the agent can observe the environment and receives some information in form of its \emph{state} $s_t \in \Es$, where $\Es$ is the space of all possible states and will for the remainder of the thesis be assumed to be a finite-dimensional vector space over the real numbers.
Based on this information, the agent has to decide on which action $a_t \in \Ah$ to perform.
The space of all possible actions $\Ah$ is also assumed to be a finite-dimensional vector space over the real numbers and is the same for all time steps and states.

The decision-process an agent employs in order to choose an action is called the agent's \emph{policy}.
A policy is a function which maps states to actions.
Policies are often encoded in closed forms such as linear functions \cite{deisenroth_efficient_2010}.
\begin{definition}[Policy]
    A \emph{policy $\pi$} an agent follows encodes the choice an agent makes when faced with a decision.
    It is a function
    \begin{align}
        \pi : \Es \to \Ah
    \end{align}
    which maps the state the system currently is in to the action the agent will perform.
\end{definition}

Once the agent has chosen an action for time step $t$, the \emph{transition dynamics} of the system determine the state $s_{t+1}$.
These dynamics are unknown to the agent and can be subject to probabilistic factors such as noise.
While the agent will always observe one single element $s_{t+1} \in \Es$ when prompted for the next decision, the transition dynamics return a probability distribution over the possible next states.

Many problems like the bicycle benchmark have a natural notion of states which are \emph{terminal}.
If the cyclist falls down or reaches the goal, the interaction between the agent and the environment comes to an end and there are no more decisions to make.
Such tasks are called \emph{episodic} and one sequence from a start state to a terminal state is called an \emph{episode}.
Notationally, $\Tee$ is defined to be the set of all terminal states with $\Tee \cap \Es = \emptyset$.
An episode ends when the transition dynamics return a state which is in $\Tee$.
The set $\Es \cup \Tee$ is called the \emph{extended state space $\Es^+$} and combines both non-terminal and terminal states.
A task which does not have terminal states is called \emph{continuous} and has episodes of infinite length.

\begin{definition}[Transition Dynamics]
    The \emph{transition dynamics $\Dyn$} of a system encode the physical behaviour of a system.
    These dynamics
    \begin{align}
        \Dyn : \Es \times \Ah \to \Dists(\Es^+)
    \end{align}
    mapping a state and an action to a distribution over possibly terminal states stay constant over time but can be probabilistic in nature.
\end{definition}
An important observation following from this definition is that the transition dynamics fulfill the Markov property \cite{sutton_reinforcement_1998}.
This means that the distribution of the state $s_{t+1}$ is independent of all states before $s_t$ given $s_t$.
In other words, the transition dynamics only depend on the present state of the system and are conditionally independent of the past.

At time step $t+1$, the agent observes the environment again in the form of the state $s_{t+1}$.
Additionally, it also receives immediate feedback about this new state called the \emph{reward $r_{t+1}$}.
The reward is a quality assigned to each state and serves as a measure of how good a state to be in it is.
This measure is independent of future or past development.
It is obtained from a real-valued \emph{reward function $\Rwd$} such that $r_{t+1} = \Rwd(s_{t+1})$.
\begin{definition}[Reward Function]
    The \emph{reward function $\Rwd$} assigns a quality to each state in the extended state space and has the signature
    \begin{align}
        \Rwd : \Es^+ \to \R.
    \end{align}
    This reward is the immediate feedback an agent receives when interacting with the system.
\end{definition}

The goal of the agent is to maximize the sum of all rewards earned throughout an episode.
A greedy agent which is only concerned about the next immediate reward might not be the most successful, since it may be necessary to make a decision which is bad in the short term to gain an advantage in the long term, such as sacrificing a piece in chess to end up in a better position overall.

The \emph{value function} is a measure for how good a policy behaves in the long-run.
Given a policy and a start state, it is defined as the expected sum of rewards earned in a time horizon $T$.
Since the transition dynamics are assumed to be probabilistic, the states at all time steps greater than zero are random variables.
Given a distribution of the state $\rv{s_t}$, the distribution for the next state $\rv{s_{t+1}}$ for all $t \in \N_{>0}$ can be calculated as
\begin{align}
    \Prob{\rv{s_{t+1}}} &= \int \Prob{\rv{s_{t+1}} \given \rv{s_t}, \pi}\Prob{\rv{s_t}} \diff \rv{s_t} \\
    \intertext{where}
    \Prob{\rv{s_{t+1}} \given \rv{s_t}, \pi} &= \begin{cases}
        \delta_{\rv{s_t}, \rv{s_{t+1}}} & \text{ if $\rv{s_t} \in \Tee$} \\
        \Dyn\Cond{\rv{s_{t+1}} \given \rv{s_t}, \pi(\rv{s_t})} & \text{ otherwise.}
    \end{cases}
\end{align}
Here, $\delta$ denotes the Dirac-delta-function and the notation $\Dyn\Cond{\rv{s_{t+1}} \given \rv{s_t}, \pi(\rv{s_t})}$ means the probability of $\rv{s_{t+1}}$ under the distribution $\Dyn(\rv{s_t}, \pi(\rv{s_t}))$.
Once the dynamics have reached a terminal state, all future states will be this same state.
This extends all episodes to potentially infinite length.

The time horizon $T$ can be chosen freely but does not depend on either the current policy or state.
The larger the time horizon, the more far-sighted a agent has to be to be successful.
For large values of $T$, it can be helpful to focus to weigh potential rewards further into the future less.
This can be achieved using the discount factor $\gamma$ which is a constant real number between zero and one.
In the case of an infinite time horizon, $\gamma$ must be chosen smaller than one to ensure the well-definedness of the function.

\begin{definition}[Value Function]
    Given a policy $\pi$, a time horizon $T \in \N \cup \left\{ \infty \right\}$ and a discount factor $0 \leq \gamma \leq 1$, the \emph{value function $\Vlu^\pi$} denotes the expected long-term reward of a state and is given by
    \begin{align}
        \Vlu^\pi : \left\{
            \begin{aligned}
                \Es^+ &\to \R \\
                s &\mapsto \Moment*{\E}{\sum_{t=0}^T \gamma^t \Rwd \left( \rv{s_t} \right) \given \pi, \rv{s_0} = s }.
            \end{aligned}
        \right.
    \end{align}
    If the time horizon is infinite, $\gamma$ must be smaller than 1.
\end{definition}

Assume now a known distribution of possible start states $\Prob{\rv{s_0}}$.
The sets $\Es^+$ and $\Ah$ of states and actions together with the transition dynamics $\Dyn$, the reward function $\Rwd$ and this distribution $\Prob{\rv{s_0}}$ defines a fully observable Markov decision process (MDP) \cite{sutton_reinforcement_1998,murphy_machine_2012}.

The objective of the reinforcement learning problem stated here is to find the most successful policy to control this decision process under the assumption that no expert knowledge about the transition dynamics is available.
This optimal policy $\pi^*$ maximizes the expected value for all start states, that is it solves the optimization problem
\begin{align}
    \label{eq:optimal_policy}
    \pi^* &\in \argmax_{\pi} \Moment*{\E_{\rv{s_0}}}{V^\pi(\rv{s_0})} \\
    &= \argmax_{\pi} \int V^\pi(\rv{s_0}) \Prob{\rv{s_0}} \diff \rv{s_0}.
\end{align}

There is a multitude of different approaches of arriving at such an optimal policy.
The following will introduce the important distinction of model-based and model-free methods and give a high level view of how the former will be used within this thesis.

\subsection{Model-Based Reinforcement Learning}
To find a good policy, an agent has to gain experience about its environment via interaction.
In \emph{direct reinforcement learning} \cite{sutton_reinforcement_1998}, or model-free methods, this experience is used to update the current policy or its value function directly.
After enough time spent with the system, iteratively improving the current policy can converge to the optimal policy.

In \emph{model-based} reinforcement learning, the experience is used to learn an internal representation of the transition dynamics $\Dyn$.
This allows the agent to make predictions about the future behaviour of the system and therefore approximatively evaluate the value function without actually interacting with the system.
A closed-form policy can then be found by solving the non-linear optimization problem proposed in \cref{eq:optimal_policy}.

\citeauthor{deisenroth_efficient_2010} \cite{deisenroth_efficient_2010} describes an algorithmic scheme called PILCO in which phases of exploration on the real system to gather more data to improve the internal model alternate with phases of improvement of the current policy based on this internal simulation.
This iteration allows the agent to explore promising directions in the state space using intermediate policies.

In this thesis however, it is assumed that all interaction with the system has to happen before any learning can take place.
This assumption is sensible in the context of industrial applications where interactions with a system can be very expensive and dangerous and where a possibly bad agent cannot be allowed to choose actions to perform.
Instead of allowing interaction, the agent is presented with a data set of observations of the transition dynamics in the form of tuples $(\mat{s_t}, \mat{a_t}, \mat{s_{t+1}})$.
These observations can be obtained via a mix of random exploration and actions chosen by a sub-optimal controller.

Using an internal representation instead of the real transition dynamics to choose actions leads to one of the major drawbacks of model-based reinforcement learning.
If this representation does not capture the important characteristics of the original system well, a policy found to be optimal on the simulation might not lead to good results on the correct dynamics.
This effect is called the \emph{model bias} of a policy.

Reducing this model bias is the main goal of explicitly representing uncertainties within this thesis.
Instead of focusing on one single dynamics model, a probabilistic representation of all plausible models to explain the observed data will be considered.
This yields a measure of uncertainty of predictions one step into the future and can be extended to long-term predictions as required in the value function.
Assuming deterministic transition dynamics such as the bicycle benchmark, this measure does not describe a property of the system but rather the uncertainty about the model itself.

Gaussian Processes provide a framework to represent such a distribution over possible dynamic models.
The following will introduce their definition and how they can be used to solve regression problems.
The last part of this chapter will be concerned with introducing a policy-representation based on these results which does not depend on a closed-form solution.

\section{Gaussian Process Regression}
The transition dynamics $\Dyn : \Es \times \Ah \to \Dists(\Es^+)$ are a function mapping states and actions to a probability distribution of following states.
In order to estimate them using Gaussian Processes, some assumptions about the structure of this function are needed.
First, it will be assumed that both the set of states $\Es$ and the set of actions $\Ah$ are euclidean real valued vector spaces and that the set of terminal states $\Tee$ is empty, that is, $\Es^+$ and $\Es$ are assumed equal, requiring that episode endings have to be modelled separately.
And secondly, the probability distribution of the following state is assumed to be unimodal.
This unimodality can result from deterministic transition functions, such as the one of the bicycle benchmark defined in \cref{cha:the_bicycle_benchmark}, being disturbed slightly by Gaussian noise.

Estimating a function $f$ on the basis of observations $\mat{y_i} = f(\mat{x_i}) + \epsilon_i \in \R$ with input vectors $\mat{x_i} \in \R^d$ and a noise term $\epsilon_i$ is a \emph{regression problem}.
Since the number of observations is finite and the function $f$ is an infinite object, the estimation of $f$ is uncertain and based on prior assumptions about its structure.

In classic control scenarios, these prior assumptions often follow from physical descriptions of the system to be modelled.
While the physics of driving a bicycle is understood quite well and can be described using differential equations, a controller for a specific bicycle depends on some parameters $\gamma$ such as the masses and measures detailed in \cref{tab:bicycle_constants}.
In this setting, solving the regression problem corresponds to a choice of parameters $\gamma^\ast$ which explain the observations of the true system best.

In a Bayesian context, instead of deciding on one specific vector of parameters, it might be more interesting to derive a distribution $\Prob{\gamma^*}$ of probable parameter values which then represents the uncertainty about their true value.
When making a prediction for a new input point $\mat{x_\ast}$, this uncertainty can be used to derive a joint distribution $\Prob{y_\ast \given \mat{x_\ast}, \mat{\gamma^\ast}}$, which propagates this uncertainty through the model to the prediction.

This approach represents uncertainty about the correct choice of parameters but assumes that the predefined structure of the function is correct, making it a \emph{parametric model}.
Such structure has the advantage of making it easier to find the best set of parameters, since the search space is relatively limited.
It does however limit the expressiveness of the model, which can lead to bad performance.
A physical description of the system might be too idealized and not take account of all factors in reality, such as the assumptions of frictionless mechanics or limited turbulences in fluid mechanics.
Accounting for all possible effects can make the model very complicated.
This means that both the number of parameters becomes large and it may be hard to interpret the model in a physical sense.

\emph{Non-parametric models} are not based on insights about the concrete structure of the function to be modelled but rather only make assumptions about properties of the function itself, such as smoothness or differentiability.
Instead of modelling a distribution of parameter values, a Bayesian non-parametric model is concerned with finding a distribution $\Prob{f^\ast}$ of probable functions which represents the believe of the model about the function $f$ to be estimated.

\emph{Gaussian Processes (GPs)} are a state-of-the-art framework for non-parametric regression.
They are a way of representing a probability distribution over functions in a way which is both computationally feasible and allows for Bayesian inference.
This section introduces Gaussian Processes and describes how to encode a prior distribution over functions to represent preference in the space of all possible functions $f$.
Based on observed data, GPs can be used make predictions about the joint distribution $\Prob{y_\ast \given \mat{x_\ast}, \mat{f^\ast}}$ taking all functions in the distribution $\Prob{f^\ast}$ into account.
Since these predictions are not computationally cheap, an extension of Gaussian Processes for large data sets, sparse Gaussian processes using pseudo-inputs \cite{snelson_sparse_2005}, is introduced last.

\subsection{Definition}
\begin{itemize}
    \item High-level intro to Stochastic processes
\end{itemize}
\begin{definition}[Stochastic Process]
    \label{def:stochastic_process}
    Given a probability space $(\Omega, \mathcal{F}, P)$, an index set $T$ and a measurable space $Y$, a \emph{stochastic process $\rv{X}$} is a function
    \begin{align}
        \rv{X} : \left\{\begin{aligned}
            T \times \Omega &\to Y \\
            (t, \omega) &\mapsto \rv{X_t}(\omega)
        \end{aligned}\right.
    \end{align}
    mapping indices $t$ to $Y$-valued random-variables.
    For a fixed $\omega \in \Omega$, $\rv{X}(\cdot, \omega)$ is called a \emph{trajectory} of the process \cite{astrom_introduction_1971}.
\end{definition}
\begin{definition}[Gaussian Process]
    \label{def:gaussian_process}
    A stochastic process $\rv{X}$ is called a \emph{Gaussian process} if for any finite subset $\tau \subseteq T$ of its index set, the random variables $\rv{X}_\tau$ have a joint Gaussian distribution \cite{astrom_introduction_1971}.
\end{definition}
\begin{itemize}
    \item Mean-Function, Covariance-Function
    \item Assumption of mean function equal to zero
    \item Notational things $\Dyn \sim \GP\Cond*{0, \K(\cdot, \cdot)}$
\end{itemize}

\subsection{Kernels}
\begin{definition}[Gram Matrix]
    Given a non-empty set $M$, a function $\K : M^2 \to \R$ and two sets $X = \Set*{x_i \in M \with i \in [n]}$ and $Y = \Set*{y_j \in M \with j \in [m]}$.
    The $n \times m$ matrix
    \begin{align}
        \K(X, Y) = \mat{K_{XY}} \coloneqq \bigg( \K(x_i, y_j) \bigg)_{\substack{i \in [n], \\ j \in [m]}}
    \end{align}
    is called the \emph{Gram matrix} of $\K$ with respect to $X$ and $Y$ \cite{scholkopf_learning_2002}.
\end{definition}
\begin{definition}[Kernel]
    Given a non-empty set $M$, a function
    \begin{align}
        \K : M^2 \to \R
    \end{align}
    is called a \emph{(positive definite) kernel} or \emph{covariance function}, if for any subset $X \subseteq M$, the Gram matrix $\K(X, X)$ is positive definite \cite{scholkopf_learning_2002}.
\end{definition}
\begin{definition}[Linear Kernel]
    For a finite dimensional euclidean vector space $\R^d$, the \emph{linear kernel} is defined as
    \begin{align}
        \K_{\text{linear}}(\mat{x}, \mat{y}) \coloneqq \mat{x}^T \mat{y} = \left\langle \mat{x}, \mat{y}\right\rangle.
    \end{align}
\end{definition}
\begin{definition}[Squared Exponential Kernel]
    For a finite dimensional euclidean vector space $\R^d$, the \emph{squared exponential kernel} (or \emph{RBF kernel}) is defined as
    \begin{align}
        \K_{\text{SE}} \coloneqq \sigma_f^2 \cdot \exp \left( -\frac{1}{2} (\mat{x} - \mat{y})^T \mat{\Lambda}^{-1} (\mat{x} - \mat{y}) \right).
    \end{align}
    The parameter $\sigma_f^2 \in \R_{>0}$ is called the \emph{signal variance} and $\mat{\Lambda} = \diag(l_1^2, \dots, l_d^2)$ is a diagonal matrix of the squared \emph{lengthscales} $l_i \in \R_{>0}$.
\end{definition}
\begin{itemize}
    \item Sampling from Kernels without any data
    \item Description of hyperparameters
    \item Comparison of hyperparameter values
\end{itemize}

\subsection{Predictions and Posterior}
We observe some data
$\D = \left\{ \mat{X}, \mat{y} \right\}$ with $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma_n^2 \Eye}$
where the noise assumption means that for $\mat{X}$ there is a slightly different covariance
\begin{align}
    \mat{\Sigma_y} = \K(\mat{X}, \mat{X}) + \sigma_n^2 \Eye
\end{align}
other than thet, the definition directly yields for some new points $\mat{X_\ast}$ that
\begin{align}
    \begin{pmatrix}
        \mat{y} \\
        \mat{f_\ast}
    \end{pmatrix} &\sim \Gaussian*{\mat{0}, \begin{bmatrix}
        \K(\mat{X}, \mat{X}) + \sigma_n^2 \Eye & \K(\mat{X}, \mat{X_\ast}) \\
        \K(\mat{X_\ast}, \mat{X}) & \K(\mat{X_\ast}, \mat{X_\ast})
    \end{bmatrix}} \\
    &= \Gaussian*{\mat{0}, \begin{bmatrix}
        \mat{K_N} + \sigma_n^2 \Eye & \mat{K_{N\ast}} \\
        \mat{K_{\ast N}} & \mat{K_{\ast}}
    \end{bmatrix}}
\end{align}

Using operations on Gaussians this gives us
\begin{lemma}[GP predictive posterior]
    Given a latent function with a Gaussian process distribution $f \sim \GP(\mat{0}, \K)$ and training points $\mat{X}$ with noisy observations of the form $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma_n^2 \Eye}$.
    The predictive posterior $\mat{f_\ast}$ of the test points $\mat{X_\ast}$ is then given by
    \begin{align}
        \Prob{\mat{f_\ast} \given \mat{X}, \mat{y}, \mat{X_\ast}} &= \Gaussian*{\mat{f_\ast} \given \mat{\mu_\ast}, \mat{\Sigma_\ast}} \text{, where} \\
        \mat{\mu_\ast} &= \mat{K_{\ast N}} \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{y} \\
        \mat{\Sigma_\ast} &= \mat{K_\ast} - \mat{K_{\ast N}} \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{K_{N\ast}}.
    \end{align}
\end{lemma}

Lastly, we can interpret this in terms of Gaussian posteriors: The posterior distribution $\Prob{f \given \mat{X}, \mat{y}, \mat{\theta}}$ is a GP with new mean and covariance-functions stated above.

\subsection{Choosing hyperparameters}
Proper Bayesian way means choosing a prior for $\mat{\theta}$.
\begin{align}
    \Prob{f} &= \int \Prob{f \given \mat{\theta}} \Prob{\mat{\theta}} \diff \theta \\
    \Prob{\mat{y} \given \mat{X}} &= \int \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{\theta}
\end{align}
But this is not doable since we cannot solve the integral \cite[109]{rasmussen_gaussian_2006}. <Good Argument here>.

Instead: Do a point estimate. Next best thing would be a MAP estimate:
\begin{align}
    \Prob{\mat{\theta} \given \mat{X}, \mat{y}} &= \frac{\Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}}}{\Prob{\mat{y} \given \mat{X}}} \\ &= \frac{\Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}}}{\int \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}} \diff \theta}
\end{align}
But again: Solving this in general is hard.
If we assume a flat prior for $\mat{\theta}$ (i.e. a "uniform" distribution), then we have that
\begin{align}
    \Prob{\mat{\theta} \mid \mat{X}, \mat{y}} \propto \Prob{\mat{y} \given \mat{X}, \mat{\theta}}
\end{align}
and we can do maximum (marginal) likelihood:
\begin{align}
    \mat{\theta}^\ast &\in \argmax_{\mat{\theta}} \Prob{\mat{y} \given \mat{X}, \mat{\theta}}
\end{align}

This one we can solve, since with $\mat{f} \coloneqq f(\mat{X})$ we have
\begin{align}
    \Prob{\mat{y} \given \mat{X}, \mat{\theta}} &= \int \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2 \Eye} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f} \\
    &= \int \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2 \Eye} \cdot \Gaussian{\mat{f} \given \mat{0}, \mat{K_N}} \diff \mat{f}
\end{align}
Using standard results about Gaussians we have
\begin{align}
-\log\Prob{\mat{y} \given \mat{X}, \mat{\theta}} =
    \underbracket[1pt]{\frac{1}{2} \mat{y^T} \left( \mat{K_N} + \sigma_n^2 \Eye \right)^{-1} \mat{y}}_{\text{data-fit term}} +
    \underbracket[1pt]{\frac{1}{2} \log \abs{\mat{K_N} + \sigma_n^2 \Eye}}_{\text{complexity term}} +
    \frac{N}{2} \log(2\pi)
\end{align}
which can be solved using nonlinear optimization.
This is called a \emph{type II maximum likelihood estimate (ML-II)} since it optimizes over the marginal likelihood.
This implements Occam's razor by the way. Which we should maybe skip.

\subsection{Sparse Approximations using Inducing Inputs}
\begin{itemize}
    \item Definition of Pseudo-Inputs
    \item Common Distribution
    \item Likelihood Function
    \item Special Kernel Function
    \item Predictive Posterior
    \item Asymptotic behviour
    \item Differences of Titsias approach
\end{itemize}
\begin{lemma}[SPGP predictive posterior]
    content
\end{lemma}

\section{Particle Swarm Optimization}
\subsection{Basic PSO}
\subsection{Improvements}
