\chapter{Conclusions}
This thesis was concerned with incorporating information about uncertainties into reinforcement learning using ideas from Bayesian statistics.
In model-based reinforcement learning, a model of the true system's transition dynamics extracted from limited observations is used to make approximations about the system's future behaviour.
These observations were considered to be gathered beforehand using a simple controller or random exploration instead of allowing the controller to directly interact with the system.
This restriction is often imposed in industrial systems due to financial or security concerns.
Instead of relying on a single deterministic model which introduces model bias, a Bayesian framework considers distributions over models obtained by combining broad and simple prior assumptions about the transition function with their likelihood given the observed data.
Since the observed data is finite, there are multiple plausible models which have to be considered.
Gaussian processes make predictions about successive states by marginalizing the distribution of plausible models and can give estimations about the uncertainty of their predictions in a principled manner.
Instead of learning a closed policy representation using these transition models, this thesis used the PSO-Policy, which chooses actions by directly optimizing the expected long-term reward.

The deterministic bicycle system by \citeauthor{randlov_learning_1998} was used as a benchmark problem to illustrate the challenges introduced by considering uncertain predictions.
While Gaussian processes directly provide a measure of uncertain predictions given a certain input, there is no analytical solution to propagating already uncertain belief about the current state through the transition model to obtain the belief about the successive state.
Besides considering classic deterministic predictions without any uncertainty, this thesis considered using both one-step and multi-step uncertainties.
While for one-step uncertainties, long-term predictions are obtained by assuming the state to be deterministic before every application of the transition model, multi-step uncertainties approximate the posterior distribution with a Gaussian by linearizing the transition model around the mean of the belief about the prior state, thereby propagating uncertainty through the nonlinear transition function.

In the bicycle benchmark, the controller has to balance a bicycle to keep it from falling over and navigate towards a goal region.
This introduces the possibility of failure and the concept of terminal states.
In contrast to deterministic long-term predictions, where for every time step the predictive state is either terminal or not, uncertain predictions can spread probability mass over both regions of the state space, which represents the belief that the trajectory could both have ended or not.
Since the transition models only handle non-terminal states, this thesis proposed a technique which models and propagates this belief separately in order to correctly calculate the expected long-term reward.
Additionally, this belief can be used to refine the state distributions by truncating the Gaussian to non-terminal states.

This thesis provided experimental evidence that considering model uncertainties during planning is beneficial when creating trajectories for the bicycle benchmark using PSO-P.
Instead of aggressively exploiting the model and therefore falling victim to model bias, multi-step uncertainties incentivize the controller to choose actions where the transition model is confident about its predictions and where small mistakes of the models do not cause the trajectory to fail.
This increased the success rate considerably when compared to classical maximum-a-posteriori predictions.

Since this thesis made few assumptions specific to the bicycle problem, this result is promising with respect to other control problems where no or limited expert knowledge is available.
Uncertainty information reflecting the confidence of the transition model allow a controller to make more informed decisions during planning and to reduce risk by avoiding unknown parts of the system.
On the other hand, the uncertainty in the transition model can also be used to drive exploration in settings where a controller is allowed interaction with the system, thereby reducing the amount of interaction needed.

There are multiple different directions of possible future work based on this thesis.
While the uncertainties about the transition model are used as a basis for long-term predictions and provide a more reliable estimation of the expected long-term reward when compared to deterministic predictions, these uncertainties are not directly utilized during the optimization process of PSO-P.
Instead, they only indirectly improve the policy by making the objective function more robust.
Bayesian optimization schemes \cite{brochu_tutorial_2010} could be used on the uncertain reward function to choose actions which result in high expected reward with lower variance.
Additionally, the computational cost of choosing actions could be reduced considerably.
Alternatively, policies in closed form could be learned by maximizing the expected reward of simulated trajectories as proposed by \citeauthor{deisenroth_efficient_2010} in \cite{deisenroth_efficient_2010}.

The transition models operate under the assumption that the prior state is not terminal.
In order to handle terminal states with uncertain long-term predictions, the probabilities of ever having reached a terminal state in the past are calculated via dynamic programming.
In order to avoid predictions under wrong assumptions, this thesis proposed to truncate the state distributions to remove the probability mass in terminal states.
However, performing this truncation has a detrimental effect on the performance of PSO-P in the bicycle benchmark.
This could be specific to the bicycle benchmark or point to an adverse interaction between PSO-P and the truncation given high uncertainties.

In order to ensure analytical tractability, this thesis assumes Gaussian distributions of intermediate states and often approximates unknown distributions.
Additionally, the true transition function is assumed to be deterministic or at least unimodal when modelling it using a Gaussian process.
However, problems with higher modality are common in reinforcement learning and cannot easily be modelled using the approaches presented in this thesis.
Recent work by \citeauthor{depeweg_learning_2016} presents an extension of long-term predictions to higher modality by Monte-Carlo-sampling trajectories using Bayesian neural networks \cite{depeweg_learning_2016}, which could also be applied to Gaussian processes.
