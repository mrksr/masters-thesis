\chapter{Uncertainties in the Bicycle Benchmark}
\label{cha:solution}
\Cref{cha:theory} introduced reinforcement learning as a general mathematical framework to describe the problem of controlling a bicycle in the benchmark presented in \cref{cha:the_bicycle_benchmark}.
This thesis is concerned with model-based reinforcement learning, where the transition function of the system to be controlled is represented with some function approximation which is learnt from observations of the system and is used to make predictions about the future.
Learning a model of the true system introduces model bias, where actions considered to be good with respect to the model's predictions can show bad performance in reality because the model is incorrect.
In order to reduce this bias, Gaussian Processes can be used which do not only yield one specific function approximation but a distribution over all plausible models.
This uncertainty about the correct model can be propagated through to predictions about specific test points and instead of a single point, Gaussian processes predict a Gaussian distribution about possible function values.

Modelling the transition dynamics allows the prediction of a state $\mat{s_{t+1}}$ given a concrete pair of state and action $(\mat{s_t}, \mat{a_t})$ for the previous time step.
Assuming a deterministic model which yields exactly one posterior state, the \emph{long-term prediction} of states multiple time steps into the future reduces to iterated one-step predictions.
Given a (deterministic) reward function $\Rwd$ as detailed in \cref{def:reward_function}, it is possible to evaluate the action value function $\AVlu$ of PSO-P and thus directly use the model to extract a policy, since the expected value in \cref{eq:action_value_function} is a simple sum of deterministic values.

In the Bayesian context however, the transition model predicts a distribution over posterior states.
This complicates long-term predictions since the uncertainty of intermediate states has to be propagated through the (non-linear) transition model to accumulate the uncertainties of multiple predictions.
Additionally, the original deterministic reward function possibly has to be adapted to make it possible to evaluate the expected reward for all time steps.
Once these problems are solved and the action value function can be calculated, PSO-P can be used in the same way as in the deterministic case to choose appropriate actions.

Based on the bicycle benchmark, this chapter compares the classical deterministic long-term predictions to two approaches of integrating uncertainty into the predictions.
The first section describes the creation of data sets used to train the transition model and the design-choices made to obtain suitable models.
These models are then interpreted as deterministic to obtain a base-line for comparison in the next section.
The last two sections describe how to use uncertainties in the planning.
The first approach is to use the one-step uncertainties of the Gaussian Process models in the reward function but to still create long-term predictions using deterministic states.
The second fully Bayesian approach completely propagates state uncertainties both to the reward function and subsequent states.

\section{Transition Dynamics}
\subsection{Data Sets}
\begin{algorithm}[p]
    \caption{Sampling bicycle transitions}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    The minimal and maximal values for the state variables and the actions can be found in \cref{tab:bicycle_variables,tab:bicycle_actions}.
    \begin{algorithmic}[1]
        \Function{SampleBicycleState}{}
            \State $\left( \theta, \dot{\theta}, \omega, \dot{\omega} \right) \gets \Gaussian{\mat{0}, \sfrac{1}{4} \cdot \diag \left( \theta^{\text{max}}, \dot{\theta}^{\text{max}}, \omega^{\text{max}}, \dot{\omega}^{\text{max}} \right)}$
            \State $x \gets \Uniform{x^{\text{min}}, x^{\text{max}}}$
            \State $y \gets \Uniform{y^{\text{min}}, y^{\text{max}}}$
            \State $\psi \gets \Uniform{-\pi, \pi}$
            \State \Return $\Bicycle*{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleAction}{}
            \State $d \gets \Uniform{d^{\text{min}}, d^{\text{max}}}$
            \State $T \gets \Uniform{T^{\text{min}}, T^{\text{max}}}$
            \State \Return $\Bicycle{d, T}$
        \EndFunction
        \Statex
        \Function{SampleTransitions}{$N$}
            \For{$i \gets 1, N$}
                \State $\mat{s_i} \gets \Call{SampleBicycleState}$
                \State $\mat{a_i} \gets \Call{SampleAction}$
                \State $\mat{s^\prime_{i}} \gets \DynBicycle(\mat{s_i}, \mat{a_i})$
            \EndFor
            \State \Return $\left( \left( \mat{s_0}, \mat{a_0}, \mat{s^\prime_0} \right), \left( \mat{s_1}, \mat{a_1}, \mat{s^\prime_1} \right), \dots, \left( \mat{s_{N}}, \mat{a_{N}}, \mat{s^\prime_{N}} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[p]
    \caption{Sampling a bicycle trajectory}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    \begin{algorithmic}[1]
        \Function{SampleBicycleStartState}{}
            \State $\Bicycle*{\_, \_, \_, \_, x, y, \psi} \gets \Call{SampleBicycleState}$
            \State \Return $\Bicycle*{0, 0, 0, 0, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleTrajectory}{}
            \State $\mat{s_0} \gets \Call{SampleBicycleStartState}$
            \State $t \gets 0$
            \While{$\mat{s_t}$ is not terminal}
                \State $\mat{a_t} \gets \Call{SampleAction}$
                \State $\mat{s_{t+1}} \gets \DynBicycle(\mat{s_t}, \mat{a_t})$
                \State $t \gets t + 1$
            \EndWhile
            \State \Return $\left( \left( \mat{s_0}, \mat{a_0}, \mat{s_1} \right), \left( \mat{s_1}, \mat{a_1}, \mat{s_2} \right), \dots, \left( \mat{s_{t-1}}, \mat{a_{t-1}}, \mat{s_t} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
    \caption{Sampling a bicycle data set}
    \begin{algorithmic}[1]
        \Function{SampleMixedBicycleDataSet}{$N$}
            \State $\Es \gets \emptyset$
            \While{$\abs{\Es} < N$}
                \State $T \gets \Call{SampleTrajectory}$
                \State $R \gets \Call{SampleTransitions}{\abs{T}}$
                \State $\Es \gets \Es \cup T \cup R$
            \EndWhile
            \State \Return $\Es$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\subsection{Gaussian Process Models}
\section{Predictions without Uncertainties}
\subsection{Deterministic reward function}
\begin{definition}[Deterministic Bicycle Reward Function]
    Given the set $\Es^+$ of possible states of the bicycle benchmark, the \emph{deterministic bicycle reward function} is defined as
    \begin{align}
        \RwdBicycle : \left\{
            \begin{aligned}
                \Es^+ &\to \R \\
                \mat{s} &\mapsto \begin{cases}
                    2 & \text{if $\mat{s} \in \Goal$} \\
                    0 & \text{if $\mat{s} \in \Fallen$} \\
                    1 - \frac{\abs{\psi_{\mat{s}} - \varphi_{\mat{s}}}}{\pi} & \text{otherwise}
                \end{cases}
            \end{aligned}
        \right.
    \end{align}
    where $\psi_{\mat{s}}$ and $\varphi_{\mat{s}}$ denote the respective angles in state $\mat{s}$.
    It assigns constant reward for terminal states and reward inversely proportional to the angle between the bicycle's heading and the goal otherwise.
\end{definition}
\subsection{Long-Term predictions}
\subsection{Results and Problems}
\section{Predictions with One-Step Uncertainties}
\subsection{Probabilistic reward function}
\subsection{Long-Term predictions}
\subsection{Results and Problems}
\section{Predictions with Multi-Step Uncertainties}
\subsection{Linearization}
\subsection{Truncation of Gaussians}
\subsection{Results and Problems}
