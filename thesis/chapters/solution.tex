\chapter{Uncertainties in the Bicycle Benchmark}
\label{cha:solution}
\Cref{cha:theory} introduced reinforcement learning as a general mathematical framework to describe the problem of controlling a bicycle in the benchmark presented in \cref{cha:the_bicycle_benchmark}.
This thesis is concerned with model-based reinforcement learning, where the transition function of the system to be controlled is represented with some function approximation which is learnt from observations of the system and is used to make predictions about the future.
Learning a model of the true system introduces model bias, where actions considered to be good with respect to the model's predictions can show bad performance in reality because the model is incorrect.
In order to reduce this bias, Gaussian Processes can be used which do not only yield one specific function approximation but a distribution over all plausible models.
This uncertainty about the correct model can be propagated through to predictions about specific test points and instead of a single point, Gaussian processes predict a Gaussian distribution about possible function values.

Modelling the transition dynamics allows the prediction of a state $\mat{s_{t+1}}$ given a concrete pair of state and action $(\mat{s_t}, \mat{a_t})$ for the previous time step.
Assuming a deterministic model which yields exactly one posterior state, the \emph{long-term prediction} of states multiple time steps into the future reduces to iterated one-step predictions.
Given a (deterministic) reward function $\Rwd$ as detailed in \cref{def:reward_function}, it is possible to evaluate the action value function $\AVlu$ of PSO-P and thus directly use the model to extract a policy, since the expected value in \cref{eq:action_value_function} is a simple sum of deterministic values.

In the Bayesian context however, the transition model predicts a distribution over posterior states.
This complicates long-term predictions since the uncertainty of intermediate states has to be propagated through the (non-linear) transition model to accumulate the uncertainties of multiple predictions.
Additionally, the original deterministic reward function possibly has to be adapted to make it possible to evaluate the expected reward for all time steps.
Once these problems are solved and the action value function can be calculated, PSO-P can be used in the same way as in the deterministic case to choose appropriate actions.

Based on the bicycle benchmark, this chapter compares the classical deterministic long-term predictions to two approaches of integrating uncertainty into the predictions.
The first section describes the creation of data sets used to train the transition model and the design-choices made to obtain suitable models.
These models are then interpreted as deterministic to obtain a base-line for comparison in the next section.
The last two sections describe how to use uncertainties in the planning.
The first approach is to use the one-step uncertainties of the Gaussian Process models in the reward function but to still create long-term predictions using deterministic states.
The second fully Bayesian approach completely propagates state uncertainties both to the reward function and subsequent states.

\section{Transition Dynamics}
\label{sec:transition_dynamics}
The goal in the bicycle benchmark is to learn to both balance a bicycle and to ride it to a target position.
The state of a bicycle, as described in \cref{cha:the_bicycle_benchmark}, consists of the real valued vector $\Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$, where $\theta$ is the angle of the handlebars, $\omega$ is the vertical angle of the bicycle frame, $x$ and $y$ are euclidean coordinates and $\psi$ is the orientation of the bicycle.
The bicycle starts in almost upright position and the task of the controller is to choose the actions $\Bicycle{d, T}$, where $d$ is the horizontal leaning displacement of the driver and $T$ is the torque applied to the handlebars at every time step.
The transition dynamics are derived from a physical approximation of the system and are completely deterministic.

This thesis assumes that the actor is not allowed to interact with the system in order to try or improve its policy.
In contrast, the agent is presented with a predefined data set of observations of the system obtained with a simple and sub-optimal controller.
This constraint is meant to mimic industrial systems where it is comparatively cheap to obtain measurements of running systems but allowing an agent to explore is either very expensive or a security concern.
It is therefore not possible to apply an on-line learning scheme on the system or to explore in specific directions in order to improve the dynamics model.

This section first describes how data sets are sampled from the bicycle benchmark in order to simulate this constraint.
These data sets are then used to train the Gaussian Processes used in PSO-P to form a controller.

\subsection{Data Sets}
\begin{figure}[tp]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.35\textheight]{Length of Episodes}
        \caption{Length of Episodes}
        \label{fig:data_set_properties:episode_length}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.35\textheight]{Omega-Plot for one episode}
        \caption{Omega-Plot for one episode}
        \label{fig:data_set_properties:omega_example}
    \end{subfigure}
    \caption{Data Set properties}
    \label{fig:data_set_properties}
\end{figure}
\begin{algorithm}[tp]
    \caption{Sampling bicycle transitions}
    \label{alg:bicycle_transitions}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    The minimal and maximal values for the state variables and the actions can be found in \cref{tab:bicycle_variables,tab:bicycle_actions}.
    \begin{algorithmic}[1]
        \Function{SampleBicycleState}{}
            \State $\left( \theta, \dot{\theta}, \omega, \dot{\omega} \right) \gets \Gaussian{\mat{0}, \sfrac{1}{4} \cdot \diag \left( \theta^{\text{max}}, \dot{\theta}^{\text{max}}, \omega^{\text{max}}, \dot{\omega}^{\text{max}} \right)}$
            \State $x \gets \Uniform{x^{\text{min}}, x^{\text{max}}}$
            \State $y \gets \Uniform{y^{\text{min}}, y^{\text{max}}}$
            \State $\psi \gets \Uniform{-\pi, \pi}$
            \State \Return $\Bicycle*{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleAction}{}
            \State $d \gets \Uniform{d^{\text{min}}, d^{\text{max}}}$
            \State $T \gets \Uniform{T^{\text{min}}, T^{\text{max}}}$
            \State \Return $\Bicycle{d, T}$
        \EndFunction
        \Statex
        \Function{SampleTransitions}{$N$}
            \For{$i \gets 1, N$}
                \State $\mat{s_i} \gets \Call{SampleBicycleState}$
                \State $\mat{a_i} \gets \Call{SampleAction}$
                \State $\mat{s^\prime_{i}} \gets \DynBicycle(\mat{s_i}, \mat{a_i})$
            \EndFor
            \State \Return $\left( \left( \mat{s_1}, \mat{a_1}, \mat{s^\prime_1} \right), \left( \mat{s_2}, \mat{a_2}, \mat{s^\prime_2} \right), \dots, \left( \mat{s_{N}}, \mat{a_{N}}, \mat{s^\prime_{N}} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[tp]
    \caption{Sampling a bicycle trajectory}
    \label{alg:bicycle_trajectories}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    \begin{algorithmic}[1]
        \Function{SampleBicycleStartState}{}
            \State $\Bicycle*{\_, \_, \_, \_, x, y, \psi} \gets \Call{SampleBicycleState}$
            \State \Return $\Bicycle*{0, 0, 0, 0, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleTrajectory}{}
            \State $\mat{s_0} \gets \Call{SampleBicycleStartState}$
            \State $t \gets 0$
            \While{$\mat{s_t}$ is not terminal}
                \State $\mat{a_t} \gets \Call{SampleAction}$
                \State $\mat{s_{t+1}} \gets \DynBicycle(\mat{s_t}, \mat{a_t})$
                \State $t \gets t + 1$
            \EndWhile
            \State \Return $\left( \left( \mat{s_0}, \mat{a_0}, \mat{s_1} \right), \left( \mat{s_1}, \mat{a_1}, \mat{s_2} \right), \dots, \left( \mat{s_{t-1}}, \mat{a_{t-1}}, \mat{s_t} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[tp]
    \caption{Sampling a bicycle data set}
    \label{alg:bicycle_data_set}
    \begin{algorithmic}[1]
        \Function{SampleMixedBicycleDataSet}{$N$}
            \State $\D \gets \emptyset$
            \While{$\abs{\D} < N$}
                \State $T \gets \Call{SampleTrajectory}$
                \State $R \gets \Call{SampleTransitions}{\abs{T}}$
                \State $\D \gets \D \cup T \cup R$
            \EndWhile
            \State \Return $\D$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{figure}[tp]
    \centering
    \missingfigure[figheight=.5\textheight]{Positional Plot of a few Episodes}
    \caption{Positional Plot of a few Episodes}
    \label{fig:data_set_plot}
\end{figure}
The bicycle benchmark's dynamics are introduced in \cref{cha:the_bicycle_benchmark} by defining the derivatives of the state variables and choosing values for the relevant constants in \cref{tab:bicycle_constants}.
Given a starting state and an appropriate number of actions, these derivatives can be used to approximate the future behaviour of the system using iterative numerical methods for approximating ordinary differential equations.
For this thesis, the bicycle benchmark was implemented in Python \cite{rossum_python_1995} with NumPy \cite{walt_numpy_2011} and using the classical Runge-Kutta scheme \cite{kutta_beitrag_1901}.

In their experiments, \citeauthor{randlov_learning_1998} chose a time discretization of 0.01 seconds.
During this time, the action applied by the agent remains constant and after one such time step, the agent can choose a new action
This results in a controller frequency of \SI[mode=text]{100}{\Hz}.
A high frequency gives the agent a high degree of control which is often not possible to achieve in real systems.
The experiments in this thesis are based on the same time discretization of 0.01 seconds but only allow the actor to choose a new action every ten time steps, keeping it constant for the time steps in between.
This yields a controller frequency of \SI[mode=text]{10}{\Hz}.
Combined with the differential equations, the choice of time discretization completely defines the interface between the actor and the bicycle system.
The resulting transition dynamics used for the interaction of the controller and the system are called $\DynBicycle$ in the following.

Applying a controller to the bicycle benchmark produces time series beginning at some starting state and ending when the cyclist either falls down or reaches the goal.
It is assumed that no expert knowledge is available, so the data sets available for learning transition dynamics should not be based a controller which can successfully balance the bicycle.
Instead, this thesis chooses an uninformed controller which applies random actions to the system.

\Cref{alg:bicycle_transitions,alg:bicycle_trajectories,alg:bicycle_data_set} describe how data sets were created for the experiments.
A data set consists of both complete trajectories and single random samples from the state space.
A trajectory always starts in an upright position, that is, the state variables $\theta$, $\dot{\theta}$, $\omega$ and $\dot{\omega}$ are all set to zero, while the remaining positional variables are sampled uniformly.
This is both a sensible assumption and increases the mean lengths of the sampled trajectories when compared to more random starting states.
As shown in \cref{fig:data_set_properties:episode_length,fig:data_set_plot}, an average trajectory in the data set is quite short, since random actions are not suitable to balance the bicycle.

\Cref{fig:data_set_properties:omega_example} shows this in more detail, as it depicts the values of $\omega$ for a typical trajectory.
While the bicycle starts in an upright position, it quickly starts leaning heavily towards one side and, since the controller does not choose actions to stabilize the bicycle, falls over.
The sampled trajectories do not contain many state transitions where the bicycle drives straight or the actions counteract falling.

In order to reduce this bias, the data set also contains random samples from the complete state space as shown in \cref{alg:bicycle_data_set}.
While those random samples add more balanced observations of the system, they also increase the difficulty of the learning problem.
Not every combination of angles in the state space is sensible and can be reached from an upright starting state by applying actions.
The transition models therefore also have to learn irrelevant information about the dynamics.
A heuristic to reduce the amount of improbable states is to not sample the angles uniformly but rather to sample them from a broad random distribution around zero, resampling values which fall outside of the range of allowed values.
Since terminal states are modelled separately, both the last transition of a trajectory and all samples which result in a terminal state are removed from the data set.

\subsection{Gaussian Process Models}
The Gaussian process models for the transition dynamics are trained using data sets of the form $\D = \Set{(\mat{s_i}, \mat{a_i}, \mat{s_i^\prime}) \in \Es \times \Ah \times \Es \with i \in [N]}$ of pairs of states and actions and their corresponding following state $\mat{s_i^\prime} = \DynBicycle(\mat{s_i}, \mat{a_i})$.
Since the transition dynamics of the bicycle benchmark are deterministic, these observations have no probabilistic element and they are not noisy.
The model $f$ of the transition dynamics is a compact statistical representation of this collected knowledge and is to be used to predict successive states of unobserved combinations of states and actions $(\mat{s_\ast}, \mat{a_\ast})$.

Besides predicting a concrete following state, the model should provide a measure about the uncertainty of its predictions.
Since there is no randomness in the dynamics themselves, this uncertainty comes from the imperfect information about the true system dynamics and is dependent on the location of both the training data and the required predictions.
If a query is made to the model in a part of the state space in which it has not seen many observations, the model should express its uncertainty and not assume that its best guess is close to the truth.

\begin{figure}[tb]
    \centering
    \missingfigure[figheight=.25\textheight]{GP Posterior}
    \caption{GP posterior}
    \label{fig:gp_transition_models}
\end{figure}
The Gaussian processes presented in \cref{sec:gp_regression} represent a distribution over all plausible transition dynamics given a data set.
In figure \cref{fig:gp_transition_models}, the x-axis represents pairs of states and actions while the y-axis represents the successive state.
Since the observations are noise-free, the GP is completely certain about predicting them and, since it assumed a smooth RBF-prior, it is also confident about predicting states closed to the observed data.
Between the data points, uncertainties are higher since there are many different models which are plausible.
Gaussian processes are called \emph{non-degenerate}, since for predictions far away from the training set, the predicted uncertainty does not converge to zero.
In contrast, for parts of the input space without any knowledge, the GP falls back to the prior assumptions about uncertainties and the mean function.
Given a large enough data set which is spread out through the complete state and action space, the model becomes more and more confident about its predictions and converges towards the true transition dynamics.

Gaussian processes as presented in this thesis can only model functions with univariate output.
Approximating successive states requires multivariate predictions however.
While there do exist extensions of Gaussian processes for multidimensional output \cite{rasmussen_gaussian_2006}, a common solution is to train $D$ separate GPs, one for every dimension in the state space $\R^D$.
While this requires longer training time, it allows choosing a different set of hyperparameters for every dimension.
Since the training set does not contain transitions which result in terminal states, the transition models do not know about them.
The signature of the function represented by the transition model is $f : \Es \times \Ah \to \Es$, where $\Es = \R^D$ and $\Ah = R^k$.
Similar to the absence of terminal states, the transition models are also not aware of the rectangular boundaries of the state space described in \cref{tab:bicycle_variables}, which means that it is possible for the transition models to predict illegal states, which also have to be handled separately.

All models are trained using the squared exponential kernel presented in \cref{def:rbf_kernel}.
The bicycle benchmark represents a physical system, which makes smoothness of the transition function a natural assumption.
The RBF kernel is the standard choice in Gaussian process regression when no special knowledge about the shape of the transition function is available\todo{maybe mention other kernels? Is there anything to say about them?}.

Opposed to learning successive states directly, the training targets for the $d$th dimension are the differences to the current state given by
\begin{align}
    \Delta s_{i,d} \coloneqq \DynBicycle(\mat{s_i}, \mat{a_i})_d - s_{i,d} = s_{i,d }^\prime - s_{i,d},
\end{align}
where $i \in [N]$ and $d \in [D]$.
This can be advantageous since differences vary less than the original function \cite{deisenroth_efficient_2010}.
Learning differences can also introduce independences in the data, since predicting the change in position of the bicycle only depends on the direction of movement but not on the previous position.
Having learned models for the differences, the mean and the variance of the Gaussian posterior state distribution $\Prob{f_d(\mat{s_\ast}, \mat{a_\ast})}$ is given by
\begin{align}
    \Moment*{\E}{f_d(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} &= s_{\ast,d} + \Moment*{\E}{\Delta s_{\ast,d} \given \mat{s_\ast}, \mat{a_\ast}}, \\
    \Moment*{\var}{f_d(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} &= \Moment*{\var}{\Delta s_{\ast,d} \given \mat{s_\ast}, \mat{a_\ast}},
\end{align}
respectively, since the prior state is considered constant and non-probabilistic.
Uncertainties in the predictions only originate from the amount of confidence expressed by the models for the differences.
The values of the expected value and variances are calculated according to \cref{lem:gp_posterior}.

Since the different Gaussian processes are trained independently of each other and their training sets only contain their respective output dimension, their predictions are conditionally independent given the input.
With the predictive distribution for the single dimension being Gaussian, the joint predictive state distribution is also Gaussian with a diagonal covariance matrix and is given by
\begin{align}
    \Prob*{f(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} &=
    \Gaussian*{f(\mat{s_\ast}, \mat{a_\ast}) \given \mat{\mu_f}, \mat{\Sigma_f}}\text{, where} \\
    \mat{\mu_f} &= \begin{pmatrix}
    \Moment*{\E}{f_1(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} \\
    \vdots \\
    \Moment*{\E}{f_D(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} \\
    \end{pmatrix} \\
    \mat{\Sigma_f} &= \diag\left(
        \Moment*{\var}{f_1(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}},
        \cdots,
        \Moment*{\var}{f_D(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}}
    \right).
\end{align}
This diagonal covariance matrix illustrates the implicit independence assumption of the different output dimensions introduced by training one model per output dimension.
While this assumption is not true in most cases, it can be used as an approximation and generally yields good results.

The state of the bicycle system is given by a vector $\Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$ composed of the internal dynamics of the bicycle and its position and orientation in euclidean space.
During simulation with the transition model, the coordinates were transformed to polar coordinates given by
\begin{align}
    \varphi(x, y) &\coloneqq \atanTwo(y, x) \\
    r(x, y) &\coloneqq \sqrt{x^2 + y^2},
\end{align}
where $\atanTwo$ is the arctangent function with two arguments.
Polar coordinates uniquely represent a two-dimensional point by its angle to the x-axis and its distance to the origin.
This representation both increases model performance and simplifies calculating the bicycle's relative position to the goal in the origin.

Additionally, representing both $\varphi$ and $\psi$ as numbers between $-\pi$ and $\pi$ leads to a loss of information.
While two angles with absolute value close to $\pi$ but opposite signs are close together on a circle, their representations have a large euclidean distance.
A Gaussian Process using the RBF-Kernel cannot recognize their similarity.
In this case, it is possible to choose a specialized periodic variant of the squared exponential kernel which recognizes periodicity.
Equivalently, an angle can be represented as a complex number on the unit circle, replacing it by its sine and cosine.
Therefore, the internal representation of a bicycle state in the simulation is given by a vector
\begin{align}
    \mat{s} &= \Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, \varphi, r, \psi} \in \R^7 \\
    \intertext{which is transformed to}
    \mat{\hat{s}} &= \Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, \sin\varphi, \cos\varphi, r, \sin\psi, \cos\psi} \in \R^9
\end{align}
when presented to the GPs.
The transition model consists of seven Gaussian processes, each with nine-dimensional input.

The models are implemented in Python using \citeauthor{titsias_variational_2009}'s sparse variational GP regression implemented in \citetitle{gpy_gpy:_2012} \cite{gpy_gpy:_2012} and trained using expectation maximization as presented in \cref{sub:gp_hyperparameters}.
The optimization of the likelihood function is calculated using scaled conjugated gradients with multiple restarts to avoid local minima.

The performance of the transition models is highly dependent on the size of the training set $N$ and the number of inducing inputs $M$.
For $N$ smaller than 35000, the performance of the transition models for long-term predictions was not good enough to allow PSO-P to succeed for any of the approaches presented below.
Conversely, for large $N$ and $M$ larger than 250, the models are good enough such that PSO-P finds perfect solutions for all approaches.
The experiments in this thesis focus on choices for $N$ and $M$ which are inbetween these extremes and where information about the model uncertainties can be used to improve performance.
The next section presents the classic approach of long-term predictions without the use of uncertainty information, which is used as a baseline for comparison for the following techniques.

\section{Predictions without Uncertainties}
The transition model trained on a predefined data set allows the prediction of a successive state distribution $\Prob{\mat{s_1}}$ given a deterministic pair of a state and an action $(\mat{s_0}, \mat{a_0})$.
To evaluate the action value function $\AVlu$, two extensions need to be made.
Firstly, beyond specifying the goal, \cref{cha:the_bicycle_benchmark} does not define a concrete reward function.
This section introduces a variant of the reward function used by \citeauthor{randlov_learning_1998} in \cite{randlov_learning_1998}.

And secondly, for a time horizon $T$ longer than one step into the future, the predictive state distributions $\Prob{\mat{s_1}}$ up to $\Prob{\mat{s_T}}$ are required.
Since the GP dynamics model returns a Gaussian predictive distribution for all states beyond the starting state to account for the model uncertainty, all states beyond the starting state are no longer deterministic.
In order to mimic a classic non-Bayesian model without a measure of uncertainty, the approach presented in this section discards this information and considers the maximum-a-posteriori estimation to be the deterministic prediction of the transition model.
Having established the deterministic mode of evaluating the action value function, this section finally introduces the evaluation setup used in this thesis and discusses the results of applying this technique to the bicycle system.

\subsection{Deterministic Bicycle Reward Function}
Solving the bicycle benchmark is a composite problem.
An agent has to both learn to balance a bicycle and drive to the goal.
Instead of having to solve the two tasks one after the other, they both have to be solved simultaneously, switching between them.
While an agent is in control of the bicycle, it can try to drive towards the goal.
If any action applied to the system leads to the danger of falling over however, the agent has to quickly change its focus towards preventing this.

Without expert knowledge available, the controller must learn this distinction autonomously, given the reward function.
The most basic and uninformed reward function possible assigns positive reward for reaching the goal, a punishment (in the form of negative reward) for falling over and weighs all other states equally between the two extremes.
While this can be enough to teach the short-term task of avoiding to fall down, the agent has no initiative of driving towards the goal besides actually hitting it.
For most situations, the goal cannot be reached within the time-horizon of one PSO-P instance.
In this setting, PSO-P would optimize towards a trajectory for which the chance of falling down is minimal.
This trajectory is a circle with large radius~\cite{randlov_learning_1998}.

To give the controller a chance of reaching the goal, it has to receive some hint about the correct direction to drive.
Encoding this information in the reward function goes against the assumption of the complete absence of expert knowledge.
If it is too detailed, it introduces the risk of significantly simplifying the learning problem or pushing the agent towards a policy which is only locally optimal.
This reduction of the hard problem of finding the goal to a series of easier problems of driving in the correct direction and then going straight is called \emph{shaping} \cite{sutton_reinforcement_1998,randlov_learning_1998}.

The hint towards the goal encoded in the reward function should be a term which represents information which is local in the sense that its value can change considerably within the time horizon.
The most simple term to consider is a punishment based on the current distance to the goal.
This formulation is problematic however, since the agent should not care about the actual distance rather than the change of distance with respect to the previous state, which cannot be expressed in the reward.
While an increase in reward would express movement in the correct direction, for any non-linear punishment, the amount of increase is dependent on the current position in the input space and can lead to numerical problems if it gets to small.
If it were linear, the punishment might at some point be larger than the punishment for falling.
At this point, the agent's correct choice would be to fall down as quickly as possible.

In order to avoid these problems, the reward function used in this thesis uses the current angle between the frame of the bicycle and the direction towards the goal as a hint.
Since the goal's position is at the origin of the coordinate system, this angle can be calculated as the difference of the current rotation of the bicycle $\psi$ and the angular component of its polar coordinates in space $\varphi$.
This difference can be scaled to an interval between zero and one and can take values in the complete range if the time horizon is long enough to contain a curve of the bicycle.
The reward is scaled to be zero if the bicycle points straight away from the goal and one if directly towards it.
If the bicycle has reached the goal, the agent receives a constant award of two which is double the amount which can be obtained for any state which is not in the goal.
Similarly, the reward for falling is constant zero.

\begin{definition}[Deterministic Bicycle Reward Function]
    Given the set $\Es^+$ of possible states of the bicycle benchmark, the \emph{deterministic bicycle reward function} is defined as
    \begin{align}
        \RwdBicycle : \left\{
            \begin{aligned}
                \Es^+ &\to \R \\
                \mat{s} &\mapsto \begin{cases}
                    2 & \text{if $\mat{s} \in \Goal$} \\
                    0 & \text{if $\mat{s} \in \Fallen$} \\
                    1 - \frac{\abs{\psi_{\mat{s}} - \varphi_{\mat{s}}}}{\pi} & \text{otherwise}
                \end{cases}
            \end{aligned}
        \right.
    \end{align}
    where $\psi_{\mat{s}}$ and $\varphi_{\mat{s}}$ denote the respective angles in state $\mat{s}$.
    The difference is defined to take values between $-\pi$ and $\pi$.

    The reward function assigns constant reward for terminal states and reward inversely proportional to the angle between the bicycle's heading and the goal otherwise.
\end{definition}

The agent does not receive negative reward for falling down.
Instead, the reward is zero, which is equal to pointing away from the goal.
The agent is still punished when falling down though, since the episode has to end and the agent is not able to collect additional reward by driving towards the goal.

\subsection{Long-Term predictions}
\begin{align}
    \Es &\coloneqq \Set*{\mat{s} \in \R^7 \with \forall i : \mat{s}_i \in \left[ \mat{s}_i^{\text{min}}, \mat{s}_i^{\text{max}} \right]} \\
    \Goal &\coloneqq \Set*{\mat{s} \in \R^7 \with r_{\mat{s}} \leq 5 \vphantom{\frac{\pi}{15}}} \\
    \Fallen &\coloneqq \Set*{\mat{s} \in \R^7 \with \abs{\omega_{\mat{s}}} > \frac{\pi}{15}} \setminus \Goal \\
\end{align}
and $\Tee = \Goal \cup \Fallen$, $\Es^+ = \Es \cup \Tee$.

\begin{align}
    \argmax_{\mat{s_{t+1}} \in \R^7} \Prob{\mat{s_{t+1}} \given \mat{s_t}, \mat{a_t}} &= \argmax_{\mat{s_{t+1}} \in \R^7} f\Cond{\mat{s_{t+1}} \given \mat{s_t}, \mat{a_t}} \\
    &= \argmax_{\mat{s_{t+1}} \in \R^7} \Gaussian{\mat{s_{t+1}} \given \mat{\mu_{t+1}}, \mat{\Sigma_{t+1}}} \\
    &= \Set{\mat{\mu_{t+1}}}
\end{align}

\begin{align}
    \map{\mat{s_{t+1}}} : \left\{
        \begin{aligned}
            \Es^+ \times \Ah &\to \Es^+ \\
            (\mat{s_t}, \mat{a_t}) &\mapsto \begin{cases}
            \mat{s_t} & \text{if $\mat{s_t} \in \Tee$} \\
            \squeeze(\mat{\mu_{t+1}}) & \text{otherwise}
        \end{cases}
    \end{aligned}
    \right.
\end{align}
where $\squeeze$ is a function to ensure physical bounds, i.e. which fixes everything besides $\omega$.

\begin{align}
    \AVlu(\mat{a_0}, \dots, \mat{a_{T-1}}) &= \Moment*{\E}{\sum_{t=0}^T \gamma^t \RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
    &= \sum_{t=0}^T \gamma^t \Moment*{\E}{\RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
    &= \sum_{t=0}^T \gamma^t \RwdBicycle(\map{\mat{s_t}})
\end{align}
where $\map{\mat{s_0}} = \mat{s_0}$ and all the other states are calculated using dynamic programming.
\subsection{Evaluation Setup}
\subsection{Results and Problems}
\section{Predictions with One-Step Uncertainties}
\subsection{Probabilistic reward function}
\subsection{Long-Term predictions}
\subsection{Results and Problems}
\section{Predictions with Multi-Step Uncertainties}
\subsection{Linearization}
\subsection{Truncation of Gaussians}
\subsection{Results and Problems}
