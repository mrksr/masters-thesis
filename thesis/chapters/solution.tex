\chapter{Uncertainties in the Bicycle Benchmark}
\label{cha:solution}
\Cref{cha:theory} introduced reinforcement learning as a general mathematical framework to describe the problem of controlling a bicycle in the benchmark presented in \cref{cha:the_bicycle_benchmark}.
This thesis is concerned with model-based reinforcement learning, where the transition function of the system to be controlled is represented with some function approximation which is learnt from observations of the system and is used to make predictions about the future.
Learning a model of the true system introduces model bias, where actions considered to be good with respect to the model's predictions can show bad performance in reality because the model is incorrect.
In order to reduce this bias, Gaussian Processes can be used which do not only yield one specific function approximation but a distribution over all plausible models.
This uncertainty about the correct model can be propagated through to predictions about specific test points and instead of a single point, Gaussian processes predict a Gaussian distribution about possible function values.

Modelling the transition dynamics allows the prediction of a state $\mat{s_{t+1}}$ given a concrete pair of state and action $(\mat{s_t}, \mat{a_t})$ for the previous time step.
Assuming a deterministic model which yields exactly one posterior state, the \emph{long-term prediction} of states multiple time steps into the future reduces to iterated one-step predictions.
Given a (deterministic) reward function $\Rwd$ as detailed in \cref{def:reward_function}, it is possible to evaluate the action value function $\AVlu$ of PSO-P and thus directly use the model to extract a policy, since the expected value in \cref{eq:action_value_function} is a simple sum of deterministic values.

In the Bayesian context however, the transition model predicts a distribution over posterior states.
This complicates long-term predictions since the uncertainty of intermediate states has to be propagated through the (non-linear) transition model to accumulate the uncertainties of multiple predictions.
Additionally, the original deterministic reward function possibly has to be adapted to make it possible to evaluate the expected reward for all time steps.
Once these problems are solved and the action value function can be calculated, PSO-P can be used in the same way as in the deterministic case to choose appropriate actions.

Based on the bicycle benchmark, this chapter compares the classical deterministic long-term predictions to two approaches of integrating uncertainty into the predictions.
The first section describes the creation of data sets used to train the transition model and the design-choices made to obtain suitable models.
These models are then interpreted as deterministic to obtain a base-line for comparison in the next section.
The last two sections describe how to use uncertainties in the planning.
The first approach is to use the one-step uncertainties of the Gaussian Process models in the reward function but to still create long-term predictions using deterministic states.
The second fully Bayesian approach completely propagates state uncertainties both to the reward function and subsequent states.

\section{Transition Dynamics}
\label{sec:transition_dynamics}
The goal in the bicycle benchmark is to learn to both balance a bicycle and to ride it to a target position.
The state of a bicycle, as described in \cref{cha:the_bicycle_benchmark}, consists of the real valued vector $\Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$, where $\theta$ is the angle of the handlebars, $\omega$ is the vertical angle of the bicycle frame, $x$ and $y$ are euclidean coordinates and $\psi$ is the orientation of the bicycle.
The bicycle starts in almost upright position and the task of the controller is to choose the actions $\Bicycle{d, T}$, where $d$ is the horizontal leaning displacement of the driver and $T$ is the torque applied to the handlebars at every time step.
The transition dynamics are derived from a physical approximation of the system and are completely deterministic.

This thesis assumes that the actor is not allowed to interact with the system in order to try or improve its policy.
In contrast, the agent is presented with a predefined data set of observations of the system obtained with a simple and sub-optimal controller.
This constraint is meant to mimic industrial systems where it is comparatively cheap to obtain measurements of running systems but allowing an agent to explore is either very expensive or a security concern.
It is therefore not possible to apply an on-line learning scheme on the system or to explore in specific directions in order to improve the dynamics model.

This section first describes how data sets are sampled from the bicycle benchmark in order to simulate this constraint.
These data sets are then used to train the Gaussian Processes used in PSO-P to form a controller.

\subsection{Data Sets}
\begin{figure}[tp]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.35\textheight]{Length of Episodes}
        \caption{Length of Episodes}
        \label{fig:data_set_properties:episode_length}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \missingfigure[figheight=.35\textheight]{Omega-Plot for one episode}
        \caption{Omega-Plot for one episode}
        \label{fig:data_set_properties:omega_example}
    \end{subfigure}
    \caption{Data Set properties}
    \label{fig:data_set_properties}
\end{figure}
\begin{algorithm}[tp]
    \caption{Sampling bicycle transitions}
    \label{alg:bicycle_transitions}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    The minimal and maximal values for the state variables and the actions can be found in \cref{tab:bicycle_variables,tab:bicycle_actions}.
    \begin{algorithmic}[1]
        \Function{SampleBicycleState}{}
            \State $\left( \theta, \dot{\theta}, \omega, \dot{\omega} \right) \gets \Gaussian{\mat{0}, \sfrac{1}{4} \cdot \diag \left( \theta^{\text{max}}, \dot{\theta}^{\text{max}}, \omega^{\text{max}}, \dot{\omega}^{\text{max}} \right)}$
            \State $x \gets \Uniform{x^{\text{min}}, x^{\text{max}}}$
            \State $y \gets \Uniform{y^{\text{min}}, y^{\text{max}}}$
            \State $\psi \gets \Uniform{-\pi, \pi}$
            \State \Return $\Bicycle*{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleAction}{}
            \State $d \gets \Uniform{d^{\text{min}}, d^{\text{max}}}$
            \State $T \gets \Uniform{T^{\text{min}}, T^{\text{max}}}$
            \State \Return $\Bicycle{d, T}$
        \EndFunction
        \Statex
        \Function{SampleTransitions}{$N$}
            \For{$i \gets 1, N$}
                \State $\mat{s_i} \gets \Call{SampleBicycleState}$
                \State $\mat{a_i} \gets \Call{SampleAction}$
                \State $\mat{s^\prime_{i}} \gets \DynBicycle(\mat{s_i}, \mat{a_i})$
            \EndFor
            \State \Return $\left( \left( \mat{s_1}, \mat{a_1}, \mat{s^\prime_1} \right), \left( \mat{s_2}, \mat{a_2}, \mat{s^\prime_2} \right), \dots, \left( \mat{s_{N}}, \mat{a_{N}}, \mat{s^\prime_{N}} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[tp]
    \caption{Sampling a bicycle trajectory}
    \label{alg:bicycle_trajectories}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    \begin{algorithmic}[1]
        \Function{SampleBicycleStartState}{}
            \State $\Bicycle*{\_, \_, \_, \_, x, y, \psi} \gets \Call{SampleBicycleState}$
            \State \Return $\Bicycle*{0, 0, 0, 0, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleTrajectory}{}
            \State $\mat{s_0} \gets \Call{SampleBicycleStartState}$
            \State $t \gets 0$
            \While{$\mat{s_t}$ is not terminal}
                \State $\mat{a_t} \gets \Call{SampleAction}$
                \State $\mat{s_{t+1}} \gets \DynBicycle(\mat{s_t}, \mat{a_t})$
                \State $t \gets t + 1$
            \EndWhile
            \State \Return $\left( \left( \mat{s_0}, \mat{a_0}, \mat{s_1} \right), \left( \mat{s_1}, \mat{a_1}, \mat{s_2} \right), \dots, \left( \mat{s_{t-1}}, \mat{a_{t-1}}, \mat{s_t} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[tp]
    \caption{Sampling a bicycle data set}
    \label{alg:bicycle_data_set}
    \begin{algorithmic}[1]
        \Function{SampleMixedBicycleDataSet}{$N$}
            \State $\Es \gets \emptyset$
            \While{$\abs{\Es} < N$}
                \State $T \gets \Call{SampleTrajectory}$
                \State $R \gets \Call{SampleTransitions}{\abs{T}}$
                \State $\Es \gets \Es \cup T \cup R$
            \EndWhile
            \State \Return $\Es$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{figure}[tp]
    \centering
    \missingfigure[figheight=.5\textheight]{Positional Plot of a few Episodes}
    \caption{Positional Plot of a few Episodes}
    \label{fig:data_set_plot}
\end{figure}
The bicycle benchmark's dynamics are introduced in \cref{cha:the_bicycle_benchmark} by defining the derivatives of the state variables and choosing values for the relevant constants in \cref{tab:bicycle_constants}.
Given a starting state and an appropriate number of actions, these derivatives can be used to approximate the future behaviour of the system using iterative numerical methods for approximating ordinary differential equations.
For this thesis, the bicycle benchmark was implemented in Python \cite{rossum_python_1995} with NumPy \cite{walt_numpy_2011} and using the classical Runge-Kutta scheme \cite{kutta_beitrag_1901}.

In their experiments, \citeauthor{randlov_learning_1998} chose a time discretization of 0.01 seconds.
During this time, the action applied by the agent remains constant and after one such time step, the agent can choose a new action
This results in a controller frequency of \SI[mode=text]{100}{\Hz}.
A high frequency gives the agent a high degree of control which is often not possible to achieve in real systems.
The experiments in this thesis are based on the same time discretization of 0.01 seconds but only allow the actor to choose a new action every ten time steps, keeping it constant for the time steps in between.
This yields a controller frequency of \SI[mode=text]{10}{\Hz}.
Combined with the differential equations, the choice of time discretization completely defines the interface between the actor and the bicycle system.
The resulting transition dynamics used for the interaction of the controller and the system are called $\DynBicycle$ in the following.

Applying a controller to the bicycle benchmark produces time series beginning at some starting state and ending when the cyclist either falls down or reaches the goal.
It is assumed that no expert knowledge is available, so the data sets available for learning transition dynamics should not be based a controller which can successfully balance the bicycle.
Instead, this thesis chooses an uninformed controller which applies random actions to the system.

\Cref{alg:bicycle_transitions,alg:bicycle_trajectories,alg:bicycle_data_set} describe how data sets were created for the experiments.
A data set consists of both complete trajectories and single random samples from the state space.
A trajectory always starts in an upright position, that is, the state variables $\theta$, $\dot{\theta}$, $\omega$ and $\dot{\omega}$ are all set to zero, while the remaining positional variables are sampled uniformly.
This is both a sensible assumption and increases the mean lengths of the sampled trajectories when compared to more random starting states.
As shown in \cref{fig:data_set_properties:episode_length,fig:data_set_plot}, an average trajectory in the data set is quite short, since random actions are not suitable to balance the bicycle.

\Cref{fig:data_set_properties:omega_example} shows this in more detail, as it depicts the values of $\omega$ for a typical trajectory.
While the bicycle starts in an upright position, it quickly starts leaning heavily towards one side and, since the controller does not choose actions to stabilize the bicycle, falls over.
The sampled trajectories do not contain many state transitions where the bicycle drives straight or the actions counteract falling.

In order to reduce this bias, the data set also contains random samples from the complete state space as shown in \cref{alg:bicycle_data_set}.
While those random samples add more balanced observations of the system, they also increase the difficulty of the learning problem.
Not every combination of angles in the state space is sensible and can be reached from an upright starting state by applying actions.
The transition models therefore also have to learn irrelevant information about the dynamics.
A heuristic to reduce the amount of improbable states is to not sample the angles uniformly but rather to sample them from a broad random distribution around zero, resampling values which fall outside of the range of allowed values.

\subsection{Gaussian Process Models}
\begin{itemize}
    \item Model the deltas
    \item Terminal states
    \item One model per dimension
    \item Represent angles with sines and cosines
    \item Polar coordinates
    \item We want analytically independent dimensions
    \item Implicit assumption that outputs are independent
    \item Which model depends on which dimension
    \item Values for N, M, Kernel
        \begin{itemize}
            \item N too low: Models very bad
            \item N large, M large: Models very good
            \item We are interested in the behaviour in-between
            \item We use RBF kernels, trials seemed that others do not behave much
                better
        \end{itemize}
    \item Training Parameters
        \begin{itemize}
            \item Algorithm
            \item Multiple restarts
            \item Many iterations
        \end{itemize}
    \item Explicit posterior distribution?
    \item Interpretation of uncertainties
    \item Behaviour of GPs (e.g. away from data)
\end{itemize}
\section{Predictions without Uncertainties}
\subsection{Deterministic reward function}
\begin{definition}[Deterministic Bicycle Reward Function]
    Given the set $\Es^+$ of possible states of the bicycle benchmark, the \emph{deterministic bicycle reward function} is defined as
    \begin{align}
        \RwdBicycle : \left\{
            \begin{aligned}
                \Es^+ &\to \R \\
                \mat{s} &\mapsto \begin{cases}
                    2 & \text{if $\mat{s} \in \Goal$} \\
                    0 & \text{if $\mat{s} \in \Fallen$} \\
                    1 - \frac{\abs{\psi_{\mat{s}} - \varphi_{\mat{s}}}}{\pi} & \text{otherwise}
                \end{cases}
            \end{aligned}
        \right.
    \end{align}
    where $\psi_{\mat{s}}$ and $\varphi_{\mat{s}}$ denote the respective angles in state $\mat{s}$.
    It assigns constant reward for terminal states and reward inversely proportional to the angle between the bicycle's heading and the goal otherwise.
\end{definition}
\subsection{Long-Term predictions}
\subsection{Results and Problems}
\section{Predictions with One-Step Uncertainties}
\subsection{Probabilistic reward function}
\subsection{Long-Term predictions}
\subsection{Results and Problems}
\section{Predictions with Multi-Step Uncertainties}
\subsection{Linearization}
\subsection{Truncation of Gaussians}
\subsection{Results and Problems}
