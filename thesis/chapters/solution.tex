\chapter{Incorporating Uncertainty in Model-Based Reinforcement Learning}
\label{cha:solution}
\Cref{cha:theory} introduced reinforcement learning as a general mathematical framework to describe the problem of controlling a bicycle in the benchmark presented in \cref{cha:the_bicycle_benchmark}.
This thesis is concerned with model-based reinforcement learning, where the transition function of the system to be controlled is represented with some function approximation which is learnt from observations of the system and is used to make predictions about the future.
Learning a model of the true system introduces model bias, where actions considered to be good with respect to the model's predictions can show bad performance in reality because the model is incorrect.
In order to reduce this bias, Gaussian processes can be used as they do not only yield one specific function approximation but a distribution over all plausible models.
This uncertainty about the correct model can be propagated through to predictions about specific test points and instead of a single point, Gaussian processes predict a Gaussian distribution about possible function values.

Modeling the transition dynamics allows the prediction of a state $\mat{s_{t+1}}$ given a concrete pair of state and action $(\mat{s_t}, \mat{a_t})$ for the previous time step.
Assuming a deterministic model which yields exactly one posterior state, the \emph{long-term prediction} of states multiple time steps into the future reduces to iterated one-step predictions.
Given a reward function $\Rwd$ as detailed in \cref{def:reward_function}, it is possible to evaluate the action value function $\AVlu$ of PSO-P and thus directly use the model to extract a policy, since the expected value in \cref{eq:action_value_function} is a simple sum of deterministic values.

In the Bayesian context however, the transition model predicts a distribution over posterior states.
This complicates long-term predictions since the uncertainty of intermediate states has to be propagated through the nonlinear transition model to accumulate the uncertainties of multiple predictions.
Additionally, the reward function must be formulated in such a way that it is possible to evaluate the expected reward for all time steps.
Once these problems are solved and the action value function can be calculated, PSO-P can be used in the same way as in the deterministic case to choose appropriate actions.

Based on the bicycle benchmark, this chapter compares the classical deterministic long-term predictions to two approaches of integrating uncertainty into the predictions.
The first section describes the creation of data sets and the design-choices made to obtain suitable models.
These models are then interpreted as deterministic to obtain a base-line for comparison in the next section.
The last two sections describe how to use uncertainties in the planning process.
The first approach is to use the one-step uncertainties of the Gaussian process models in the reward function but to still create long-term predictions using deterministic states.
The second fully Bayesian approach completely propagates state uncertainties both to the reward function and subsequent states.

\section{Transition Models}
The goal in the bicycle benchmark is to learn to both balance a bicycle and to ride it to a target position.
The state of a bicycle, as described in \cref{cha:the_bicycle_benchmark}, consists of the real valued vector $\Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$, where $\theta$ is the angle of the handlebars, $\omega$ is the vertical angle of the bicycle frame, $x$ and $y$ are euclidean coordinates and $\psi$ is the orientation of the bicycle.
The bicycle starts in almost upright position and the task of the controller is to choose the actions $\Bicycle{d, T}$ at every time step, where $d$ is the horizontal leaning displacement of the driver and $T$ is the torque applied to the handlebars at every time step.
The transition dynamics are derived from a physical approximation of the system and are completely deterministic.

This thesis assumes that the actor is not allowed to interact with the system in order to evaluate or improve its policy.
In contrast, the agent is presented with a predefined data set of observations of the system obtained with a simple and sub-optimal controller.
This constraint is meant to mimic industrial systems where it is comparatively cheap to obtain measurements of running systems but allowing an agent to explore is either very expensive or a security concern.
It is therefore not possible to apply an on-line learning scheme on the system or to explore in specific directions in order to improve the dynamics model.

This section first describes how data sets are sampled from the bicycle benchmark in order to simulate this constraint.
These data sets are then used to train the Gaussian processes used by PSO-P.

\subsection{Data Sets}
\label{sub:data_sets}
\begin{algorithm}[p]
    \caption{Sampling bicycle transitions}
    \label{alg:bicycle_transitions}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    The minimal and maximal values for the state variables and the actions can be found in \cref{tab:bicycle_variables,tab:bicycle_actions}.
    \begin{algorithmic}[1]
        \Function{SampleBicycleState}{}
            \State $\left( \theta, \dot{\theta}, \omega, \dot{\omega} \right) \gets \Gaussian{\mat{0}, \sfrac{1}{4} \cdot \diag \left( \theta^{\text{max}}, \dot{\theta}^{\text{max}}, \omega^{\text{max}}, \dot{\omega}^{\text{max}} \right)}$
            \State $x \gets \Uniform{x^{\text{min}}, x^{\text{max}}}$
            \State $y \gets \Uniform{y^{\text{min}}, y^{\text{max}}}$
            \State $\psi \gets \Uniform{-\pi, \pi}$
            \State \Return $\Bicycle*{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleAction}{}
            \State $d \gets \Uniform{d^{\text{min}}, d^{\text{max}}}$
            \State $T \gets \Uniform{T^{\text{min}}, T^{\text{max}}}$
            \State \Return $\Bicycle{d, T}$
        \EndFunction
        \Statex
        \Function{SampleTransitions}{$N$}
            \For{$i \gets 1, N$}
                \State $\mat{s_i} \gets \Call{SampleBicycleState}$
                \State $\mat{a_i} \gets \Call{SampleAction}$
                \State $\mat{s^\prime_{i}} \gets \DynBicycle(\mat{s_i}, \mat{a_i})$
            \EndFor
            \State \Return $\left( \left( \mat{s_1}, \mat{a_1}, \mat{s^\prime_1} \right), \left( \mat{s_2}, \mat{a_2}, \mat{s^\prime_2} \right), \dots, \left( \mat{s_{N}}, \mat{a_{N}}, \mat{s^\prime_{N}} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[p]
    \caption{Sampling a bicycle trajectory}
    \label{alg:bicycle_trajectories}
    Let $\DynBicycle$ denote the transition function of the bicycle benchmark.
    \begin{algorithmic}[1]
        \Function{SampleBicycleStartState}{}
            \State $\Bicycle*{\_, \_, \_, \_, x, y, \psi} \gets \Call{SampleBicycleState}$
            \State \Return $\Bicycle*{0, 0, 0, 0, x, y, \psi}$
        \EndFunction
        \Statex
        \Function{SampleTrajectory}{}
            \State $\mat{s_0} \gets \Call{SampleBicycleStartState}$
            \State $t \gets 0$
            \While{$\mat{s_t}$ is not terminal}
                \State $\mat{a_t} \gets \Call{SampleAction}$
                \State $\mat{s_{t+1}} \gets \DynBicycle(\mat{s_t}, \mat{a_t})$
                \State $t \gets t + 1$
            \EndWhile
            \State \Return $\left( \left( \mat{s_0}, \mat{a_0}, \mat{s_1} \right), \left( \mat{s_1}, \mat{a_1}, \mat{s_2} \right), \dots, \left( \mat{s_{t-1}}, \mat{a_{t-1}}, \mat{s_t} \right) \right)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[p]
    \caption{Sampling a bicycle data set}
    \label{alg:bicycle_data_set}
    \begin{algorithmic}[1]
        \Function{SampleMixedBicycleDataSet}{$N$}
            \State $\D \gets \emptyset$
            \While{$\abs{\D} < N$}
                \State $T \gets \Call{SampleTrajectory}$
                \State $R \gets \Call{SampleTransitions}{\abs{T}}$
                \State $\D \gets \D \cup T \cup R$
            \EndWhile
            \State \Return $\D$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\begin{figure}[p]
    \centering
    \includestandalonewithpath{figures/solution_data_xy_trajectories}
    \caption[Episodes in the training set]{
        The $x$ and $y$ coordinates of the front tyre in representative episodes in the training set with marks at the starting states.
        Since random actions cannot successfully balance the bicycle, episodes are fairly short and end with the bicycle falling over.
    }
    \label{fig:data_set_plot}
\end{figure}
The bicycle benchmark's dynamics are introduced in \cref{cha:the_bicycle_benchmark} by defining the derivatives of the state variables and choosing values for the relevant constants in \cref{tab:bicycle_constants}.
Given a starting state and an appropriate number of actions, these derivatives can be used to approximate the future behaviour of the system using iterative numerical methods for approximating ordinary differential equations.
For this thesis, the bicycle benchmark was implemented in Python \cite{van_rossum_python_1995} with NumPy \cite{walt_numpy_2011} and using the classical Runge-Kutta scheme \cite{kutta_beitrag_1901}.

In their experiments, \citeauthor{randlov_learning_1998} chose a time discretization of 0.01 seconds.
During this time, the action applied by the agent remains constant and after one such time step, the agent can choose a new action.
This results in a controller frequency of \SI[mode=text]{100}{\Hz}.
A high frequency gives the agent a high degree of control which is often not possible to achieve in real systems.
The experiments in this thesis are based on the same time discretization of 0.01 seconds but only allow the actor to choose an action every ten time steps, keeping it constant for the time steps in between.
This yields a controller frequency of \SI[mode=text]{10}{\Hz}.
The choice of time discretization is the only free parameter in the transition dynamics for the bicycle system described in \cref{cha:the_bicycle_benchmark}.
The resulting transition dynamics used for the interaction of the controller and the system are called $\DynBicycle$ below.

Applying a controller to the bicycle benchmark produces time series beginning at some starting state and ending when the cyclist either falls down or reaches the goal.
It is assumed that no expert knowledge is available, so the data sets available for learning transition dynamics should not be based a controller which can successfully balance the bicycle.
Instead, this thesis chooses an uninformed controller which applies random actions to the system.

\begin{figure}[t]
    \centering
    \begin{subfigure}{\subfigurewidth}
        \centering
        \includestandalonewithpath{figures/solution_data_lengths}
        \caption{Histogram of episode lengths}
        \label{fig:data_set_properties:episode_length}
    \end{subfigure}
    \begin{subfigure}{\subfigurewidth}
        \centering
        \includestandalonewithpath{figures/solution_data_omega}
        \caption{The variable $\omega$ for one episode}
        \label{fig:data_set_properties:omega_example}
    \end{subfigure}
    \caption[Data set properties]{
        \Cref{fig:data_set_properties:episode_length} shows the distribution of episode lengths in a typical training set.
        Most trajectories are between five and ten time steps long, with a clear peak at six steps.
        \Cref{fig:data_set_properties:omega_example} shows the development of the angle $\omega$ for an exceptionally long episode.
        Even for long episodes, the bicycle quickly starts leaning towards one side and does not recover.
        The last value of $\omega$ is in the valid range of values, since the last transition which results in a terminal state is not par of the training set.
    }
    \label{fig:data_set_properties}
\end{figure}
\Cref{alg:bicycle_transitions,alg:bicycle_trajectories,alg:bicycle_data_set} describe how data sets were created for the experiments.
A data set consists of both complete trajectories and single random samples from the state space.
A trajectory always starts in an upright position.
The state variables $\theta$, $\dot{\theta}$, $\omega$ and $\dot{\omega}$ are all set to zero, while the remaining positional variables are sampled uniformly.
This is both a sensible assumption about the distribution of starting states and also increases the mean lengths of the sampled trajectories when compared to more random starting states.
As shown in \cref{fig:data_set_properties:episode_length,fig:data_set_plot}, an average trajectory in the data set is quite short, since random actions are not suitable to balance the bicycle.

\Cref{fig:data_set_properties:omega_example} shows this in more detail, as it depicts the values of $\omega$ for a typical trajectory.
While the bicycle starts in an upright position, it quickly starts leaning heavily towards one side and, since the controller does not choose actions to stabilize the bicycle, falls over.
The sampled trajectories do not contain many state transitions where the bicycle drives straight or the actions counteract falling.

In order to reduce this bias, the data set also contains random samples from the complete state space as shown in \cref{alg:bicycle_data_set}.
While those random samples add more balanced observations of the system, they also increase the difficulty of the learning problem.
Not every combination of angles in the state space is sensible and can be reached from an upright starting state by applying actions.
The transition models therefore also have to learn irrelevant information about the dynamics.
A heuristic to reduce the amount of improbable states is to not sample the angles uniformly but rather to sample them from a broad normal distribution around zero, resampling values which fall outside of the range of allowed values.
Since terminal states are modelled separately, both the last transition of a trajectory and all samples which result in a terminal state are removed from the data set.

\subsection{Gaussian Process Models}
\label{sub:gp_models}
The Gaussian process models for the transition dynamics are trained using data sets of the form $\D = \Set{(\mat{s_i}, \mat{a_i}, \mat{s_i^\prime}) \in \Es \times \Ah \times \Es \with i \in [N]}$ of pairs of states and actions and their corresponding following state $\mat{s_i^\prime} = \DynBicycle(\mat{s_i}, \mat{a_i})$.
Since the transition dynamics of the bicycle benchmark are deterministic, these observations have no probabilistic element and they are not noisy.
The model $f$ of the transition dynamics is a compact statistical representation of this collected knowledge and is to be used to predict successive states of unobserved combinations of states and actions $(\mat{s_\ast}, \mat{a_\ast})$.

Besides predicting a concrete following state, the model should provide a measure about the uncertainty of its predictions.
Since there is no randomness in the dynamics themselves, this uncertainty comes from the imperfect information about the true system dynamics and is dependent on the location of both the training data and the required predictions.
If a query is made to the model in a part of the state space in which it has not seen many observations, the model should express its uncertainty and not assume that its best guess is close to the truth.

\begin{figure}[tb]
    \centering
    \includestandalonewithpath{figures/solution_gp_posterior}
    \caption[GP transition models]{
        Since observations are completely certain, the transition models have no uncertainty at the black observations.
        The further away from observations a test point is, the higher the uncertainty becomes.
        Since Gaussian processes are non-degenerate, uncertainties do not converge to zero away from the observed data.
        Instead, for values with absolute value larger than six, the transition model falls back to the prior.
    }
    \label{fig:gp_transition_models}
\end{figure}
The Gaussian processes presented in \cref{sec:gp_regression} represent a distribution over all plausible transition dynamics given a data set.
In \cref{fig:gp_transition_models}, the x-axis represents pairs of states and actions while the y-axis represents the successive state.
Since the observations are noise-free, the GP is completely certain about predicting them and, since it assumed a smooth RBF-prior, it is also confident about predicting states close to the observed data.
Between the data points, uncertainties are higher since there are many different models which are plausible.
Gaussian processes are called \emph{non-degenerate}, since for predictions far away from the training set, the predicted uncertainty does not converge to zero.
In contrast, for parts of the input space without any knowledge, the GP falls back to the prior assumptions about uncertainties and the mean function.
Given a large enough data set which is spread out through the complete state and action space, the model becomes more and more confident about its predictions and converges towards the true transition dynamics.

Gaussian processes as presented in this thesis can only model functions with univariate output.
Approximating successive states requires multivariate predictions however.
While there do exist extensions of Gaussian processes for multidimensional output \cite{rasmussen_gaussian_2006}, a common solution is to train $D$ separate GPs, one for every dimension in the state space $\R^D$.
While this requires more training time, it allows choosing a different set of hyperparameters for every dimension.
Since the training set does not contain transitions which result in terminal states, the transition models do not know about the terminal conditions for trajectories.
The signature of the function represented by the transition model is $f \colon \Es \times \Ah \to \Es$, where $\Es = \R^D$ and $\Ah = R^k$.
Similar to the absence of terminal states, the transition models are also not aware of the rectangular boundaries of the state space described in \cref{tab:bicycle_variables}, which means that it is possible for the transition models to predict illegal states, which also have to be handled separately.

All models are trained using the squared exponential kernel presented in \cref{def:rbf_kernel}.
The bicycle benchmark represents a physical system, which makes smoothness of the transition function a natural assumption.
The RBF kernel is the standard choice in Gaussian process regression when no special knowledge about the shape of the transition function is available\todo{maybe mention other kernels? Is there anything to say about them?}.

Opposed to learning successive states directly, the training targets for the $d$th dimension are the differences to the current state given by
\begin{align}
    \Delta s_{i,d} \coloneqq \DynBicycle(\mat{s_i}, \mat{a_i})_d - s_{i,d} = s_{i,d }^\prime - s_{i,d},
\end{align}
where $i \in [N]$ and $d \in [D]$.
This can be advantageous since differences tend to vary less than the original function.
Learning differences can also introduce independences in the data, since predicting the change in position of the bicycle only depends on the direction of movement but not on the previous position.
Having learned models for the differences, the mean and the variance of the Gaussian posterior for the partial model for the $d$-th dimension $\Prob{f_d(\mat{s_\ast}, \mat{a_\ast})}$ is given by
\begin{align}
    \Moment*{\E}{f_d(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} &= s_{\ast,d} + \Moment*{\E}{\Delta s_{\ast,d} \given \mat{s_\ast}, \mat{a_\ast}}, \\
    \Moment*{\var}{f_d(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} &= \Moment*{\var}{\Delta s_{\ast,d} \given \mat{s_\ast}, \mat{a_\ast}},
\end{align}
respectively, since the prior state is for now considered constant and non-probabilistic.
Uncertainties in the predictions only originate from the amount of confidence expressed by the models for the differences.
The values of the expected value and variances are calculated according to \cref{lem:gp_posterior}.

Since the different Gaussian processes are trained independently of each other and their training sets only contain their respective output dimension, their predictions are conditionally independent given the input.
With the predictive distribution for the single dimension being Gaussian, the joint predictive state distribution is also Gaussian with a diagonal covariance matrix and is given by
\begin{align}
    \Prob*{f(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} &=
    \Gaussian*{f(\mat{s_\ast}, \mat{a_\ast}) \given \mat{\mu_f}, \mat{\Sigma_f}}\text{, where} \\
    \mat{\mu_f} &= \begin{pmatrix}
    \Moment*{\E}{f_1(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} \\
    \vdots \\
    \Moment*{\E}{f_D(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}} \\
    \end{pmatrix} \\
    \mat{\Sigma_f} &= \diag\left(
        \Moment*{\var}{f_1(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}},
        \ldots,
        \Moment*{\var}{f_D(\mat{s_\ast}, \mat{a_\ast}) \given \mat{s_\ast}, \mat{a_\ast}}
    \right).
\end{align}
This diagonal covariance matrix illustrates the implicit independence assumption of the different output dimensions introduced by training one model per output dimension.
While this assumption is not true in most cases, it can be used as an approximation and generally yields good results.

The state of the bicycle system is given by a vector $\Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, x, y, \psi}$ composed of the internal dynamics of the bicycle and its position and orientation in euclidean space.
During simulation with the transition model, the coordinates were transformed to polar coordinates given by
\begin{align}
    \varphi(x, y) &\coloneqq \atanTwo(y, x), \\
    r(x, y) &\coloneqq \sqrt{x^2 + y^2},
\end{align}
where $\atanTwo$ is the arctangent function with two arguments.
Polar coordinates uniquely represent a two-dimensional point by its angle to the x-axis and its distance to the origin.
This representation both increases model performance and simplifies calculating the bicycle's relative position to the goal in the origin.

Additionally, representing both $\varphi$ and $\psi$ as numbers between $-\pi$ and $\pi$ leads to a loss of information.
While two angles with absolute value close to $\pi$ but opposite signs are close together on a circle, their representations have a large euclidean distance.
A Gaussian process using the RBF-Kernel cannot recognize their similarity.
In this case, it is possible to choose a specialized periodic variant of the squared exponential kernel which recognizes periodicity.
Equivalently, an angle can be represented as a complex number on the unit circle, replacing it by its sine and cosine.
Therefore, the internal representation of a bicycle state in the simulation which is given by a vector
\begin{align}
    \mat{s} &= \Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, \varphi, r, \psi} \in \R^7 \\
    \intertext{is transformed to the \emph{augmented state}}
    \mat{\hat{s}} &= \Bicycle{\theta, \dot{\theta}, \omega, \dot{\omega}, \sin\varphi, \cos\varphi, r, \sin\psi, \cos\psi} \in \R^9 \label{eq:augmented_state}
\end{align}
when presented to the GPs.
The transition model consists of seven Gaussian processes, each with nine-dimensional input.

The models are implemented in Python using \citeauthor{titsias_variational_2009}'s sparse variational GP regression implemented in GPy \cite{gpy_gpy:_2012} and trained using expectation maximization as presented in \cref{sub:gp_hyperparameters}.
The optimization of the likelihood function is calculated using scaled conjugated gradients with multiple restarts to avoid local minima.

The performance of the transition models is highly dependent on the size of the training set $N$ and the number of inducing inputs $M$.
For $N$ smaller than 35000, the performance of the transition models for long-term predictions is not good enough to allow PSO-P to succeed for any of the approaches presented below.
Conversely, for large $N$ and $M$ larger than 250, the models are good enough such that PSO-P finds perfect solutions for all approaches.
The experiments in this thesis focus on choices for $N$ and $M$ which are between these extremes and where information about the model uncertainties can be used to improve performance.
The next section presents the classic approach of long-term predictions without the use of uncertainty information, which is used as a baseline for comparison for the following techniques.

\section{Predictions without Uncertainties}
The transition model trained on a predefined data set allows the prediction of a successive state distribution $\Prob{\mat{s_1}}$ given a deterministic pair of a state and an action $(\mat{s_0}, \mat{a_0})$.
To evaluate the action value function $\AVlu$, two extensions need to be made.
Firstly, beyond specifying the goal, \cref{cha:the_bicycle_benchmark} does not define a concrete reward function.
This section introduces a variant of the reward function used by \citeauthor{randlov_learning_1998} in \cite{randlov_learning_1998}.

And secondly, for a time horizon $T$ longer than one step into the future, the predictive state distributions $\Prob{\mat{s_1}}$ up to $\Prob{\mat{s_T}}$ are required.
Since the GP dynamics model returns a Gaussian predictive distribution for all states beyond the starting state to account for the model uncertainty, all states beyond the starting state are no longer deterministic.
In order to mimic a classic non-Bayesian model without a measure of uncertainty, the approach presented in this section discards this information and considers the maximum-a-posteriori estimation to be the deterministic prediction of the transition model.
Having established the deterministic mode of evaluating the action value function, this section finally introduces the evaluation setup used in this thesis and discusses the results of applying this technique to the bicycle system.

\subsection{Bicycle Reward Function}
\label{sub:reward_function}
Solving the bicycle benchmark is a composite problem.
An agent has to both learn to balance a bicycle and drive to the goal.
Instead of having to solve the two tasks one after the other, they both have to be solved simultaneously, constantly switching between them.
While an agent is in control of the bicycle's balance, it can try to drive towards the goal.
If any action applied to the system leads to the danger of falling over however, the agent has to quickly change its focus towards preventing this.

Without expert knowledge available, the controller must learn this distinction autonomously, given the reward function.
The most basic and uninformed reward function possible assigns positive reward for reaching the goal, a punishment (in the form of negative reward) for falling over and weighs all other states equally somewhere between the two extremes.
While this can be enough to teach the short-term task of avoiding to fall down, the agent has no initiative of driving towards the goal besides actually hitting it.
For most situations, the goal cannot be reached within the time-horizon of one PSO-P instance.
In this setting, PSO-P would optimize towards a trajectory for which the chance of falling down is minimal.
This trajectory is a circle with large radius~\cite{randlov_learning_1998}.

To give the controller a motivation of reaching the goal, it has to receive some hint about the correct direction to drive.
Encoding this information in the reward function goes against the assumption of the complete absence of expert knowledge.
If it is too detailed, it introduces the risk of significantly simplifying the learning problem or pushing the agent towards a policy which is only locally optimal.
This reduction of the hard problem of finding the goal to a series of easier problems of driving in the correct direction and then going straight is called \emph{shaping} \cite{sutton_reinforcement_1998,randlov_learning_1998}.

The hint towards the goal encoded in the reward function should be a term which represents information which is local in the sense that its value can change considerably within the time horizon.
The most simple term to consider is a punishment based on the current distance to the goal.
This formulation is problematic however, since the agent should not care about the actual distance rather than the change of distance with respect to the previous state, which cannot be expressed in the reward.
Using the distance itself, an increase in reward would express movement in the correct direction.
However, for any non-linear punishment, the amount of increase is dependent on the current position in the input space and can lead to numerical problems if it gets to small.
If it were linear, the punishment might at some point be larger than the punishment for falling.
At this point, the agent's correct choice would be to fall down as quickly as possible.

In order to avoid these problems, the reward function used in this thesis is based on the current angle between the frame of the bicycle and the direction towards the goal.
Since the goal's position is at the origin of the coordinate system, this angle can be calculated as the difference of the current rotation of the bicycle $\psi$ and the angular component of its polar coordinates in space $\varphi$.
It is in the agent's interest to minimize this angle, since if it zero, the agent is heading towards the goal on the shortest route possible.
As the bicycle moves at a constant speed, a successful trajectory which locally minimizes this angle is also a globally optimal trajectory, since it results in the shortest path possible.

\begin{figure}[t]
    \centering
    \includestandalone{figures/solution_saturating_reward}
    \caption[Comparison of linear and saturating reward functions]{
        The blue quadratic saturating reward function is a Gaussian densityprobability density which has been renormalized to a maximum value of one.
        When compared to the dashed piecewise linear reward function, the saturating reward is more forgiving for small errors in the angle difference $\Delta^\Psi$, but punishes large deviations more.
    }
    \label{fig:saturating_reward}
\end{figure}
Instead of a linear reward based on the difference, this thesis chooses a quadratic saturating reward function as proposed by \citeauthor*{deisenroth_gaussian_2015} in \cite{deisenroth_gaussian_2015}.
The quadratic saturating reward function is a Gaussian density function normalized to a value of one at its maximum.
It behaves locally quadratic around this maximum and shows exponential drop off towards zero for large deviations from the mean.
\Cref{fig:saturating_reward} compares the saturating reward function to a linear one.
The Gaussian reward is more forgiving for small errors around zero but punishes large deviations more, which is a good behaviour for the bicycle benchmark since it is often good enough for the bicycle to only approximately head towards the goal.
If the bicycle has reached the goal, the agent receives a constant award of two.
This is double the reward which can be obtained for any state which is not in the goal.
Similarly, the reward for any state where the bicycle has fallen over is zero, which is less then for any non-terminal state in the benchmark.

\begin{definition}[Bicycle Reward Function]
    Given the set $\Es^+$ of possible states of the bicycle benchmark and the sets $\Goal$ and $\Fallen$ of all terminal states where the bicycle has reached the goal or fallen respectively, the \emph{bicycle reward function} is defined as
    \begin{align}
        \RwdBicycle \colon \left\{
            \begin{aligned}
                \Es^+ &\to \R \\
                \mat{s} &\mapsto \begin{cases}
                    2 & \text{if $\mat{s} \in \Goal$} \\
                    0 & \text{if $\mat{s} \in \Fallen$} \\
                \sqrt{2\pi\sigma_{\text{angle}}^2}\Gaussian{\Delta^\psi_{\mat{s}} \given 0, \sigma_{\text{angle}}^2} & \text{otherwise}
                \end{cases}
            \end{aligned}
        \right.
    \end{align}
    where $\Delta^\psi_{\mat{s}} = \abs{\psi_{\mat{s}} - \varphi_{\mat{s}}} \in (-\pi, \pi]$ denotes the difference between the bicycle's heading and the goal given a state $\mat{s}$ and $\sigma_{\text{angle}}^2$ is a positive real constant.
    The Gaussian density function $\Norm$ is normalized to a maximum value of 1.
\end{definition}

The agent does not receive negative reward for falling down.
Instead, the reward is zero.
The agent is still punished when falling down though, since the episode has to end and the agent is not able to collect additional reward by driving towards the goal at later time steps.

\subsection{Long-Term predictions}
\label{sub:map_predictions}
The models for the bicycle transition dynamics are Gaussian processes with the assumption that their underlying space is $\R^7$, a complete vector space.
The set $\Es$ of valid non-terminal states in the bicycle benchmark is an axis-parallel cuboid however, whose axis-wise boundaries are defined by the vectors $\mat{s}^{\text{min}}$ and $\mat{s}^{\text{max}}$.
The minimal and maximal values for every variable can be found in \cref{tab:bicycle_variables}.
The transition models transform the euclidean coordinates $x$ and $y$ to the polar coordinates $\varphi$ and $r$.
Their intervals of valid values are given by the intervals ($-\pi$, $\pi$] and [0, 100] respectively.

A state of the bicycle benchmark is terminal if the bicycle has either reached the goal or if it has fallen over.
The goal is defined as a circle around the origin with radius five.
A bicycle has fallen if the absolute value of the angle $\omega$ is larger than $\sfrac{\pi}{15}$.
If the bicycle is inside the goal area but has also fallen down, the state is defined to be successful.
Using these constraints, the set of terminal states $\Tee$ and the set of non-terminal states $\Es$ are defined as
\begin{align}
    \Goal &\coloneqq \Set*{\mat{s} \in \R^7 \with r_{\mat{s}} \leq 5 \vphantom{\frac{\pi}{15}}}, \\
    \Fallen &\coloneqq \Set*{\mat{s} \in \R^7 \with \abs{\omega_{\mat{s}}} > \frac{\pi}{15}} \setminus \Goal, \\
    \Es &\coloneqq \Set*{\mat{s} \in \R^7 \with \forall i \colon \mat{s}_i \in \left[ \mat{s}_i^{\text{min}}, \mat{s}_i^{\text{max}} \right]} \setminus \Goal,
\end{align}
respectively, where $\Tee \coloneqq \Goal \cup \Fallen$.
The set of all possible states in the bicycle benchmark is given by $\Es^+ \coloneqq \Es \cup \Tee$.
Note that $\Es^+$ is not equal to $\R^7$.
A state where $\psi$ is greater than $\pi$ or where $\theta$ is greater than $\sfrac{\pi}{2}$ is considered physically impossible.
This is unknown to the transition models and it is possible for them to predict impossible states.
Since these extreme states are rare, this thesis considers all physically impossible predictions to be predictions of the closest possible value.
A prediction of $\sfrac{\pi}{6} + \epsilon$ for $\theta$ is interpreted as a prediction of $\sfrac{\pi}{6}$.
Notationally, this thesis uses a projection function $\squeeze \colon \R^7 \to \Es^+$ to indicate the correction of impossible states.

Gaussian processes are Bayesian models which predict values by averaging over all plausible latent functions, resulting in a Gaussian belief about the prediction.
In order to simulate a deterministic transition model, this first approach for long-term predictions discards this belief and only predicts a single successive state.
The most plausible choice for this single successive state is the maximum-a-posteriori estimation given by the maximum of the posterior density function.
With a deterministic state $\mat{s_t}$ and an action $\mat{a_t}$, this MAP estimate of the transition model $f$ is given by
\begin{align}
    \argmax_{\mat{s_{t+1}} \in \R^7} \Prob{\mat{s_{t+1}} \given \mat{s_t}, \mat{a_t}} &= \argmax_{\mat{s_{t+1}} \in \R^7} f\Cond{\mat{s_{t+1}} \given \mat{s_t}, \mat{a_t}} \\
    &= \argmax_{\mat{s_{t+1}} \in \R^7} \Gaussian{\mat{s_{t+1}} \given \mat{\mu_{t+1}}, \mat{\Sigma_{t+1}}} \\
    &= \Set{\mat{\mu_{t+1}}}.
\end{align}
A Gaussian distribution's density function has a unique maximum at the mean.

\begin{figure}[t]
    \centering
    \includestandalonewithpath{figures/solution_map_predictions}
    \caption[MAP long-term predictions]{
        Colored long-term predictions for one time horizon using MAP estimates and starting from a deterministic state compared to the dashed simulation.
        After about half the time horizon, iterating MAP-prediction produces considerable errors.
    }
    \label{fig:map_predictions}
\end{figure}
The transition model is trained using observations in $\Es$ and is unaware of terminal states.
Since states at all time steps are deterministic, terminal states can easily be modelled as fixed points in the function defined as
\begin{align}
    \map{\mat{s_{t+1}}} \colon \left\{
        \begin{aligned}
            \Es^+ \times \Ah &\to \Es^+ \\
            (\mat{s_t}, \mat{a_t}) &\mapsto \begin{cases}
            \mat{s_t} & \text{if $\mat{s_t} \in \Tee$} \\
            \squeeze(\Moment{\E}{f(\mat{s_t}, \mat{a_t})}) & \text{otherwise,}
        \end{cases}
    \end{aligned}
    \right.
\end{align}
where the expected value of $f(\mat{s_t}, \mat{a_t})$ can directly be computed using \cref{lem:gp_posterior}.
Given a time horizon $T$, a starting state $\mat{s_0}$ and a vector of actions $(\mat{a_0}, \dots, \mat{a_{T-1}})$, this function can be used to calculate the intermediate states $\map{\mat{s_1}}$ to $\map{\mat{s_T}}$ using dynamic programming.
\Cref{fig:map_predictions} \todo{mention that we only show internal variables} compares these iterated MAP-predictions with the correct evolution of the bicycle system in an example.

With these intermediate states and the reward function $\RwdBicycle$ from \cref{sub:reward_function}, it is possible to evaluate the action value function $\AVlu$ defined in \cref{eq:action_value_function} and therefore apply PSO-P to the bicycle system.
Using the MAP-estimates for intermediate states, the action value function can be calculated as
\begin{align}
    \label{eq:map_action_value_function}
    \begin{split}
        \map{\AVlu}_{\mat{s_0}}(\mat{a_0}, \dots, \mat{a_{T-1}}) &= \Moment*{\E}{\sum_{t=0}^T \gamma^t \RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
        &= \sum_{t=0}^T \gamma^t \Moment*{\E}{\RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
        &= \sum_{t=0}^T \gamma^t \RwdBicycle(\map{\mat{s_t}}),
    \end{split}
\end{align}
where $\map{\mat{s_0}} \coloneqq \mat{s_0}$.
Since all states are deterministic, all expected values collapse to applications of the reward function to the single deterministic state.

\subsection{Evaluation Setup}
\begin{table}[t]
    \centering
    \caption{The parameters used for evaluation in this thesis.}
    \label{tab:evaluation_parameters}
    \begin{tabularx}{\tablewidth}{ccXc}
        \toprule
        Part & Parameter & Description & Values \\
        \midrule
        Simulation & $\tau$ & Time Discretization & \SI[mode=text]{0.01}{\second} \\
        & & Successive Steps & 10 \\
        \addlinespace
        Evaluation & $I$ & Number of data sets & 46 \\
        & $S$ & Number of trajectories & 15 \\
        & & Maximum trajectory length & 120 \\
        \addlinespace
        PSO-P & $T$ & Time horizon & 15 \\
        & $\sigma_{\text{angle}}^2$ & Reward width & $(\sfrac{\pi}{2})^2$ \\
        \addlinespace
        GP models & $\mat{N}$ & Number of training points & \{60000, 70000\} \\
        & $\mat{M}$ & Number of pseudo inputs & \{20, 30, 40, 50, 75, 100, 150\} \\
        & $\K$ & Kernel & RBF \\
        \bottomrule
    \end{tabularx}
\end{table}
\begin{algorithm}[tp]
    \caption{Bicycle evaluation setup}
    \label{alg:evaluation_setup}
    Let $\AVlu$ denote an implementation of the action value function to be evaluated.
    All other constants are described in \cref{tab:evaluation_parameters} and \cref{alg:bicycle_data_set}.
    \begin{algorithmic}[1]
        \For{$i \gets 1, I$}
            \State $\D_i \gets \Call{SampleMixedBicycleDataSet}{\max \mat{N}}$
            \ForAll{$(N, M) \in \mat{N} \times \mat{M}$}
                \State Train transition model $f^{(N, M)}$ on $\D_i$ with kernel $\K$
                \For{$s \gets 1, S$}
                \State $\mat{s^{(s)}_0} \gets \Call{SampleBicycleStartState}{\null} + \Gaussian{\mat{0}, \diag(10^{-4} \cdot \mat{s}^{\text{max}})}$
                    \State Observe trajectory $\Trajectory_{i, s}$ from $\mat{s^{(s)}_0}$ using PSO-P on $f^{(N, M)}$ and $\AVlu$
                \EndFor
            \EndFor
        \EndFor
        \State Evaluate $\AVlu$ using the trajectories $\Set{\Trajectory_{i, s} \with i \in [I], s \in [S]}$
    \end{algorithmic}
\end{algorithm}
\Cref{alg:evaluation_setup} describes the setup used to evaluate and compare the different implementations of the action value function used in this thesis.
PSO-P directly optimizes the action value function in order choose actions to be applied to the bicycle system.
Since the action value function is based on the dynamics models, this policy highly depends on the performance of the transition models.
In order to reduce this bias in the evaluations, the performance is measured based on multiple different transition models.

The behaviour of the transition models depends on the quality of the underlying data set, the number of input points used for training and the prior choices of the hyperparameters.
The evaluation samples multiple data sets and then considers a number of combinations of training points $N$ and pseudo inputs $M$ as shown in \cref{tab:evaluation_parameters}.
The number of pseudo inputs determines the expressiveness of the model.
Many pseudo inputs allow the transition models to describe the data with more detail while few pseudo inputs force the model to generalize.

Standard choices for the number of pseudo inputs is in the range from a few ten to several hundred inputs \cite{snelson_flexible_2007}, but the correct choice depends on the function to be learned and cannot easily be determined.
The models in this thesis range from very simple models with 20 pseudo inputs to more powerful models with 150 pseudo inputs.
The boundaries were chosen based on empirical studies.
At some point below 20 pseudo inputs, the models collapse since they loose their expressive power.
For a very high number of pseudo inputs, all techniques discussed in this thesis behave very similarly, since the predictions of the models become almost perfect.
Similarly, the number of training points was chosen such that there is enough data to catch all relevant parts of the dynamics but not enough data for the models to be perfect.

These different choices of hyperparameters for the transition models are applied to a number of different data sets sampled according to \cref{alg:bicycle_data_set}.
For every such data set, transition models are trained using the different combinations of the numbers of training points and pseudo inputs.
Based on every such model, multiple trajectories are generated using PSO-P and the action value function $\AVlu$ to be evaluated.
A trajectory is created by sampling a starting state $\mat{s_0}$ and using PSO-P to choose an action $\mat{a_0}$ to be applied.
Using this pair, the successive state $\mat{s_1} = \DynBicycle(\mat{s_0}, \mat{a_0})$ can be calculated using the simulation.
This process is iterated until the bicycle reaches the goal, falls over or the trajectory becomes longer than the maximum trajectory length.
The time horizon $T$ used in PSO-P is chosen to be long enough to contain about a third of a rotation of the bicycle on a tight curve.
This proofed to be a long enough horizon to solve the benchmark while still remaining computationally feasible.

The application of \cref{alg:evaluation_setup} leads to a large set of trajectories based on different data and different models.
In order to compare different implementations of the action value function $\AVlu$, PSO-P is applied to the same starting states using the same models using the respective functions.

\subsection{Results}
\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/solution_map_goal_percentage}
    \caption[Results using MAP-predictions]{
        The success rate for different numbers of pseudo inputs with the standard error of the mean.
        The performance of MAP-predictions increases slightly with the number of pseudo inputs but always remains below 60 percent.
    }
    \label{fig:map_results}
\end{figure}
\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/solution_map_trajectory}
    \caption[Successful MAP trajectory]{
        A single successful MAP trajectory.
        The bicycle starts at about 18 meters distance from the goal with an angle of about 90 degrees.
        PSO-P starts by increasing both the distance and angle towards the goal in the first five time steps.
        This allows the agent to make a more aggressive turn until time step 30, after which the bicycle is heading towards the goal in a mostly straight line.
    }
    \label{fig:map_successful_trajectory}
\end{figure}
\begin{figure}[p]
    \centering
    \includestandalonewithpath{figures/solution_map_xy_trajectories}
    \caption[Episodes using MAP-predictions]{
        The $x$ and $y$ coordinates of the front tyre in representative episodes when using MAP-predictions with marks at the starting states.
        Successful episodes are colored in green and failed episodes are colored in red.
        PSO-P tends to be successful for starting states which do not require a curve to reach the goal.
        For other episodes, the policy usually fails during curved driving.
        If successful, trajectories usually consist of one curve to orient the bicycle followed by a straight line towards the goal.
    }
    \label{fig:map_trajectories}
\end{figure}
\Cref{fig:map_successful_trajectory} shows an example of a successful trajectory generated using PSO-P on the maximum-a-posteriori action value function $\map{\AVlu}$ derived in \cref{eq:map_action_value_function}.
Since the PSO policy does not have to be trained, the first interaction with the system can be successful, based on the quality of the transition models.
PSO-P tends to aggressively exploit model bias to choose good actions in the time horizon.
Since it does not have any kind of memory, it does not follow a long-term strategy.

The bicycle reward function hints towards the goal by increasing the reward if the bicycle's frame points towards the goal.
A typical trajectory created with the PSO policy therefore first describes a curve with a radius as small as possible in order to quickly point in the correct direction.
If the bicycle points into the correct direction, PSO-P drives straight towards the goal until it is reached.
The policy avoids falling over on the model since falling over reduces the achieved reward.
It will however choose actions which are as close to falling over as possible, since the minimum radius of a curve is constrained by the maximum leaning angle $\omega$.

\Cref{fig:map_trajectories} shows that this strategy often is not successful.
Minimal errors in the predictions of the models when the cyclist is leaning close to the maximum amount lead to the cyclist falling over.
Therefore, most trajectories fail while driving a curve.
If a trajectory starts at a state which does not require a tight curve, PSO-P can often reach the goal, since it can very effectively solve the balancing problem in non-extreme states and drive straight.

While \cref{fig:map_successful_trajectory,fig:map_trajectories} are based on a single transition model and give an idea about the performance of PSO-P using MAP-predictions, statistically significant results can be obtained using the multiple iterations of the evaluation algorithm with different data sets and transition models.
\Cref{fig:map_results} shows the percentage of trajectories which reach the goal plotted against the number of pseudo inputs.
The more expressive the model, the more successful PSO-P with MAP-predictions becomes, but overall, about half of the trajectories trajectories end in failure.

In order to reduce the aggressiveness with which PSO-P exploits the model, the uncertainty measure provided by the Gaussian processes are going to be integrated into the action value function.
If the model predicts that with a large probability, the true system dynamics lead to failure, even if the MAP-prediction might still be a valid state, PSO-P should choose a less extreme action.
The following section introduces how the reward function can be extended to beliefs about states and uses the uncertainty predicted by the transition model during planning without propagating it through multiple time steps.

\section{Predictions with One-Step Uncertainties}
PSO-P on the basis of MAP-predictions discards all information about model uncertainties by assuming that the mean of the predictive distribution produced by the transition model is the correct successive state.
Long-term predictions for states multiple time steps into the future can be obtained by iterative application of the transition model since all intermediate states are deterministic.
In the same way, the expected reward for every time step can easily be calculated for arbitrary reward functions since no integration using the state's density function is needed.

The first approach to incorporating uncertainties into planning with PSO-P presented in this thesis is a direct extension of the MAP approach.
Instead of discarding them at every time step, the uncertainties returned by the transition model are used to evaluate the expected reward.
These uncertainties are still not propagated however, as the algorithm for deriving intermediate states in long-term predictions essentially remains the same compared to the previous technique.

If states are not considered to be deterministic, it is no longer clear when an episode should be considered to be over.
For every time step, the predictive state distributions $\Prob{\mat{s_1}}$ to $\Prob{\mat{s_T}}$ can have their probability mass spread over both terminal and non-terminal states.
After defining how to calculate the intermediate state distributions using one-step uncertainties, this section considers how the probability that the predictive trajectory has already ended can be calculated with respect to the whole trajectory.

\subsection{Long-Term predictions}
\label{sub:os_predictions}
Similar to the MAP-predictions defined in \cref{sub:map_predictions}, long-term predictions with one-step uncertainties are based on iterating the transition model using the means of the predicted Gaussian distributions.
The intermediate states are no longer assumed to be deterministic however.
They are random variables with the distribution returned by the transition model.
The function mapping a pair of a state distribution $\mat{s_t}$ and an action $\mat{a_t}$ to the successive state distribution $\mat{s_{t+1}}$ is given by
\begin{align}
    \label{eq:os_dynamic_programming}
    \os{\mat{s_{t+1}}} \colon \left\{
        \begin{aligned}
            \Dists(\Es^+) \times \Ah &\to \Dists(\Es^+) \\
            (\rv{s_t}, \mat{a_t}) &\mapsto \squeeze(f(\Moment{\E}{\rv{s_t}}, \mat{a_t})),
    \end{aligned}
    \right.
\end{align}
where $\squeeze(f(\mat{\mu_t}, \mat{a_t})) = \squeeze(\Gaussian{\mat{\mu_{t+1}}, \mat{\Sigma_{t+1}}}) = \Gaussian{\squeeze(\mat{\mu_{t+1}}), \mat{\Sigma_{t+1}}}$ denotes the elimination of physically impossible states by correcting the posterior mean.
Note that there can still be probability mass assigned to impossible states since the Gaussian is symmetric around the mean.
Since only the mean is used for predictions, this does not cause a problem in this setting however.

The intermediate states $\os{\mat{s_1}}$ to $\os{\mat{s_T}}$ can again be calculated using dynamic programming.
Terminal states are no longer modelled using fixed points in the transition function.
Since the intermediate states are probabilistic, it is no longer clear when the predicted trajectory has ended.
Instead, for every state, the probability of having reached the goal can be calculated as
\begin{align}
    \label{eq:terminal_probabilies}
    \begin{split}
        \Prob*{\os{\mat{s_t}} \in \Goal} &= \Prob*{r_{\os{\mat{s_t}}} \leq 5}, \\
        \Prob*{\os{\mat{s_t}} \in \Fallen} &= \Prob*{\abs{\omega_{\os{\mat{s_t}}}} > \frac{\pi}{15}}
    \end{split}
\end{align}
respectively.
Since the intermediate state distributions are Gaussian, the marginal distributions for every variable in the state are (not necessarily independent) univariate Gaussian distributions.
Given a state distribution $\rv{s} \sim \Gaussian{\mat{\mu_s}, \mat{\Sigma_s}}$, the distribution of the $i$-th state variable is given by
\begin{align}
    \rv{s_i} \sim \Gaussian*{\mu_{s}^{(i)}, \Sigma_{s}^{(i,i)}}.
\end{align}
Probabilities of the form $\Prob{\rv{s_i} \leq k}$ are then given by
\begin{align}
    \Prob*{\rv{s_i} \leq k} &= \Phi\!\left( \frac{k - \mu_s^{(i)}}{\sqrt{\Sigma_s^{(i,i)}}} \right),
\end{align}
where $\Phi$ denotes the cumulative distribution function of the standard normal distribution.

The probability $\Prob{\os{\mat{s_t}} \in \Fallen}$ denotes the transition model's belief that given that the bicycle has not fallen down in the past, it will fall down exactly at time step $t$.
Now consider the state $\os{\mat{s_{t+1}}}$ one time step later.
Again, the probability $\Prob{\os{\mat{s_{t+1}}} \in \Fallen}$ denotes the belief that the bicycle will fall down at exactly time step $t+1$.
The state for time step $t+1$ was calculated under the assumption that the trajectory has not already ended, since the transition model does not know about terminal states.
However, the correct trajectory could have also ended at time step $t$.
In this case, the trajectory should be considered to have been stuck at that terminal state.
The probability $\Prob{\os{\mat{s_{t+1}}} \in \Fallen}$ then looses its significance in this case, since it has been calculated under wrong assumptions.

In order to correctly calculate the probability that the trajectory has already ended before or at time step $t+1$, the probabilities of falling down or reaching the goal at any of the previous time steps have to be considered.
Let
\begin{align}
    \Prob*{\PGoal_t} &\coloneqq \Prob*{\exists i \leq t \colon \mat{s_i} \in \Goal}, \\
    \Prob*{\PFallen_t} &\coloneqq \Prob*{\exists i \leq t \colon \mat{s_i} \in \Fallen}
\end{align}
denote the probabilities that at any point before or at $t$, the trajectory has already ended because of reaching the goal or falling down respectively.
In other words, they denote the accumulated belief of the transition model that the trajectory has at some point hit a terminal state.
These probabilities can be calculated using dynamic programming.
They are given by
\begin{align}
    \Prob*{\PGoal_{t+1}} &= \Prob*{\PGoal_{t}} + \left( 1 - \Prob*{\PGoal_{t}}  \right) \cdot \Prob*{\mat{s_{t+1}} \in \Goal}, \\
    \Prob*{\PFallen_{t+1}} &= \Prob*{\PFallen_{t}} + \left( 1 - \Prob*{\PFallen_{t}}  \right) \cdot \Prob*{\mat{s_{t+1}} \in \Fallen},
\end{align}
respectively, where the probabilities $\Prob*{\PGoal_{0}}$ and $\Prob*{\PFallen_{0}}$ can be calculated directly.

With these probabilities describing the overall belief that the predictive trajectory has not already ended, the expected reward for every state in the trajectory can be calculated.
Consider the state $\mat{s_t}$ with the Gaussian distribution $\Gaussian{\mat{\mu_t}, \mat{\Sigma_t}}$.
Its expected reward is given by
\begin{align}
    \Moment*{\E}{\RwdBicycle(\rv{s_t})} &= \int \RwdBicycle(\rv{s_t}) \Prob{\rv{s_t}} \diff \rv{s_t} \\
    &= \int \RwdBicycle(\rv{s_t}) \Gaussian{\rv{s_t} \given \mat{\mu_t}, \mat{\Sigma_t}} \diff \rv{s_t}.
\end{align}
In \cref{sub:reward_function}, $\RwdBicycle$ was specified using a definition by cases, separating the terminal states.
To simplify the integration, it can be rewritten as a single sum using the indicator function $\Ind$ in
\begin{align}
    \RwdBicycle(\mat{s}) = \left\{ \begin{alignedat}{3}
            &&& 2 &&\cdot \Indicator{\rv{s} \in \Goal} \\
            &&{}+{}& 0 &&\cdot \Indicator{\rv{s} \not\in\Goal, \rv{s} \in \Fallen} \\
            &&{}+{}& \sqrt{2\pi\sigma_{\text{angle}}^2}\Gaussian{\Delta_\psi \given 0, \sigma_{\text{angle}}^2} &&\cdot \Indicator{\rv{s} \not\in\Goal, \rv{s} \not\in\Fallen}.
        \end{alignedat}\right.
\end{align}

In the integration required for the expected reward, the indicator functions are replaced by their corresponding probabilities according to the law of total probability.
The integral is then given by
\begin{align}
    \int \RwdBicycle(\rv{s_t}) \Prob{\rv{s_t}} \diff \rv{s_t} &= 2 \cdot \Prob{\PGoal_t} + 0 \cdot (1 - \Prob{\PGoal_t}) \Prob{\PFallen_t} \\
    &\qquad + (1 - \Prob{\PGoal_t})(1 - \Prob{\PFallen_t}) \int \RwdBicycleHat(\rv{s_t}) \Prob{\rv{s_t}} \diff \rv{s_t} \\
    &= 2 \cdot \Prob{\PGoal_t} + (1 - \Prob{\PGoal_t})(1 - \Prob{\PFallen_t}) \int \RwdBicycleHat(\rv{s_t}) \Prob{\rv{s_t}} \diff \rv{s_t} \\
    \intertext{where $\RwdBicycleHat$ denotes the reward term for states which are not terminal and is defined as}
    \RwdBicycleHat(\rv{s}) &\coloneqq \sqrt{2\pi\sigma_{\text{angle}}^2} \Gaussian{\Delta_\psi(\rv{s}) \given 0, \sigma_{\text{angle}}^2}.
\end{align}
The angle between the bicycle's heading and the goal $\Delta_\psi$ can be calculated as the difference of two entries of the state vector.
Equivalently, it is the result of applying a linear transformation to the Gaussian state distribution.
Since $\Delta_\psi = \psi - \varphi$ and the variables in the state vector are organized as presented in \cref{sub:gp_models}, this linear transformation has the form
\begin{align}
    \pi &= \left( 0, 0, 0, 0, -1, 0, 1 \right)\tran, \\
    \rv{\Delta_\psi} &= \pi\tran \rv{s_t}.
\end{align}
Linear transformations of Gaussians are also Gaussian \cite{petersen_matrix_2008}.
The distribution of $\rv{\Delta_\psi}$ is given by
\begin{align}
    \rv{\Delta_\psi} \sim \Gaussian{\pi\tran\mat{\mu_t}, \pi\tran \mat{\Sigma_t} \pi}.
\end{align}

The expected reward for non-terminal states can then be rewritten as the integral of a product of univariate Gaussian distributions.
With $c \coloneqq \sqrt{2\pi\sigma_{\text{angle}}^2}$ it is given by
\begin{align}
    \int \RwdBicycleHat(\rv{s_t}) \Prob{\rv{s_t}} \diff \rv{s_t} &=  \int c \Gaussian{\Delta_\psi(\rv{s_t}) \given 0, \sigma_{\text{angle}}^2} \Prob{\rv{s_t}} \diff \rv{s_t} \\
    &= c \int \Gaussian{\Delta_\psi(\rv{s_t}) \given 0, \sigma_{\text{angle}}^2} \Prob{\rv{s_t}} \diff \rv{s_t} \\
    &= c \int \Gaussian{\rv{\Delta_\psi} \given 0, \sigma_{\text{angle}}^2} \Prob{\rv{\Delta_\psi}} \diff \rv{\Delta_\psi} \\
    &= c \int \Gaussian{\rv{\Delta_\psi} \given 0, \sigma_{\text{angle}}^2} \Gaussian{\rv{\Delta_\psi} \given \pi\tran \mat{\mu_t}, \pi\tran \mat{\Sigma_t} \pi} \diff \rv{\Delta_\psi} \\
    &= c \Gaussian{\pi\tran \mat{\mu_t} \given 0, \sigma_{\text{angle}}^2 + \pi\tran \mat{\Sigma_t} \pi}.
\end{align}
The product of two Gaussian density functions is another unnormalized Gaussian density function.
The normalization constant can in turn be written in terms of Gaussian densities and is the result of the integration \cite{petersen_matrix_2008}.

Together these results give a closed form solution for the expected reward for the state $\rv{s_t}$ which is given by
\begin{align}
    \Moment*{\E}{\RwdBicycle(\rv{s_t})} &= 2 \cdot \Prob{\PGoal_t} + (1 - \Prob{\PGoal_t})(1 - \Prob{\PFallen_t}) \int \RwdBicycleHat(\rv{s_t}) \Prob{\rv{s_t}} \diff \rv{s_t} \\
    &= 2 \cdot \Prob{\PGoal_t} + (1 - \Prob{\PGoal_t})(1 - \Prob{\PFallen_t}) \\
    &\qquad\qquad\qquad\qquad\cdot\sqrt{2\pi\sigma_{\text{angle}}^2} \Gaussian{\pi\tran \mat{\mu_t} \given 0, \sigma_{\text{angle}}^2 + \pi\tran \mat{\Sigma_t}\pi}.
\end{align}
Note that for states with very low uncertainty, that is states for which $\pi\tran \mat{\Sigma_t} \pi$ goes to zero, the expected value converges towards the original reward function.
Similarly, both $\Prob{\PGoal_t}$ and $\Prob{\PFallen_t}$ are close zero or one for all time steps, converging to the binary behaviour of deterministic states.
Using the one-step uncertainties, the action value function can therefore be calculated as
\begin{align}
    \label{eq:os_action_value_function}
    \begin{split}
        \os{\AVlu}_{\mat{s_0}}(\mat{a_0}, \dots, \mat{a_{T-1}}) &= \Moment*{\E}{\sum_{t=0}^T \gamma^t \RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
        &= \sum_{t=0}^T \gamma^t \Moment*{\E}{\RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
        &= \sum_{t=0}^T \gamma^t \bigg( 2 \cdot \Prob{\os{\PGoal}_t} + (1 - \Prob{\os{\PGoal}_t})(1 - \Prob{\os{\PFallen}_t}) \\
        &\qquad \sqrt{2\pi\sigma_{\text{angle}}^2} \Gaussian{\pi\tran \os{\mat{\mu_t}} \given 0, \sigma_{\text{angle}}^2 + \pi\tran \os{\mat{\Sigma_t}}\pi} \bigg).
    \end{split}
\end{align}

\subsection{Results}
\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/solution_os_goal_percentage}
    \caption[Results using one step uncertainties]{
        The success rate for different numbers of pseudo inputs with the standard error of the mean.
        Considering one step uncertainties increases the success rate by about 10 percent for all numbers of pseudo inputs.
    }
    \label{fig:os_results}
\end{figure}
\begin{figure}[tp]
    \centering
    \missingfigure[figheight=.35\textheight]{OS uncertainties}
    \caption{MISSING: Some visualization of the OS uncertainties}
    \label{fig:os_uncertainties}
\end{figure}
\begin{figure}[p]
    \centering
    \includestandalonewithpath{figures/solution_os_xy_trajectories}
    \caption[Episodes using one step uncertainties]{
        The $x$ and $y$ coordinates of the front tyre in representative episodes when using one step uncertainties with marks at the starting states.
        Successful episodes are colored in green and failed episodes are colored in red.
        When compared to MAP trajectories in \cref{fig:map_trajectories}, PSO-P generates very similar results.
        However, considering one step uncertainties has a higher success rate when curved driving is needed.
    }
    \label{fig:os_trajectories}
\end{figure}
\Cref{fig:os_trajectories} shows a number of trajectories generated with PSO-P applied to the one-step action value function $\os{\AVlu}$.
The trajectories generated look very similar to the MAP-trajectories shown in \cref{fig:map_trajectories}.
Both figures were created using the same transition model and starting states.
Comparing the two figures, more trajectories were successful using the approach with one-step uncertainties.
\Cref{fig:os_results} shows that this is a statistically significant result.

For all numbers of pseudo inputs, and therefore for all different levels of model expressiveness, considering one-step uncertainties in planning is beneficial.
PSO-P aggressively exploits model bias and creates trajectories where the bicycle is nearly falling over.
Since the predictive distribution is Gaussian and therefore symmetric, half of the plausible successive states are more extreme than the predictive mean.
The expected reward reflects that these more extreme realizations might cause the bicycle to fall down and therefore the trajectory to end.
This incentivizes PSO-P to choose actions where most possible posterior states for every time step are not failure states.

The newly introduced uncertainties do not prompt PSO-P to establish a noticeable safety margin towards the maximum value of $\omega$ however.
\Cref{fig:os_uncertainties} shows why this is the case:
Since the transition model is very confident about predicting one single step into the future, the predictive uncertainties (and therefore the safety margin) are very small.
This is to be expected however, since a model which is supposed to be able to predict multiple steps into the future must show very good performance for a single step in order to not quickly deviate from the correct trajectory.

Small errors in the single predictions accumulate and after some time steps close to the correct trajectory, predicted trajectories tend to diverge from the truth rapidly.
While for every time step the transition model only makes small mistakes compared to the correct transition function, iterating it means that later invocations create predictions based on wrong assumptions about the prior state.
The intermediate prior states have up to now been assumed to be deterministic.
The next section introduces how this accumulation of errors can be modelled by propagating the uncertainty about the current state through the Gaussian process model to produce a more correct posterior distribution.

\section{Predictions with Multi-Step Uncertainties}
\label{sub:ms_predictions}
Considering the one-step uncertainties produced by the transition model for planning using PSO-P shows improved results when compared to only using MAP-predictions.
\Cref{fig:os_uncertainties} however shows that these uncertainties are very small since the transition models used for the bicycle benchmark are very confident about their predictions.
Over the course of multiple time steps, small errors nevertheless accumulate and cause large errors in the predicted trajectories which are not reflected well in the one-step uncertainties.

In order to arrive at more reliable estimates of the true uncertainty about predictions several steps into the future, the uncertainty about the state $\rv{s_t}$ has to be taken into account when predicting the distribution of the state $\rv{s_{t+1}}$.
Since the transition dynamics $\DynBicycle$ are nonlinear, calculating the propagation of the prior uncertainty through the Gaussian process directly is not analytically tractable and the resulting distribution usually is not a Gaussian.
This section introduces an approximation of this propagation via linearization of the transition model around the predictive mean as presented in \cite{ko_gp-bayesfilters:_2009} and \cite{ deisenroth_gaussian_2015}.
This approximation can be used to create long-term predictions with accumulated uncertainties.

\subsection{Propagation of Uncertainties using Linearization}
\label{sub:linearization}
A transition model $f$ described in \cref{sub:gp_models} consists of a group of Gaussian processes $f_d$, one for every dimension $d$ in a bicycle state.
Instead of predicting the successive state $\mat{s_{t+1}}$ directly given a pair of deterministic state $\mat{s_t}$ and action $\mat{a_t}$, the GPs are used to predict $\mat{\Delta_t} = \mat{s_{t+1}} - \mat{s_t}$, the difference between the two states.
The model $f_d$ is used to predict the $d$-th entry of this vector.

Using MAP-predictions, $\mat{\Delta_t}$ is assumed to be one deterministic value and using the one-step uncertainties, the complete Gaussian distribution $\rv{\Delta_t} \sim \Gaussian{\mat{\mu_\Delta}, \mat{\Sigma_\Delta}}$ is the basis the distribution of the successive state.
Since $\mat{s_t}$ is assumed deterministic in these cases, the distribution of $\os{\rv{s_{t+1}}}$ is given by
\begin{align}
    \os{\rv{s_{t+1}}} &= \mat{s_t} + \rv{\Delta_t}, \\
    \os{\rv{s_{t+1}}} &\sim \Gaussian{\mat{s_t} + \mat{\mu_\Delta}, \mat{\Sigma_\Delta}}.
\end{align}
In both cases, the state was again assumed to be deterministic for the timestep $t+1$ and located at $\Moment{\E}{\rv{s_{t+1}}}$ in order to derive the distribution of the next state $\rv{s_{t+2}}$, preventing the accumulation of uncertainties.

\begin{figure}[t]
    \centering
    \missingfigure[figheight=.4\textheight]{Linearization Visualization}
    \caption{MISSING: Linearization Visualization}
    \label{fig:linearization}
\end{figure}
Now assume the state $\rv{s_t} \sim \Gaussian{\mat{\mu_t}, \mat{\Sigma_t}}$ is already uncertain.
In order to propagate this uncertainty through the transition model, the distribution $\Prob{\rv{\Delta_t}}$ is computed by solving the integral
\begin{align}
    \Prob{\rv{\Delta_t}} = \iint \Prob{f(\rv{s_t}, \mat{a_t}) \given \rv{s_t}} \Prob{\rv{s_t}} \diff f \diff \rv{s_t},
\end{align}
that is, by marginalizing both the belief over the current state $\rv{s_t}$ and all plausible transition models $f$.
\Cref{fig:linearization} illustrates that computing the exact predictive distribution is analytically intractable, since the GP models are highly nonlinear.

\begin{algorithm}[t]
    \caption{Computing the successive state distribution}
    \label{alg:linearization}
    Let $\rv{s_t} \sim \Gaussian{\mat{\mu_t}, \mat{\Sigma_t}}$ denote a Gaussian state distribution and $\mat{a_t} \in \Ah$ a deterministic action.
    \begin{algorithmic}[1]
        \State Compute the augmented state distribution $\Prob{\rv{\hat{s}_t}}$
        \State Approximate the predictive GP distribution $\Prob{\rv{\Delta_t}} = f(\rv{\hat{s}_t}, \mat{a_t})$
        \State Approximate the successive state distribution $\Prob{\rv{s_{t+1}}}$ as the sum of $\rv{s_t}$ and $\rv{\Delta_t}$
    \end{algorithmic}
\end{algorithm}
Instead, the distribution is approximated by a Gaussian.
Assume that the distribution $\rv{\Delta_t} \sim \Gaussian{\mat{\mu_\Delta}, \mat{\Sigma_\Delta}}$ is known.
Since $\rv{s_{t+1}}$ is the sum of $\rv{s_t}$ and $\rv{\Delta_t}$, a Gaussian approximation of the posterior distribution is given by
\begin{align}
    \rv{s_{t+1}} &\sim \Gaussian{\mat{\mu_{t+1}}, \mat{\Sigma_{t+1}}} \text{, where} \\
    \mat{\mu_{t+1}} &= \mat{\mu_t} + \mat{\mu_\Delta} \\
    \mat{\Sigma_{t+1}} &= \mat{\Sigma_t} + \mat{\Sigma_\Delta} + \Moment{\cov}{\rv{s_t}, \rv{\Delta_t}} + \Moment{\cov}{\rv{\Delta_t}, \rv{s_t}},
\end{align}
where $\Moment{\cov}{\rv{s_t}, \rv{\Delta_t}}$ denotes the covariance between the prior state and the state change.
\Cref{alg:linearization} shows the different steps necessary to compute this predictive distribution.

The first step is to compute the augmented state distribution $\Prob{\rv{\hat{s}_t}} \sim \Gaussian{\mat{\hat{\mu}_t}, \mat{\hat{\Sigma}_t}}$.
Instead of using the state $\rv{s_t}$ directly, the Gaussian process models are presented with the augmented state $\rv{\hat{s}_t}$ when used for predictions.
This distinction is made since in general, the Gaussian processes may have been trained on data which has been preprocessed, for example using dimensionality reduction techniques.
In this thesis, augmenting a state means replacing the angles $\varphi$ and $\psi$ by their sine and cosine values as shown in \cref{eq:augmented_state}.
Equivalently, the angles have to be replaced in the joint distribution of the state.
Let $\rv{\alpha} \sim \Gaussian{\mu, \sigma^2}$ denote an angle with a Gaussian distribution.
Using Euler's identity, it can be shown that the expected values of its sine and cosine are given by
\begin{align}
    \Moment{\E}{\sin \rv{\alpha}} &= \exp\!\left( -\frac{\sigma^2}{2} \right)\sin\mu, \\
    \Moment{\E}{\cos \rv{\alpha}} &= \exp\!\left( -\frac{\sigma^2}{2} \right)\cos\mu,
\end{align}
respectively \cite{deisenroth_efficient_2010}.
The variances of the two random variables can be calculated as
\begin{align}
    \Moment{\var}{\sin \rv{\alpha}} &= \frac{1}{2} - \frac{1}{2} \exp\!\left( -2 \sigma^2 \right) \cos(2\mu) - \exp(- \sigma^2) \sin^2\mu, \\
    \Moment{\var}{\cos \rv{\alpha}} &= \frac{1}{2} + \frac{1}{2} \exp\!\left( -2 \sigma^2 \right) \cos(2\mu) - \exp(- \sigma^2) \cos^2\mu.
\end{align}
The mean $\mat{\hat{\mu}_t}$ and covariance matrix $\mat{\hat{\Sigma}_t}$ of the augmented are calculated by inserting these values into the original mean $\rv{\mu_t}$ and covariance matrix $\rv{\Sigma_t}$ instead of the respective angles.
Since the sine and cosine functions are nonlinear, the covariances of the transformed angles with all other variables is assumed to be zero.

The second step in the algorithm is to calculate $\Prob{\rv{\Delta_t}}$ from the augmented state distribution.
Since it cannot be calculated directly, the distribution is approximated with a Gaussian.
This approximation is obtained by linearizing the predictive mean function of the transition model around the mean of the prior distribution.
The mean function $\mu_f$ of the transition model $f$ is therefore approximated by its first-order Taylor expansion $\lin{f}$ around $\mat{\hat{\mu}}$ which is given by
\begin{align}
    \mu_f(\mat{\hat{s}_t}, \mat{a_t}) \approx \lin{f}(\mat{\hat{s}_t}, \mat{a_t}) = \mu_f(\mat{\hat{\mu}}, \mat{a_t}) + \frac{\partial \mu_f(\mat{\hat{\mu}}, \mat{a_t})}{\partial \mat{\hat{\mu}}} (\mat{\hat{s}_t} - \mat{\hat{\mu_t}}).
\end{align}
Using this representation, $\rv{\Delta_t}$ is given as the affine transformation of $\rv{\hat{s}_t}$ defined by $\lin{f}$, which is a Gaussian $\Gaussian{\mat{\mu_\Delta}, \mat{\Sigma_\Delta}}$.
This transformation is illustrated in \cref{fig:linearization}.

The predictive mean $\rv{\mu_\Delta}$ is obtained by evaluating the linearization at the prior mean and is given by
\begin{align}
    \rv{\mu_\Delta} = \lin{f}(\mat{\hat{\mu}_t}, \mat{a_t}) = \mu_f(\mat{\hat{\mu}_t}, \mat{a_t}) = \Moment{\E}{f(\rv{\hat{\mu}_t}, \mat{a_t})}.
\end{align}
The $d$-th component of this vector is calculated using the predictive mean of the Gaussian process $f_d$ in the transition model.
Using \cref{lem:gp_posterior}, it can be computed as
\begin{align}
    \rv{\mu_\Delta}^d = \mu_f^d(\rv{\hat{\mu}_t}, \mat{a_t}) = \Moment{\E}{f_d(\rv{\hat{\mu}_t}, \mat{a_t})} = \K_d( \rv{\hat{\mu}_t}, \mat{\hat{X}})\mat{\beta}_d,
\end{align}
where $\K_d$ denotes the kernel function of $f_d$, $\mat{\beta}_d$ is a precomputed vector and $\mat{\hat{X}}$ is the augmented set of training points (or pseudo-inputs).

Using the different Gaussian process models, the derivative of the predictive mean function at $\mat{\hat{\mu}}$ can be calculated explicitly.
It is a matrix
\begin{align}
    V \coloneqq \frac{\partial}{\partial \mat{\hat{\mu}}} \mu_f(\mat{\hat{\mu}}, \mat{a_t})
\end{align}
whose $d$-th line is determined by $f_d$.
It is given by
\begin{align}
    V_d &= \frac{\partial}{\partial \mat{\hat{\mu}_t}}\mu_f^d(\mat{\hat{\mu}_t}, \mat{a_t}) = \frac{\partial \K_d(\rv{\hat{\mu}_t}, \mat{\hat{X}})}{\partial \rv{\hat{\mu}_t}}\mat{\beta}_d
\end{align}
and depends on the derivative of the kernel function with respect to the augmented mean.
The Gaussian processes used in this thesis only make the prior assumption that the transition dynamics of the bicycle benchmark are a smooth function and are trained using a squared exponential kernel.
For the squared exponential kernel as shown in \cref{def:rbf_kernel}, this derivation can be calculated as
\begin{align}
    \frac{\partial}{\partial \mat{\mu}}\K_{\text{SE}}(\mat{\mu}, \mat{x}) &= \frac{\partial}{\partial \mat{\mu}} \sigma_f^2 \exp\!\left( -\frac{1}{2} (\mat{\mu} - \mat{x})\tran \mat{\Lambda}^{-1} (\mat{\mu} - \mat{x}) \right) \\
    &= \K_{\text{SE}}(\mat{\mu}, \mat{x}) \frac{\partial}{\partial \mat{\mu}}\!\left( -\frac{1}{2} (\mat{\mu} - \mat{x})\tran \mat{\Lambda}^{-1} (\mat{\mu} - \mat{x}) \right) \\
    &= -\K_{\text{SE}}(\mat{\mu}, \mat{x}) \mat{\Lambda}^{-1} (\mat{\mu} - \mat{x}).
\end{align}

The predictive variance $\mat{\Sigma_\Delta}$ consists of two components, the propagated uncertainty about the prior state and the newly added model uncertainties for this transition.
Since the transition model is linearized, the propagation of the prior uncertainties is equivalent to a linear transformation of the prior state distribution using the predictive mean's derivative $V$.
Since the transition model is only evaluated for the prior mean $\mat{\hat{\mu}_t}$ and not for all other possible states, the true model uncertainty is unknown.
Instead, it is assumed to be constant and equal to the uncertainty for the prior mean.
The two components are independent given the prior distribution and therefore, the predictive variance is given by
\begin{alignat}{2}
    \mat{\Sigma_\Delta} &= \Moment{\var}{V \rv{\hat{s}_t}} &&+ \Moment{\var}{f(\rv{\hat{\mu}_t}, \mat{a_t})} \\
    &= V \mat{\hat{\Sigma}_t} V\tran &&+ \diag\left(
        \Moment*{\var}{f_1(\rv{\hat{\mu}_t}, \mat{a_t})},
        \ldots,
        \Moment*{\var}{f_D(\rv{\hat{\mu}_t}, \mat{a_t})}
    \right),
\end{alignat}
since $\mat{\hat{s}_t}$ is Gaussian \cite{petersen_matrix_2008}.
The covariance of the augmented state and the state change is given by
\begin{align}
    \Moment{\cov}{\rv{s_t}, \rv{\Delta_t}} = \Moment{\cov}{\rv{s_t}, V\rv{s_t}} = \Moment{\cov}{\rv{s_t}, \rv{s_t}}V\tran = \mat{\hat{\Sigma}_t}V\tran,
\end{align}
using the linearity of expectations. Equivalently, it holds that $\Moment{\cov}{\rv{\Delta_t}, \rv{s_t}} = V \mat{\hat{\Sigma}_t}$.

Using the linearization of the predictive mean function, all components necessary to approximate the posterior distribution $\Prob{\rv{s_{t+1}}}$ can be calculated.
The belief about the successive state is a Gaussian distribution characterized by
\begin{align}
    \rv{s_{t+1}} &\sim \Gaussian{\mat{\mu_{t+1}}, \mat{\Sigma_{t+1}}} \text{, where} \\
    \mat{\mu_{t+1}} &= \mat{\mu_t} + \mat{\mu_\Delta} \\
    &= \mat{\mu_t} + \Moment{\E}{f(\rv{\hat{\mu}_t}, \mat{a_t})} \\
    \mat{\Sigma_{t+1}} &= \mat{\Sigma_t} + \mat{\Sigma_\Delta} + \Moment{\cov}{\rv{s_t}, \rv{\Delta_t}} + \Moment{\cov}{\rv{\Delta_t}, \rv{s_t}} \\
    &= \mat{\Sigma_t} + V\mat{\hat{\Sigma}_t}V\tran + \Moment{\var}{f(\rv{\hat{\mu}_t}, \mat{a_t})} + \mat{\hat{\Sigma}_t}V\tran + V\mat{\hat{\Sigma}_t}.
\end{align}

\subsection{Long-Term predictions}
Using the linearization presented in \cref{sub:linearization} to propagate state distributions through the transition model, it is possible to create long-term predictions with accumulated uncertainties.
While the starting state $\mat{s_0}$ is always deterministic since it is an observation of the real system, the intermediate states $\rv{s_1}$ to $\rv{s_T}$ are random variables.
In the case of the bicycle benchmark, the transition dynamics $\DynBicycle$ are completely deterministic, so all uncertainties in the predictions stem from the transition model's confidence about its predictions.

Using linearization, the intermediate states' distributions $\Prob{\rv{s_1}}$ to $\Prob{\rv{s_T}}$ are multivariate Gaussians.
Given a pair of a state distribution $\rv{s_t}$ and an action $\mat{a_t}$, the distribution of the successive state is given by
\begin{align}
    \ms{\mat{s_{t+1}}} \colon \left\{
        \begin{aligned}
            \Dists(\Es^+) \times \Ah &\to \Dists(\Es^+) \\
            (\rv{s_t}, \mat{a_t}) &\mapsto \squeeze(f(\rv{s_t}, \mat{a_t})),
    \end{aligned}
    \right.
\end{align}
where $\squeeze$ denotes the elimination of physically impossible states by correcting the posterior mean, equivalent to \cref{eq:os_dynamic_programming}.
With multi-step uncertainties it is no longer necessary to fall back to deterministic intermediate states for prediction.

\begin{figure}[t]
    \centering
    \includestandalonewithpath{figures/solution_ms_predictions_linearization}
    \caption[Long-term predictions using multi step uncertainties]{
        Colored long-term predictions for one time horizon using multi step uncertainties and starting from a deterministic state compared to the dashed simulation.
        Considering multi-step uncertainties yields the shaded measure of confidence, which in this case correctly identifies that the predictive error increases after about half the time horizon.
    }
    \label{fig:ms_predictions}
\end{figure}
\Cref{fig:ms_predictions} shows an example for long-term predictions using multi-step uncertainties.
As with the other approaches presented in this thesis, all intermediate states $\ms{\rv{s_1}}$ to $\ms{\rv{s_T}}$ can be calculated using dynamic programming.
In contrast to the one-step uncertainties presented in \cref{fig:os_uncertainties}, the state distributions obtained using linearization quite accurately estimate the predictive error.
The uncertainties reach the same order of magnitude as the dynamic range of their variable and become visible in the plot after a few time steps.
After about seven time steps, the uncertainties about $\omega$ grow and two standard deviations cover most of the value range.
At this point, the model's predictions can be interpreted as almost complete uncertainty about the correct state.
At the same time however, the predictive mean is no longer close to the correct trajectory.
The model correctly identifies the decline in accuracy of its predictions.

Large uncertainties about $\omega$ lead to probability mass in the tail of the Gaussian being placed outside of the valid range of values.
For every such time step, the accumulated probability $\Prob{\ms{\PFallen}_t}$ of already having fallen down at some time before or at time step~$t$ increases.
This probability is calculated via dynamic programming using the rules presented in \cref{sub:os_predictions}.
While falling down does not cause a direct punishment (since the associated reward is zero), a higher probability of an ended trajectory lowers the amount of reward which can be earned for the later time steps in the prediction.

Given the intermediate state distributions $\ms{\rv{s_t}} \sim \Gaussian{\ms{\mat{\mu_t}}, \ms{\mat{\Sigma_t}}}$, the action value function using multi-step uncertainties is calculated analogously to the one-step action value function presented in \cref{eq:os_action_value_function}, since only the way of calculating the state distributions has changed.
Having calculated the terminal probabilities $\Prob{\ms{\PGoal_t}}$ and $\Prob{\ms{\PFallen_t}}$ it is given by
\begin{align}
    \label{eq:ms_action_value_function}
    \begin{split}
        \ms{\AVlu}_{\mat{s_0}}(\mat{a_0}, \dots, \mat{a_{T-1}}) &= \Moment*{\E}{\sum_{t=0}^T \gamma^t \RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
        &= \sum_{t=0}^T \gamma^t \Moment*{\E}{\RwdBicycle(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}} \\
        &= \sum_{t=0}^T \gamma^t \bigg( 2 \cdot \Prob{\ms{\PGoal}_t} + (1 - \Prob{\ms{\PGoal}_t})(1 - \Prob{\ms{\PFallen}_t}) \\
    &\qquad \sqrt{2\pi\sigma_{\text{angle}}^2} \Gaussian{\pi\tran \ms{\mat{\mu_t}} \given 0, \sigma_{\text{angle}}^2 + \pi\tran \ms{\mat{\Sigma_t}}\pi} \bigg).
    \end{split}
\end{align}
Since the starting state $\mat{s_0}$ is deterministic, the reward function $\RwdBicycle$ can either be evaluated directly or the state can be approximated using a distribution $\Gaussian{\mat{s_0}, \epsilon\Eye}$ with $\epsilon \to 0$.

\subsection{Posterior States using the Truncation of Gaussians}
Using the intermediate state distributions $\Prob{\ms{\rv{s_t}}}$ directly when predicting successive states is a good approximation in most cases but is not correct when the probabilities $\Prob{\rv{s_t} \in \Fallen}$ or $\Prob{\rv{s_t} \in \Goal}$ are large.
The transition model $f$ is trained using only non-terminal state transitions, that is, state transitions where neither the prior nor the posterior state is terminal.
This implies the assumption that the transition model only operates on non-terminal states, similar to transition dynamics $\Dyn$ being defined on $\Es$, the set of non terminal states, in general in \cref{def:transition_dynamics}.
For any state $\mat{s_t}$ in the set of terminal states $\Tee$, the successive state $\mat{s_{t+1}}$ is always defined to be equal to $\mat{s_t}$, regardless of the action performed by the agent.

\begin{figure}[t]
    \centering
    \includestandalonewithpath{figures/solution_ms_predictions_truncation}
    \caption[Long-term predictions using multi step uncertainties with truncation]{
        Colored long-term predictions for one time horizon using multi step uncertainties with truncation and starting from a deterministic state compared to the dashed simulation.
        While the amount of probability mass spread over terminal states is negligible, the predictions using the truncation of Gaussians is almost equal to using only linearization as shown in \cref{fig:ms_predictions}.
        However, for the last time steps, truncating the terminal regions of $\omega$ yields more correct predictions.
    }
    \label{fig:ms_truncation_predictions}
\end{figure}
In the action value function, this is reflected by the accumulative probabilities $\Prob{\PGoal}$ and $\Prob{\PFallen}$.
Any probability mass which falls to their respective terminal states is assumed to stay there for the remaining time steps, since their respective trajectories end at that point.
When predicting successive states using the function $\ms{\mat{s_{t+1}}}$, this information is not considered however.
The prior Gaussian distribution used for linearization can have considerable probability mass attributed to terminal states.
Consulting the transition model however implies the condition that the trajectory has not yet ended.
This condition is encoded in the successive state function $\tg{\mat{s_{t+1}}}$ given by
\begin{align}
    \tg{\mat{s_{t+1}}} \colon \left\{
        \begin{aligned}
            \Dists(\Es^+) \times \Ah &\to \Dists(\Es^+) \\
            (\rv{s_t}, \mat{a_t}) &\mapsto \squeeze(f\Cond{\rv{s_t}, \mat{a_t} \given \rv{s_t} \not\in \Tee}).
    \end{aligned}
    \right.
\end{align}

Given a Gaussian state distribution $\rv{s_t} \sim \Gaussian{\mat{\mu_t}, \mat{\Sigma_t}}$, the probability distribution
\begin{align}
    \Prob{\rv{s_t} \given \rv{s_t} \not\in\Tee}
\end{align}
can in general not be calculated analytically and is approximated with another Gaussian using moment matching.
In the bicycle benchmark, the density function of this distribution can be obtained by truncating the original Gaussian distribution using the hyperplanes defined by \cref{eq:terminal_probabilies} and renormalizing the result.
Algorithms to calculate the moments of the resulting distribution are described in \cite{herbrich_gaussian_2005} and \cite{toussaint_technical_2009}.
The action value function $\tg{\AVlu}_{\mat{s_0}}$ is equivalent to the action value function without the truncation of Gaussians $\ms{\AVlu}_{\mat{s_0}}$.

\Cref{fig:ms_truncation_predictions} shows predictions using multi-step uncertainties and the truncation of the state distribution Gaussian after every time step.
While the predictions with small uncertainties are almost identical to the predictions in \cref{fig:ms_predictions}, they differ once the uncertainties are large enough to distribute mass over terminal states.
After every time step, the mean of the prediction for $\omega$ tends more towards the middle of the value range and thus towards the most uninformative prediction with a mean at zero and the uncertainty spread over the complete range of possible values.

For every time step, the probability mass spread over terminal states represents steps from a non-terminal state at time step $t$ (ensured by the conditional probability in the function $\tg{\mat{s_{t+1}}}$) to a terminal state at $t+1$.
The probabilities $\Prob{\tg{\rv{s_{t+1}}} \in \Goal}$ and $\Prob{\tg{\rv{s_{t+1}}} \in \Fallen}$ therefore correctly represent the probability of reaching the goal or falling exactly at time step $t+1$ respectively.
In contrast, the probabilities $\Prob{\ms{\rv{s_{t+1}}} \in \Goal}$ and $\Prob{\ms{\rv{s_{t+1}}} \in \Fallen}$ also contain instances where $\ms{\rv{s_t}}$ already was in the respective sets.
The multi-step uncertainties without the truncation of Gaussians therefore overestimate the terminal probabilities $\Prob{\PGoal_t}$ and $\Prob{\PFallen_t}$.

\subsection{Results}
\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/solution_ms_goal_percentage}
    \caption[Results using multi step uncertainties]{
        The success rate for different numbers of pseudo inputs with the standard error of the mean.
        Using multi step uncertainties without truncation considerably improves the performance for the less expressive models and always yields an increase of the success rate of at least 10 percent when compared to MAP-predictions.
        The performance of multi step uncertainties with truncation is comparable to MAP-predictions.
    }
    \label{fig:ms_results}
\end{figure}
\begin{figure}[p]
    \centering
    \includestandalonewithpath{figures/solution_ms_trajectory_linearization}
    \caption[Successful trajectory using multi step uncertainties]{
        A single successful trajectory using multi step uncertainties.
        Equivalently to the successful MAP trajectory in \cref{fig:map_successful_trajectory}, the bicycle starts at about 18 meters distance from the goal with an angle of about 90 degrees.
        Similar to the first trajectory, PSO-P is first concerned with reducing the angle towards the goal in the first thirty time steps and then keeps driving towards the goal.
        Using multi step uncertainties, instead of driving straight, the bicycle's trajectory describes a wave, with the controller always keeping the bicycle leaning towards one side.
    }
    \label{fig:ms_successful_trajectory}
\end{figure}
\begin{figure}[p]
    \centering
    \includestandalonewithpath{figures/solution_ms_xy_trajectories_linearization}
    \caption[Episodes using multi step uncertainties without truncation]{
        The $x$ and $y$ coordinates of the front tyre in representative episodes when using multi step uncertainties without truncation with marks at the starting states.
        Successful episodes are colored in green and failed episodes are colored in red.
        Incorporating multi step uncertainties can increase the success rate considerably.
        However, instead of driving in relatively straight lines as in \cref{fig:map_trajectories,fig:os_trajectories}, the PSO-Policy favours wavy trajectories when considering multi step uncertainties.
    }
    \label{fig:ms_trajectories}
\end{figure}
\begin{figure}[p]
    \centering
    \includestandalonewithpath{figures/solution_ms_xy_trajectories_truncation}
    \caption[Episodes using multi step uncertainties with truncation]{
        The $x$ and $y$ coordinates of the front tyre in representative episodes when using multi step uncertainties with truncation with marks at the starting states.
        Successful episodes are colored in green and failed episodes are colored in red.
        Trajectories generated when using the truncation of Gaussians look similar to the trajectories without truncation as showed in \cref{fig:ms_trajectories}, also describing waves.
    }
    \label{fig:ms_truncation_trajectories}
\end{figure}
\Cref{fig:ms_trajectories,fig:ms_truncation_trajectories} show trajectories generated by applying PSO-P to the one-step value functions $\ms{\AVlu}_{\mat{s_0}}$ and $\tg{\AVlu}_{\mat{s_0}}$ respectively.
Both the models and starting states are the same as the ones used in \cref{fig:map_trajectories,fig:os_trajectories}, the corresponding figures for the maximum-a-posteriori and the one-step uncertainties approaches.
While the trajectories look very similar to each other, they are different to the trajectories generated without informative uncertainties.

PSO-P is able to balance the bicycle longer for almost every starting state and is more successful when using multi-step uncertainties without the truncation of Gaussians.
If PSO-P is successful using MAP-predictions or one-step uncertainties, the trajectories consist of a single curve to orient the bicycle directly followed by driving straight towards the goal.
Using multi-step uncertainties, PSO-P avoids driving in a straight line.
\todo{Is this true?}Instead, the bicycle's movement describes a wave, where the policy is always leaning slightly into one direction.
This effect is also visible in \cref{fig:ms_successful_trajectory}, which shows all state variables over time for one successful trajectory.

When using multi-step uncertainties, PSO-P must avoid action sequences which lead to very uncertain predictions, since for high uncertainties, the probabilities for reaching a terminal state grow.
A possible explanation for the wavy movement is that driving straight is hard to predict for the transition model for two reasons.
Firstly, the training data described in \cref{sub:data_sets} does not contain many samples of driving straight.
The training trajectories start with a bicycle in an upright position going straight, but applying random actions cannot balance the bicycle.
Since the bicycle quickly begins to fall, most transitions observed in the trajectories are do not show a balanced bicycle.
Similarly, randomly sampled transitions do not show an upright bicycle with high probability.
Because of this, the transition models are more confident about the parts of the system where the bicycle is leaning slightly and therefore, the long-term uncertainties are smaller in this case.
And secondly, a balanced bicycle is unstable and if left alone, the bicycle will start to fall down.
If the bicycle is exactly upright, it is hard for the models to predict if the bicycle will fall towards the left or the right, which leads to a bimodal distribution.
Since all approximations used in this thesis are unimodal, this leads to a higher uncertainty compared to states where the bicycle is leaning slightly, since in this case, the natural development of the system is easier to predict.

Not driving in straight lines means that PSO-P takes longer to reach the goal for successful trajectories compared to the other approaches.
However, the statistical evaluation in \cref{fig:ms_results} shows that in most cases, using multi-step uncertainties allows PSO-P to reach the goal significantly more often.
The policy trades the higher reward of a quicker solution against a safer route which is more defensive and which the transition models can more easily predict.
\todo{I am basically guessing here}For models with a higher numer of pseudo-inputs, using multi-step uncertainties with the truncation of Gaussians becomes less successful than the other approaches however.
A possible explanation for this is the fact that the truncation of Gaussians has the tendency to create uninformed predictions for later time steps, independent of the actions proposed by PSO-P.
It therefore becomes harder for the optimization process to identify good action sequences.
While for weaker models, this can be beneficial because is avoids the exploitation of model bias, it might prevent the exploitation of model knowledge for better models.

\section{Discussion of the Approaches}
\begin{table}[t]
    \centering
    \caption{Comparison of the results of the evaluation.}
    \label{tab:evaluation_results}
    \begin{tabularx}{\tablewidth}{>{\hsize=2.6\hsize}X>{\centering\hsize=.6\hsize}X>{\centering\hsize=.6\hsize}X>{\centering\hsize=.6\hsize}X>{\centering\hsize=.6\hsize\arraybackslash}X}
        \toprule
        Metric & MAP & OS & MS & MS-TG \\
        \midrule
        Number of Trajectories & 9660 & 9660 & 9660 & 9660 \\
        \addlinespace
        Success Rate & \SI[mode=text]{53.4}{\percent} & \SI[mode=text]{63.8}{\percent} & \textbf{\SI[mode=text,detect-weight]{75.8}{\percent}} & \SI[mode=text]{52.5}{\percent} \\
        Failure Rate & \SI[mode=text]{46.6}{\percent} & \SI[mode=text]{36.2}{\percent} & \textbf{\SI[mode=text,detect-weight]{24.2}{\percent}} & \SI[mode=text]{47.5}{\percent} \\
        \addlinespace
        Mean Mean-Reward & 0.87 & 1.01 & \textbf{1.17} & 0.91 \\
        Median Mean-Reward & 0.85 & 1.00 & \textbf{1.24} & 0.96 \\
        \addlinespace
        Mean Time to Goal & \textbf{59.9} & 62.0 & 66.5 & 68.1 \\
        Median Time to Goal & \textbf{60} & \textbf{60} & 63 & 63 \\
        \bottomrule
    \end{tabularx}
\end{table}
In \cref{tab:evaluation_results}, the results of the evaluation of the four approaches presented in this thesis are compared.
These approaches are using maximum-a-posteriori predictions only (\emph{MAP}) as described in \cref{sub:map_predictions}, using one-step uncertainties (\emph{OS}) as described in \cref{sub:os_predictions} and using multi-step uncertainties, both without (\emph{MS}) and with the truncation of Gaussians (\emph{MS-TG}), both of which are detailed in \cref{sub:ms_predictions}.
All evaluations were performed according to \cref{alg:evaluation_setup} and \cref{tab:evaluation_parameters}.
To create the respective 9660 trajectories, the same data sets, models and starting states were used for all techniques.

Considering the model uncertainties provided by the Gaussian process transition models during planning with PSO-P can improve the generated trajectories.
Both the knowledge about one-step uncertainties and multi-step uncertainties allows the policy to account for errors in the transition model and prevent the exploitation of model bias to some extent.
In the bicycle benchmark, exploiting model bias mostly means planning trajectories for which the model assumes that the resulting value of $\omega$ will be equal to its maximum value as precisely as possible.
In this setting, even small mistakes of the models cause the bicycle to fall down and the trajectory to fail.

\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/solution_results_goal_percentage}
    \caption[Success rates of all approaches] {
        The success rate for different numbers of pseudo inputs with the standard error of the mean.
        Considering uncertainties during long-term planning can be beneficial.
        Multi step uncertainties without truncation perform much better with less expressive models when compared to one step uncertainties.
        For a larger number of pseudo inputs, the two approaches both perform about 10 percent better than MAP-predictions.
    }
    \label{fig:results_goal_percentage}
\end{figure}
\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/solution_results_mean_mean_reward}
    \caption[Mean mean rewards of all approaches]{
        The mean mean reward for different numbers of pseudo inputs with the standard error of the prediction.
        The results of the mean mean reward metric are comparable to the success rate.
        While multi step uncertainties with truncation performs about as good as MAP-predictions, both one step and multi step uncertainties without truncation perform considerably better.
        Considering uncertainties is more beneficial for less expressive models.
    }
    \label{fig:results_mean_mean_reward}
\end{figure}
\Cref{tab:evaluation_results} shows that multi-step uncertainties improve the overall success rate of PSO-P in the bicycle benchmark from about half the trajectories to about 75 percent.
However, the results when using the truncation of Gaussians are comparable with only using MAP-predictions.
In \cref{fig:results_goal_percentage}, the success rates of models with different complexities are compared.
While both the one-step and multi-step approaches without the truncation of Gaussians consistently perform better than the classic alternative, the last technique only shows some improvement for less expressive models.

This trend continues when comparing the mean rewards earned throughout a trajectory.
Given a series of observed (and therefore deterministic) states beginning with the starting state $\mat{s_0}$, the mean reward earned throughout a trajectory is defined as
\begin{align}
    \MeanRwdBicycle(\mat{s_0}, \dots, \mat{s_T}) \coloneqq \frac{1}{T} \sum_{t=0}^T \RwdBicycle(\mat{s_t}).
\end{align}
Note that $T$ is a constant defined in \cref{tab:evaluation_parameters} and can be larger than the length of the trajectory.
In this case, the terminal state at the end of the trajectory is repeated infinitely as defined in \cref{eq:rl_successive_state}.
\Cref{fig:results_mean_mean_reward} shows the mean mean reward of all trajectories created with the different classes of models.
Especially for less expressive models which are forced to generalize more, the techniques which consider model uncertainties perform better.
While both OS and MS earn significantly more reward, MS-TG only slightly outperforms the baseline.

The increase in performance of MS comes with a price however.
When comparing the mean number of time steps required to reach the goal in successful trajectories, it can be seen that MAP is able to reach the goal more quickly on average than either OS or MS.
Using MAP, PSO-P plans trajectories which are as short and aggressive as possible, therefore minimizing the required time.
With MS, the bicycle drives much more defensive but takes more time to reach the goal, as can be seen in \cref{fig:ms_trajectories}.
The mean time to goal presented in \cref{tab:evaluation_results} are somewhat biased though, as PSO-P with MAP tends to fail more often for longer or more complex trajectories which require long curves.
The median time to goal suggests that for similar situations, the time difference between the two approaches could be smaller.

\section{Summary}
This chapter applied the mathematical foundations presented in \cref{cha:theory} to the bicycle benchmark problem.
It described the sampling of data sets and the design of Gaussian process models for the transition function $\DynBicycle$.
Based on these models, it presented the classical solution of using deterministic maximum-a-posterior predictions to create long-term predictions.
As a baseline for comparison, PSO-P was applied to the action-value-function $\AVlu$ using a reward function $\RwdBicycle$ formulated using reward shaping.

A first improvement over plain MAP-predictions was given by using the one-step uncertainties yielded by the transition models to approximate the expected reward $\Moment{\E}{\RwdBicycle(\rv{s_t})}$ of the intermediate states $\rv{s_t}$ more exactly.
If the intermediate states are no longer deterministic, it is no longer clear when a predictive trajectory ends.
The solution proposed in this thesis is to calculate accumulative probabilities $\Prob{\PGoal_t}$ and $\Prob{\PFallen_t}$ for the different termination conditions for every time step and weigh the expected reward accordingly.

Since one-step uncertainties cannot correctly model the iterated confidence of the transition model, the next approach is to propagate the state uncertainties through the nonlinear transition models using linearization.
These accumulated uncertainties reduce model bias since they give an accurate measure of the correctness of long-term trajectories and reduce model bias.
Using PSO-P with multi-step uncertainties results in more defensive trajectories which are more often successful.
