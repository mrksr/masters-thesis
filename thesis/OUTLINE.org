* Introduction
  - Machine Learning, Machine Intelligence
  - High-Level Idea of "Learning to control systems" or "Making Decisions"
  - Why do we have uncertainties? Why are they important?
  - Current examples (AlphaGo? Humans in Robotics? Cars?)

* Definition of the Bicycle Benchmark
  - Hands-On example of a control problem
  - State variables
  - Dynamics
  - Actions
  - Boundaries
  - Derivatives

* State of the Art
** Reinforcement Learning
*** Problem statement
   - "Generalization" of Bicycle Benchmark
   - Agent and environment
   - Similarities to "traditional" controllers (like Kalman filters)
   - States and Actions
   - Markov-Property of the state, Problem is a (continuous) Markov-Decision-Process
   - Reward functions
   - Return and Value functions
*** Model-Based Reinforcement Learning
   - Interaction vs. Simulation
   - Interaction is expensive but makes the model better
   - Online vs. Offline: We gather all data first
   - Three layers
     + Learning the dynamics (GP)
     + Long-Term Predictions (Trivial summation vs. Linearization)
     + Policy Optimization (not here) or Model Predictive Control (PSO)
     + NOTE: We are not quite happy with using the term Model Predictive
       Control here
   - Use uncertainties to reduce model bias
   - We can use uncertainties in all three
** Dynamics: Gaussian Processes
*** Definition
    - Gaussian processes as Stochastic processes
    - Every subset of which is Gaussian
    - Distribution over functions rather than vectors
    - Probably only the function-space view
*** Kernel functions
    - General properties
    - Simple Example like linear
    - Stationary Kernels: RBF
    - Maybe hint to Non-Stationary: Neural Network
*** Regression with GPs
    - Predictive posterior
    - Marginal likelihood for finding hyperparameters
    - Those are the basis for SPGP
*** Sparse Approximations (SPGP)
    - Inverting K_NN is O(N^3)
    - Approximate in smaller subspace
    - Formulation of SPGP
    - Interpretation as GP with specific Kernel function, therefore O(NM^2)
    - Probably skip the concrete likelihood and implementation details
      (matrix inversion lemma)
    - Mention variational approaches for likelihood optimization
** PSO
   - Optimization of value function
   - Why do we not use gradient-based approaches
*** Basic PSO
    - Positions, Velocities
    - Neighborhoods
    - Update-Step
*** Improvements
    - Velocity clamping
    - Values for omega and gammas

* Solution of the Bicycle Benchmark
** Standard Approach: Means Only
*** Dynamics: with GP Models
   - Model the deltas
   - One model per dimension
   - Implicit assumption that outputs are independent
   - Choice of Data Sets
     + Random exploration
     + Random sampling
     + Size of data sets
   - Values for N, M, Kernel
*** MPC/Unrolling: PSO-Optimization of Value Function
   - Reward function: Non-Probabilistic
   - PSO Parameters
*** Results and Discussion of Problems
   - Evaluation
     + Create data set
     + Train models
     + ~15 PSO runs
     + Mean-Mean Reward over multiple runs
     + Some interesting single trajectories
   - Actual results
** Our first Approach: One-Step Uncertainties
*** New Reward Function Reward function
   - Definition without uncertainties
   - Extension with probabilities
   - "Stop Probabilities" and Bimodality
   - Extension of PILCO, truncated gaussians
** Our second Approach: Linearization
   - Now we do proper long-term predictions
   - Starting point: Model that gives posterior uncertainties
   - But only for single points
   - How to get a posterior distribution given a prior one?
   - Derive formulae for GPs via Taylor expansion
   - Maybe mention moment matching?

* Conclusion
** Discussion of the Approaches
  - Does it actually help?
  - Comparative Plots
** Possible Improvements
  - Linearization does not combine so well with PSO
  - Can we do better with closed policies?
