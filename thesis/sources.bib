
@article{deisenroth_gaussian_2015,
	title = {Gaussian processes for data-efficient learning in robotics and control},
	volume = {37},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6654139},
	pages = {408--423},
	number = {2},
	journaltitle = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
	urldate = {2016-02-01},
	date = {2015},
	file = {Snapshot:/home/markus/sync/zotero/storage/RJFT85BV/articleDetails.html:text/html}
}

@inproceedings{deisenroth_pilco:_2011,
	title = {{PILCO}: A model-based and data-efficient approach to policy search},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf},
	shorttitle = {{PILCO}},
	pages = {465--472},
	booktitle = {Proceedings of the 28th International Conference on machine learning ({ICML}-11)},
	author = {Deisenroth, Marc and Rasmussen, Carl E.},
	urldate = {2016-02-01},
	date = {2011},
	file = {[PDF] von wustl.edu:/home/markus/sync/zotero/storage/UCEUWK2U/Deisenroth and Rasmussen - 2011 - PILCO A model-based and data-efficient approach t.pdf:application/pdf}
}

@thesis{damianou_deep_2015,
	title = {Deep Gaussian processes and variational propagation of uncertainty},
	url = {http://etheses.whiterose.ac.uk/id/eprint/9968},
	institution = {University of Sheffield},
	type = {phdthesis},
	author = {Damianou, Andreas},
	urldate = {2016-02-01},
	date = {2015},
	file = {[PDF] von whiterose.ac.uk:/home/markus/sync/zotero/storage/S8W2ZIJR/Damianou - 2015 - Deep Gaussian processes and variational propagatio.pdf:application/pdf;Snapshot:/home/markus/sync/zotero/storage/GTC5SWKF/9968.html:text/html}
}

@inproceedings{titsias_variational_2009,
	title = {Variational learning of inducing variables in sparse Gaussian processes},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS09_Titsias.pdf},
	pages = {567--574},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Titsias, Michalis K.},
	urldate = {2016-02-01},
	date = {2009},
	file = {[PDF] von wustl.edu:/home/markus/sync/zotero/storage/ENE55SUP/Titsias - 2009 - Variational learning of inducing variables in spar.pdf:application/pdf}
}

@article{rasmussen_gaussian_2006,
	title = {Gaussian processes for machine learning},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.3414},
	author = {Rasmussen, Carl Edward},
	urldate = {2016-02-01},
	date = {2006},
	file = {Snapshot:/home/markus/sync/zotero/storage/Q9WU9NSU/summary.html:text/html}
}

@book{murphy_machine_2012,
	title = {Machine Learning: A Probabilistic Perspective},
	isbn = {978-0-262-01802-9},
	shorttitle = {Machine Learning},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package -- {PMTK} (probabilistic modeling toolkit) -- that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	pagetotal = {1098},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P.},
	date = {2012-08-24},
	langid = {english},
	keywords = {Computers / Machine Theory}
}

@book{scholkopf_learning_2002,
	title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
	isbn = {978-0-262-19475-4},
	shorttitle = {Learning with Kernels},
	abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine ({SVM}). This gave rise to a new class of theoretically elegant learning machines that use a central concept of {SVMs} -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to {SVMs} and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
	pagetotal = {658},
	publisher = {{MIT} Press},
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	date = {2002-01},
	langid = {english},
	keywords = {Computers / Intelligence ({AI}) \& Semantics, Computers / Programming / General}
}

@thesis{deisenroth_efficient_2010,
	title = {Efficient Reinforcement Learning using Gaussian Processes},
	url = {http://www.cs.washington.edu/research/projects/aiweb/media/papers/tmppqidj5},
	institution = {{KIT} Scientific Publishing},
	type = {phdthesis},
	author = {Deisenroth, Marc Peter},
	urldate = {2016-04-19},
	date = {2010},
	file = {deisenroth.pdf:/home/markus/sync/zotero/storage/TNS29KIU/deisenroth.pdf:application/pdf;[HTML] von google.de:/home/markus/sync/zotero/storage/9T77GEEF/books.html:text/html}
}

@book{engelbrecht_fundamentals_2006,
	title = {Fundamentals of computational swarm intelligence},
	url = {http://dl.acm.org/citation.cfm?id=1199518},
	publisher = {John Wiley \& Sons},
	author = {Engelbrecht, Andries P.},
	urldate = {2016-02-01},
	date = {2006},
	file = {Snapshot:/home/markus/sync/zotero/storage/FSXKBSAR/citation.html:text/html}
}

@inproceedings{snelson_sparse_2005,
	title = {Sparse Gaussian processes using pseudo-inputs},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_543.pdf},
	pages = {1257--1264},
	booktitle = {Advances in neural information processing systems},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	urldate = {2016-02-01},
	date = {2005},
	file = {[PDF] von wustl.edu:/home/markus/sync/zotero/storage/PDE2IBN3/Snelson and Ghahramani - 2005 - Sparse Gaussian processes using pseudo-inputs.pdf:application/pdf}
}

@thesis{snelson_flexible_2007,
	title = {Flexible and efficient Gaussian process models for machine learning},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.4041&rep=rep1&type=pdf},
	institution = {Citeseer},
	type = {phdthesis},
	author = {Snelson, Edward Lloyd},
	urldate = {2016-02-01},
	date = {2007},
	file = {[PDF] von psu.edu:/home/markus/sync/zotero/storage/3D3FKUK6/Snelson - 2007 - Flexible and efficient Gaussian process models for.pdf:application/pdf}
}

@article{petersen_matrix_2008,
	title = {The matrix cookbook},
	volume = {7},
	url = {http://www.cim.mcgill.ca/~dudek/417/Papers/matrixOperations.pdf},
	pages = {15},
	journaltitle = {Technical University of Denmark},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and {others}},
	urldate = {2016-02-01},
	date = {2008},
	file = {imm3274.pdf:/home/markus/sync/zotero/storage/CA4343RM/imm3274.pdf:application/pdf}
}

@inproceedings{titsias_bayesian_2010,
	title = {Bayesian Gaussian process latent variable model},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_TitsiasL10.pdf},
	pages = {844--851},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Titsias, Michalis K. and Lawrence, Neil D.},
	urldate = {2016-02-01},
	date = {2010},
	file = {[PDF] von wustl.edu:/home/markus/sync/zotero/storage/XR3RNZW2/Titsias and Lawrence - 2010 - Bayesian Gaussian process latent variable model.pdf:application/pdf}
}

@article{deisenroth_learning_????,
	title = {Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning},
	url = {http://core.ac.uk/download/pdf/241164.pdf},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward and Fox, Dieter},
	urldate = {2016-02-01},
	file = {[PDF] von core.ac.uk:/home/markus/sync/zotero/storage/IH9TN5RX/Deisenroth et al. - Learning to Control a Low-Cost Manipulator using D.pdf:application/pdf}
}

@online{_gaussian_????,
	title = {Gaussian Processes for Global Optimization - gpgo-gpss2015.pdf},
	url = {http://gpss.cc/gpss15/talks/gpgo-gpss2015.pdf},
	urldate = {2016-02-01},
	file = {Gaussian Processes for Global Optimization - gpgo-gpss2015.pdf:/home/markus/sync/zotero/storage/Q27GDN55/gpgo-gpss2015.html:text/html}
}

@online{_machine_2015,
	title = {Machine Learning Trick of the Day (4): Reparameterisation Tricks},
	url = {http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/},
	shorttitle = {Machine Learning Trick of the Day (4)},
	abstract = {Our ability to rewrite statistical problems in an equivalent but different form, to reparameterise them, is one of the most general-purpose tools we have in mathematical statistics. We used reparam…},
	titleaddon = {The Spectator},
	urldate = {2016-04-03},
	date = {2015-10-29},
	file = {Snapshot:/home/markus/sync/zotero/storage/DXE759FJ/machine-learning-trick-of-the-day-4-reparameterisation-tricks.html:text/html}
}

@book{sutton_reinforcement_1998,
	title = {Reinforcement learning: An introduction},
	url = {https://books.google.de/books?hl=de&lr=&id=CAFR6IBF4xYC&oi=fnd&pg=PA3&ots=eaSWPR76YI&sig=9_9gy-XbXlxGFZPp9NyxDJ1qBbo},
	shorttitle = {Reinforcement learning},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	urldate = {2016-02-01},
	date = {1998},
	file = {Snapshot:/home/markus/sync/zotero/storage/3IEI53VV/books.html:text/html}
}

@article{mcallister_data-efficient_2016,
	title = {Data-Efficient Reinforcement Learning in Continuous-State {POMDPs}},
	url = {http://arxiv.org/abs/1602.02523},
	journaltitle = {{arXiv} preprint {arXiv}:1602.02523},
	author = {{McAllister}, Rowan and Rasmussen, Carl Edward},
	urldate = {2016-02-28},
	date = {2016},
	file = {[PDF] von arxiv.org:/home/markus/sync/zotero/storage/8AMCCZ6R/McAllister and Rasmussen - 2016 - Data-Efficient Reinforcement Learning in Continuou.pdf:application/pdf;Snapshot:/home/markus/sync/zotero/storage/EGW68SWZ/1602.html:text/html}
}

@inproceedings{randlov_learning_1998,
	title = {Learning to Drive a Bicycle Using Reinforcement Learning and Shaping.},
	volume = {98},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.3038&rep=rep1&type=pdf},
	pages = {463--471},
	booktitle = {{ICML}},
	publisher = {Citeseer},
	author = {Randløv, Jette and Alstrøm, Preben},
	urldate = {2016-04-15},
	date = {1998},
	file = {download:/home/markus/sync/zotero/storage/QRBGIB4D/download.pdf:application/pdf}
}

@article{duvenaud_black-box_????,
	title = {Black-Box Stochastic Variational Inference in Five Lines of Python},
	url = {http://people.seas.harvard.edu/~dduvenaud/papers/blackbox.pdf},
	author = {Duvenaud, David and Adams, Ryan P.},
	urldate = {2016-04-03},
	file = {[PDF] von harvard.edu:/home/markus/sync/zotero/storage/S7VF63WJ/Duvenaud and Adams - Black-Box Stochastic Variational Inference in Five.pdf:application/pdf}
}

@incollection{watter_embed_2015,
	title = {Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images},
	url = {http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf},
	shorttitle = {Embed to Control},
	pages = {2746--2754},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2016-04-03},
	date = {2015},
	file = {NIPS Full Text PDF:/home/markus/sync/zotero/storage/TZQZA8JX/Watter et al. - 2015 - Embed to Control A Locally Linear Latent Dynamics.pdf:application/pdf;NIPS Snapshort:/home/markus/sync/zotero/storage/MRWSVSW3/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.html:text/html}
}

@article{brochu_tutorial_2010,
	title = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
	url = {http://arxiv.org/abs/1012.2599},
	abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
	journaltitle = {{arXiv}:1012.2599 [cs]},
	author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
	urldate = {2016-02-01},
	date = {2010-12-12},
	eprinttype = {arxiv},
	eprint = {1012.2599},
	keywords = {Computer Science - Learning, G.1.6, G.3, I.2.6},
	file = {arXiv\:1012.2599 PDF:/home/markus/sync/zotero/storage/VIHCEABW/Brochu et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive C.pdf:application/pdf;arXiv.org Snapshot:/home/markus/sync/zotero/storage/W3UT47IB/1012.html:text/html}
}

@video{neil_lawrence_matthew_2015,
	title = {Matthew Hoffman: Information-based methods for Bayesian Optimization},
	url = {https://www.youtube.com/watch?v=1RvgFDWPEh4&feature=youtu.be},
	shorttitle = {Matthew Hoffman},
	abstract = {The talk presented at Workshop on Gaussian Processes for Global Optimization at Sheffield, on September 17, 2015},
	author = {{Neil Lawrence}},
	urldate = {2016-02-01},
	date = {2015-09-17},
	keywords = {Bayesian Optimization, Gaussian Process, Machine Learning}
}

@book{astrom_introduction_1971,
	title = {Introduction to Stochastic Control Theory},
	isbn = {978-0-08-095579-7},
	abstract = {In this book, we study theoretical and practical aspects of computing methods for mathematical modelling of nonlinear systems. A number of computing techniques are considered, such as methods of operator approximation with any given accuracy; operator interpolation techniques including a non-Lagrange interpolation; methods of system representation subject to constraints associated with concepts of causality, memory and stationarity; methods of system representation with an accuracy that is the best within a given class of models; methods of covariance matrix estimation;methods for low-rank matrix approximations; hybrid methods based on a combination of iterative procedures and best operator approximation; andmethods for information compression and filtering under condition that a filter model should satisfy restrictions associated with causality and different types of memory.As a result, the book represents a blend of new methods in general computational analysis,and specific, but also generic, techniques for study of systems theory ant its particularbranches, such as optimal filtering and information compression.- Best operator approximation,- Non-Lagrange interpolation,- Generic Karhunen-Loeve transform- Generalised low-rank matrix approximation- Optimal data compression- Optimal nonlinear filtering},
	pagetotal = {318},
	publisher = {Elsevier},
	author = {Åström, Karl J.},
	date = {1971-02-27},
	langid = {english},
	keywords = {Mathematics / Algebra / Linear, Mathematics / Calculus, Mathematics / Mathematical Analysis, Mathematics / Numerical Analysis, Technology \& Engineering / Mechanical}
}

@article{hein_reinforcement_2016,
	title = {Reinforcement Learning with Particle Swarm Optimization Policy ({PSO}-P) in Continuous State and Actionspaces},
	volume = {7},
	number = {3},
	author = {Hein, Daniel and Hentschel, Alexander and Runkler, Thomas and Udluft, Steffen},
	date = {2016-07},
	file = {Reinforcement Learning with Particle Swarm Optimization Policy.pdf:/home/markus/sync/zotero/storage/JHW8MFMJ/Reinforcement Learning with Particle Swarm Optimization Policy.pdf:application/pdf}
}

@book{press_numerical_2007,
	title = {Numerical Recipes 3rd Edition: The Art of Scientific Computing},
	isbn = {978-0-521-88068-8},
	shorttitle = {Numerical Recipes 3rd Edition},
	abstract = {Co-authored by four leading scientists from academia and industry, Numerical Recipes Third Edition starts with basic mathematics and computer science and proceeds to complete, working routines. Widely recognized as the most comprehensive, accessible and practical basis for scientific computing, this new edition incorporates more than 400 Numerical Recipes routines, many of them new or upgraded. The executable C++ code, now printed in color for easy reading, adopts an object-oriented style particularly suited to scientific applications. The whole book is presented in the informal, easy-to-read style that made earlier editions so popular. Please visit www.nr.com or www.cambridge.org/us/numericalrecipes for more details. More information concerning licenses is available at: www.nr.com/licenses New key features:  2 new chapters, 25 new sections, 25\% longer than Second Edition Thorough upgrades throughout the text Over 100 completely new routines and upgrades of many more. New Classification and Inference chapter, including Gaussian mixture models, {HMMs}, hierarchical clustering, Support Vector {MachinesNew} Computational Geometry chapter covers {KD} trees, quad- and octrees, Delaunay triangulation, and algorithms for lines, polygons, triangles, and spheres New sections include interior point methods for linear programming, Monte Carlo Markov Chains, spectral and pseudospectral methods for {PDEs}, and many new statistical distributions An expanded treatment of {ODEs} with completely new routines  Plus comprehensive coverage of  linear algebra, interpolation, special functions, random numbers, nonlinear sets of equations, optimization, eigensystems, Fourier methods and wavelets, statistical tests, {ODEs} and {PDEs}, integral equations, and inverse theory},
	pagetotal = {1195},
	publisher = {Cambridge University Press},
	author = {Press, William H.},
	date = {2007-09-06},
	langid = {english},
	keywords = {Computers / Mathematical \& Statistical Software, Mathematics / Applied, Mathematics / General, Mathematics / Numerical Analysis}
}

@book{russell_artificial_2010,
	title = {Artificial Intelligence: A Modern Approach},
	isbn = {978-0-13-604259-4},
	shorttitle = {Artificial Intelligence},
	abstract = {Artificial Intelligence: A Modern Approach, 3e offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.   Dr. Peter Norvig, contributing Artificial Intelligence author and Professor Sebastian Thrun, a Pearson author are offering a free online course at Stanford University on artificial intelligence.    According to an article in  The New York Times , the course on artificial intelligence is “one of three being offered experimentally by the Stanford computer science department to extend technology knowledge and skills beyond this elite campus to the entire world.” One of the other two courses, an introduction to database software, is being taught by Pearson author Dr. Jennifer Widom.     Artificial Intelligence: A Modern Approach, 3e is available to purchase as an {eText} for your Kindle™, {NOOK}™, and the {iPhone}®/{iPad}®.    To learn more about the course on artificial intelligence, visit http://www.ai-class.com. To read the full New York Times article, click here.},
	pagetotal = {1153},
	publisher = {Prentice Hall},
	author = {Russell, Stuart Jonathan and Norvig, Peter},
	date = {2010},
	langid = {english},
	keywords = {Computers / Intelligence ({AI}) \& Semantics}
}