\input{preamble.tex}
\input{figures/tikz_common.tex}
\input{figures/tikz_colors.tex}
\input{figures/tikz_jumping.tex}
\input{../thesis/preamble/abbreviations.tex}

\title{Incorporating Uncertainty into Reinforcement Learning through Gaussian Processes}
\author{Markus Kaiser}
\subtitle{Master's Thesis}
\author{\href{mailto:markus.kaiser@in.tum.de}{Markus Kaiser}}
\titlegraphic{\parbox{.95\textwidth}{%
    \raisebox{-.5\height}{\includegraphics[height=1.5cm,width=.33\linewidth,keepaspectratio]{tum}}%
    \hfill%
    \raisebox{-.5\height}{\includegraphics[height=1.5cm,width=.33\linewidth,keepaspectratio]{kth}}%
    \hfill%
    \raisebox{-.5\height}{\includegraphics[height=1.5cm,width=.33\linewidth,keepaspectratio]{siemens}}%
}}
\date{4. July 2016}

\begin{document}

\begin{frame}[plain]
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}

    \tableofcontents
\end{frame}

\section{Bicycle Benchmark}
\begin{frame}
    \frametitle{Bicycle Benchmark Definition}

    \begin{columns}
        \begin{column}[b]{0.5\textwidth}
            \centering
            \includestandalone{figures/bicycle_from_above}
        \end{column}
        \begin{column}[b]{0.5\textwidth}
            \centering
            \includestandalone{figures/bicycle_from_behind}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Bicycle Benchmark Goal}

    Balance and drive to the goal
\end{frame}

\section{Gaussian Processes}
\begin{frame}
    \frametitle{Gaussian Processes}

    \begin{definition}[Gaussian Process]
        A stochastic process $\rv{X}$ is called a \structure{Gaussian process} if for any finite subset $\tau \subseteq T$ of its index set, the random variables $\rv{X}_\tau$ have a joint Gaussian distribution.
    \end{definition}
\end{frame}

\begin{frame}
    \frametitle{GP Prior and Posterior}

    \begin{block}{Yoyoyo}
        Blarghl
    \end{block}
    \vfill
    \centering
    \includestandalonewithpath{figures/gp_posterior}
\end{frame}

\begin{frame}
    \frametitle{GP Prior and Hyperparameters}

    content
\end{frame}

\begin{frame}
    \frametitle{Sparse GPs}

    \centering
    \includestandalonewithpath{figures/spgp}
\end{frame}

\section{Uncertainties in Reinforcement Learning}
\begin{frame}[label=architecture]
    \frametitle{Solution Architecture}

    \centering
    \includestandalone{figures/agent_environment}
\end{frame}

\begin{frame}
    \frametitle{Reward Function}

    \begin{definition}[Bicycle Reward Function]
        Given the set $\Es^+$ of possible states of the bicycle benchmark and the sets $\Goal$ and $\Fallen$ of all terminal states where the bicycle has reached the goal or fallen respectively, the \emph{bicycle reward function} is defined as
        \begin{align}
            \RwdBicycle \colon \left\{
                \begin{aligned}
                    \Es^+ &\to \R \\
                    \mat{s} &\mapsto \begin{cases}
                    2 & \text{if $\mat{s} \in \Goal$} \\
                    0 & \text{if $\mat{s} \in \Fallen$} \\
                    \sqrt{2\pi\sigma_{\text{angle}}^2}\Gaussian{\Delta^\psi_{\mat{s}} \given 0, \sigma_{\text{angle}}^2} & \text{otherwise}
                \end{cases}
            \end{aligned}
            \right.
        \end{align}
    where $\Delta^\psi_{\mat{s}} = \abs{\psi_{\mat{s}} - \varphi_{\mat{s}}} \in (-\pi, \pi]$ denotes the difference between the bicycle's heading and the goal given a state $\mat{s}$ and $\sigma_{\text{angle}}^2$ is a positive real constant.
    The Gaussian density function $\Norm$ is normalized to a maximum value of 1.
    \end{definition}
\end{frame}

\begin{frame}
    \frametitle{Expected Reward and PSO-P}

    \begin{align}
        \label{eq:action_value_function}
        \AVlu_{\mat{s_0}} \colon \left\{
            \begin{aligned}
                \Ah^T &\to \R \\
                \left( \mat{a_0}, \dots, \mat{a_{T-1}} \right) &\mapsto \Moment*{\E}{\sum_{t=1}^T \gamma^t \Rwd(\mat{s_t}) \given f, \mat{s_0}, \mat{a_0}, \dots, \mat{a_{T-1}}}
            \end{aligned}
            \right.
        \end{align}

    \begin{definition}[PSO-P]
        \label{def:psop}
        The \emph{Particle Swarm Optimization-Policy (PSO-P)} chooses actions via direct optimization of the action value function $\AVlu$ and is defined as
        \begin{align}
            \pi_{\text{PSO-P}} &: \left\{
            \begin{aligned}
                \Es &\to \Ah\\
                \mat{s} &\mapsto \mat{a^\ast}_0 \text{, where } \mat{a^\ast} \in \argmax_{\mat{a} \in \Ah^T} \AVlu_{\mat{s}}(\mat{a}).
            \end{aligned}
            \right.
        \end{align}
    \end{definition}
\end{frame}

\begin{frame}
    \frametitle{MAP predictions}

    content
\end{frame}

\begin{frame}
    \frametitle{One-Step Predictions}

    content
\end{frame}

\begin{frame}
    \frametitle{Multi-Step Predictions}

    \centering
    \includestandalonewithpath{figures/linearization}
\end{frame}

\section{Results}
\begin{frame}
    \frametitle{Qualitative}

    % Center ignoring margins
    \makebox[\textwidth][c] {
        \only<1> {
            \includestandalonewithpath{figures/data_xy_trajectories}
        }
        \only<2-> {
            \includestandalonewithpath{figures/xy_trajectories}
        }
    }
\end{frame}

\begin{frame}
    \frametitle{Evaluation Setup}

    content
\end{frame}

\begin{frame}
    \frametitle{Quantitative Results}

    content
\end{frame}

\end{document}
